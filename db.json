{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-icarus/source/css/cyberpunk.styl","path":"css/cyberpunk.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/css/default.styl","path":"css/default.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/avatar.png","path":"img/avatar.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/favicon.svg","path":"img/favicon.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/logo.svg","path":"img/logo.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/og_image.png","path":"img/og_image.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-bottom-black.svg","path":"img/razor-bottom-black.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-top-black.svg","path":"img/razor-top-black.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/animation.js","path":"js/animation.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/back_to_top.js","path":"js/back_to_top.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/column.js","path":"js/column.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"source/images/All_Things_ViT/A_transformer.png","path":"images/All_Things_ViT/A_transformer.png","modified":1,"renderable":0},{"_id":"source/images/All_Things_ViT/B_transformer.png","path":"images/All_Things_ViT/B_transformer.png","modified":1,"renderable":0},{"_id":"source/images/All_Things_ViT/RNN.png","path":"images/All_Things_ViT/RNN.png","modified":1,"renderable":0},{"_id":"source/images/GAN/GAN.png","path":"images/GAN/GAN.png","modified":1,"renderable":0},{"_id":"source/images/PCA/PCA.png","path":"images/PCA/PCA.png","modified":1,"renderable":0},{"_id":"source/images/R_Rollout/Bert.png","path":"images/R_Rollout/Bert.png","modified":1,"renderable":0},{"_id":"source/images/R_Rollout/R_Rollout.png","path":"images/R_Rollout/R_Rollout.png","modified":1,"renderable":0},{"_id":"source/images/R_Rollout/ViT.png","path":"images/R_Rollout/ViT.png","modified":1,"renderable":0},{"_id":"source/images/R_Rollout/ablation.png","path":"images/R_Rollout/ablation.png","modified":1,"renderable":0},{"_id":"source/images/R_Rollout/class_specific.png","path":"images/R_Rollout/class_specific.png","modified":1,"renderable":0},{"_id":"source/images/R_Rollout/perturbation.png","path":"images/R_Rollout/perturbation.png","modified":1,"renderable":0},{"_id":"source/images/R_Rollout/sample_results.png","path":"images/R_Rollout/sample_results.png","modified":1,"renderable":0},{"_id":"source/images/R_Rollout/segment.png","path":"images/R_Rollout/segment.png","modified":1,"renderable":0},{"_id":"source/images/TextCNN/fig1.png","path":"images/TextCNN/fig1.png","modified":1,"renderable":0},{"_id":"source/images/TextCNN/table1.png","path":"images/TextCNN/table1.png","modified":1,"renderable":0},{"_id":"source/images/TextCNN/table2.png","path":"images/TextCNN/table2.png","modified":1,"renderable":0},{"_id":"source/gallery/cover/cover.jpeg","path":"gallery/cover/cover.jpeg","modified":1,"renderable":0},{"_id":"source/gallery/thumbnails/cover.jpeg","path":"gallery/thumbnails/cover.jpeg","modified":1,"renderable":0},{"_id":"source/ppt/DashBot/VIS2022.pptx","path":"ppt/DashBot/VIS2022.pptx","modified":1,"renderable":0}],"Cache":[{"_id":"source/about/index.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1667363473677},{"_id":"source/_posts/All-Things-ViT-理解并解释分析视觉中的注意力机制-CVPR2023-Tutorial.md","hash":"6ee1600af613f483c76a17a59671651dfef1f776","modified":1689041781898},{"_id":"source/_posts/.DS_Store","hash":"eb1d87162da6a5873ebcb00219b0fe63338cb0f7","modified":1686105527213},{"_id":"source/_posts/Convolutional-Neural-Networks-for-Sentence-Classification-TextCNN.md","hash":"57eaf7eb532e5344512f695b55a2992d10b7b6b2","modified":1673440947336},{"_id":"source/_posts/BILSTM-CRF-原理及-pytorch-实现.md","hash":"5a22960e070cdc15923be32e249e5bf93e06ab13","modified":1673440947317},{"_id":"source/_posts/Transformer-Interpretability-Beyond-Attention-Visualization-论文精度及代码复现.md","hash":"618506444f55ceb73e6a54ef7fb8df97833c8ec2","modified":1688633956486},{"_id":"source/_posts/DashBot-Insight-Driven-Dashboard-Generation-Based-on-Deep-Reinforcement-Learning.md","hash":"81a434ed2598edde36301d898f4c310267f479f0","modified":1679636171405},{"_id":"source/_posts/Generative-Adversarial-Nets-GAN-论文精读.md","hash":"a02c8416580950f7595be601f9cce86384fdfc63","modified":1669206038189},{"_id":"source/_posts/Target-Encoding-算法比赛提分利器.md","hash":"76dd69f767673b36263051957dc3140c8a9698c9","modified":1675864599374},{"_id":"source/_posts/archive-todo-pages.md","hash":"64b2c6e87af5271c9350ad21ee8f388ecdbf2b1b","modified":1701323297203},{"_id":"source/_posts/hello-world.md","hash":"f0ad2ed880fb90bbc781439a8dd5b78abfa60b04","modified":1667363473677},{"_id":"source/_posts/VAE-原理及实现.md","hash":"62b01ded8e457cc244ae4c810051aba6d9ef64cf","modified":1675864599374},{"_id":"source/_posts/一问读懂-L1-正则化-Lasso-、L2-正则化-Ridge-、权重衰减.md","hash":"b49a6d66e3aa15a7a785d4a27620dac6b90741df","modified":1669022961342},{"_id":"source/_posts/diffusion-原理及代码.md","hash":"31bf1b59ac17aea23bf56e2b947097f73f6a51a1","modified":1687854651907},{"_id":"source/_posts/主成分分析（PCA）.md","hash":"a1fed87abae615836bf9996539a81d6ca6233153","modified":1669022953865},{"_id":"source/_posts/优化算法总结.md","hash":"0d22735dee50c817e3462e064920dc4ebc2ec0b5","modified":1676984624766},{"_id":"source/_posts/准备工作：统计与因果模型.md","hash":"05eb6fa6039f5b909eab6a5b7aa2b99ae629041e","modified":1669021199793},{"_id":"source/_posts/损失函数汇总.md","hash":"c6688191d8f0ad413c7feb6a917e1dbcfffbc4d6","modified":1679639458688},{"_id":"source/_posts/图模型及其应用.md","hash":"983ac98018e35c8aae41d9e78433b65c2566437c","modified":1669378778546},{"_id":"source/_posts/深度学习提分技巧.md","hash":"f2ea360ecee4504e59f82e31bc2eaa09af0fc96b","modified":1673440947317},{"_id":"source/_posts/深度学习调参指南.md","hash":"327327c9f0b9b34a67b0afbcef32b08a67770c6d","modified":1675864599374},{"_id":"source/_posts/激活函数汇总.md","hash":"c662b616b24f5ff0f3bddbb746adf91c0d3a2fb5","modified":1676984624767},{"_id":"source/text/first.md","hash":"579bbd1bba8b40833f14802f27dca6c6e6bf0943","modified":1665903600897},{"_id":"source/images/All_Things_ViT/RNN.png","hash":"327b032421960b110c6a056ef8df1225f13489bd","modified":1688972859102},{"_id":"source/images/R_Rollout/R_Rollout.png","hash":"fb59891739d1fb48b0900e0113854d1e2bbac322","modified":1688632608528},{"_id":"source/images/R_Rollout/Bert.png","hash":"56248a07704198d738fa36de794f3566ead865de","modified":1688633121162},{"_id":"source/images/GAN/GAN.png","hash":"cb6a901800ed6b2608e88f40cce905232d4b90bc","modified":1669193342575},{"_id":"source/images/R_Rollout/ablation.png","hash":"dee17220a7fd799d9e7478b34e33aed7fdba9fe3","modified":1688633279227},{"_id":"source/images/R_Rollout/perturbation.png","hash":"a39c077df9dc5b396ce21cf23f4e4498979ed485","modified":1688633025839},{"_id":"source/images/R_Rollout/segment.png","hash":"7a37133a3a0e84c43d39afddb8b6cce2355fadff","modified":1688633111407},{"_id":"source/images/TextCNN/table1.png","hash":"902d2d6663558ada1c920811f50ee29b9bfc8bd2","modified":1672659756817},{"_id":"source/gallery/thumbnails/cover.jpeg","hash":"34c027ed19bfbf909964685fe46b68f9069564ff","modified":1667363473678},{"_id":"source/gallery/cover/cover.jpeg","hash":"34c027ed19bfbf909964685fe46b68f9069564ff","modified":1667363473678},{"_id":"source/images/R_Rollout/ViT.png","hash":"895fc612392423ab2c9be65ec6833d61ca563988","modified":1688633343005},{"_id":"source/images/TextCNN/fig1.png","hash":"f3fb149be52d1bff0536fef8544ed36761f29f6d","modified":1672652904638},{"_id":"source/images/All_Things_ViT/A_transformer.png","hash":"03d8f7afae5c807c8fdc2a1d9cabb2b7dc030d28","modified":1688973702528},{"_id":"node_modules/hexo-theme-icarus/layout/comment/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1701326968757},{"_id":"node_modules/hexo-theme-icarus/layout/donate/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1701326968767},{"_id":"node_modules/hexo-theme-icarus/layout/misc/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1701326968779},{"_id":"node_modules/hexo-theme-icarus/layout/search/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1701326968791},{"_id":"node_modules/hexo-theme-icarus/layout/share/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1701326968800},{"_id":"node_modules/hexo-theme-icarus/include/schema/comment/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1701326968603},{"_id":"node_modules/hexo-theme-icarus/include/schema/misc/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1701326968709},{"_id":"node_modules/hexo-theme-icarus/include/schema/donate/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1701326968678},{"_id":"node_modules/hexo-theme-icarus/include/schema/search/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1701326968733},{"_id":"node_modules/hexo-theme-icarus/include/schema/share/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1701326968751},{"_id":"source/images/PCA/PCA.png","hash":"98a33f4a33084b82a0fd06bc1e60c01c902a30eb","modified":1667532997713},{"_id":"source/images/All_Things_ViT/B_transformer.png","hash":"5510e14e0430005cc8e74e23025661cb57397bf8","modified":1688973686258},{"_id":"node_modules/hexo-theme-icarus/package.json","hash":"0af7425c7201d97fb2e1acfaa990e7fcb2afcc53","modified":1701326968968},{"_id":"node_modules/hexo-theme-icarus/LICENSE","hash":"86037e5335a49321fa73b7815cab542057fac944","modified":1701326968804},{"_id":"node_modules/hexo-theme-icarus/layout/archive.jsx","hash":"05677e93d4a43f417dbbf0d63ca37a99e6349e3b","modified":1701326969044},{"_id":"node_modules/hexo-theme-icarus/README.md","hash":"32f9f4fc8cd7ec60b30544bd2e558b593519ae5d","modified":1701326969220},{"_id":"node_modules/hexo-theme-icarus/layout/category.jsx","hash":"fd15e4eac32de9ac8687aeb3dbe179ab61375700","modified":1701326969068},{"_id":"node_modules/hexo-theme-icarus/layout/layout.jsx","hash":"ac7c4e3465a116c7f05f8c2e09ee6d6b9467abf1","modified":1701326969145},{"_id":"node_modules/hexo-theme-icarus/layout/index.jsx","hash":"0a84a2348394fa9fc5080dd396bd28d357594f47","modified":1701326969138},{"_id":"node_modules/hexo-theme-icarus/layout/page.jsx","hash":"d26c2db57e5a88d6483a03aeb51cda9d191d8cea","modified":1701326969165},{"_id":"node_modules/hexo-theme-icarus/CONTRIBUTING.md","hash":"70254c6778c1e41bb2ff222bbf3a70b2239b9bc1","modified":1701326969218},{"_id":"node_modules/hexo-theme-icarus/layout/categories.jsx","hash":"b8ad43e28a4990d222bfbb95b032f88555492347","modified":1701326969065},{"_id":"node_modules/hexo-theme-icarus/layout/tag.jsx","hash":"d2f18cac32ca2725d34ccff3f2051c623be6c892","modified":1701326969211},{"_id":"node_modules/hexo-theme-icarus/languages/en.yml","hash":"3d674204d9f723c829226da745afddd180c1131d","modified":1701326969277},{"_id":"node_modules/hexo-theme-icarus/languages/de.yml","hash":"78421f09961ca0b24756a0688fb2cb2e2696e25f","modified":1701326969276},{"_id":"node_modules/hexo-theme-icarus/layout/post.jsx","hash":"d26c2db57e5a88d6483a03aeb51cda9d191d8cea","modified":1701326969178},{"_id":"node_modules/hexo-theme-icarus/layout/tags.jsx","hash":"2c42cb64778235dd220c563a27a92108ddc50cc4","modified":1701326969214},{"_id":"node_modules/hexo-theme-icarus/languages/es.yml","hash":"38579b8fad4b6997362acc770615bcd85ff20f68","modified":1701326969278},{"_id":"node_modules/hexo-theme-icarus/languages/fr.yml","hash":"06d5c819d6108a42b28cff7b52e5410d0bed55d1","modified":1701326969279},{"_id":"node_modules/hexo-theme-icarus/languages/pl.yml","hash":"2e7debb44cd91096f30efc87bf8d6b1d0d0214c9","modified":1701326969281},{"_id":"node_modules/hexo-theme-icarus/languages/tk.yml","hash":"ca583168bd2025124a1cd0e977da475d7a7496fd","modified":1701326969287},{"_id":"node_modules/hexo-theme-icarus/languages/pt-BR.yml","hash":"ee8f73350e4c6e2f63b7fc72b34472a6b1e21244","modified":1701326969282},{"_id":"node_modules/hexo-theme-icarus/languages/ja.yml","hash":"801d9930fef48d6a3f80470d5bed4f3eb78147e6","modified":1701326969280},{"_id":"node_modules/hexo-theme-icarus/languages/ru.yml","hash":"9d91358c2acbe7a0f2a25daf7f65b999ff32d068","modified":1701326969285},{"_id":"node_modules/hexo-theme-icarus/languages/tr.yml","hash":"dd0a7bfe14848d6e1aa229198fe1db03e08e305e","modified":1701326969287},{"_id":"node_modules/hexo-theme-icarus/languages/ko.yml","hash":"e3374265377809c1518114cf352b595840c0b416","modified":1701326969281},{"_id":"node_modules/hexo-theme-icarus/include/dependency.js","hash":"0ca35dec92ccf383f45db905db1a5a0e92d7209e","modified":1701326968844},{"_id":"node_modules/hexo-theme-icarus/include/register.js","hash":"ec6596b63bfb4349ba61792d905abe8e06fea625","modified":1701326968874},{"_id":"node_modules/hexo-theme-icarus/include/config.js","hash":"1ff0f174e9670074ad2bee890d5b6da486800c9a","modified":1701326968836},{"_id":"node_modules/hexo-theme-icarus/scripts/index.js","hash":"0c666db6fcb4ffc4d300f4e108c00ee42b1cbbe6","modified":1701326968856},{"_id":"node_modules/hexo-theme-icarus/languages/id.yml","hash":"5e48b1d62378cadeb64b88349477726a5c1bae47","modified":1701326969280},{"_id":"node_modules/hexo-theme-icarus/languages/vn.yml","hash":"5f2fffa642110c81d8f529949711c9d19ad6bbbe","modified":1701326969288},{"_id":"node_modules/hexo-theme-icarus/languages/zh-CN.yml","hash":"02475ba14afc70dfeaf5678467cee307835e4efa","modified":1701326969289},{"_id":"node_modules/hexo-theme-icarus/layout/common/comment.jsx","hash":"427089c33002707b76e2f38709459a6824fd0f9b","modified":1701326969099},{"_id":"node_modules/hexo-theme-icarus/layout/common/donates.jsx","hash":"889fb0a7ccc502f0a43b4a18eb330e351e50493c","modified":1701326969126},{"_id":"node_modules/hexo-theme-icarus/layout/common/head.jsx","hash":"2ec1f511f32e3a9c86d49f1338f57ae5ece18898","modified":1701326969133},{"_id":"node_modules/hexo-theme-icarus/languages/zh-TW.yml","hash":"a6826e0c8cdb9ad286324b682b466a9e2ad78e6f","modified":1701326969289},{"_id":"node_modules/hexo-theme-icarus/layout/common/article.jsx","hash":"e2765287c6bc6eb97975a1bb4e325b046e95b392","modified":1701326969055},{"_id":"node_modules/hexo-theme-icarus/layout/common/footer.jsx","hash":"baa8e40e036a3ec0114d96893b708435104b4ba9","modified":1701326969132},{"_id":"node_modules/hexo-theme-icarus/layout/common/navbar.jsx","hash":"d96e501e52861056474659f96ee0206588d8c93a","modified":1701326969159},{"_id":"node_modules/hexo-theme-icarus/layout/common/scripts.jsx","hash":"0fe1fddab431fb9f63906d8c480d5cd6b33abc32","modified":1701326969194},{"_id":"node_modules/hexo-theme-icarus/layout/common/plugins.jsx","hash":"f6826c1a5f5f59f4a0aa00c63bdb0ad4ff4eab69","modified":1701326969168},{"_id":"node_modules/hexo-theme-icarus/layout/common/search.jsx","hash":"6f244a37293031670a2964fe424ecd062e591d7b","modified":1701326969199},{"_id":"node_modules/hexo-theme-icarus/layout/common/share.jsx","hash":"c9fb0319ad5e5a10ad3636b26a6c2afed14c590f","modified":1701326969202},{"_id":"node_modules/hexo-theme-icarus/layout/common/widgets.jsx","hash":"251263b97de12f2b8d1fce2514e83430f2515b94","modified":1701326969215},{"_id":"node_modules/hexo-theme-icarus/include/migration/head.js","hash":"7189efe33d18927d3790e8afb06642fb293b8603","modified":1701326968851},{"_id":"node_modules/hexo-theme-icarus/layout/plugin/animejs.jsx","hash":"e2aa27c3501a58ef1e91e511557b77395c2c02aa","modified":1701326969020},{"_id":"node_modules/hexo-theme-icarus/layout/widget/profile.jsx","hash":"0d3a7fd922c12cc45d2c8d26a8f4d3a9a6ed0ae0","modified":1701326969190},{"_id":"node_modules/hexo-theme-icarus/include/migration/v2_v3.js","hash":"3ccb2d2ce11018bebd7172da66faecc3983bff00","modified":1701326968883},{"_id":"node_modules/hexo-theme-icarus/include/migration/v5_v5.1.js","hash":"073f22bd16e34b56f016633b1676dab2e7d8843d","modified":1701326968896},{"_id":"node_modules/hexo-theme-icarus/include/schema/config.json","hash":"b949c52d02d5ee520ae4666a016ce91faf6fb350","modified":1701326968951},{"_id":"node_modules/hexo-theme-icarus/include/migration/v3_v4.js","hash":"9faf2184d7fe87debfbe007f3fc9079dcbcafcfe","modified":1701326968886},{"_id":"node_modules/hexo-theme-icarus/layout/plugin/back_to_top.jsx","hash":"7fc0c5aaabd7d0eaff04cb68ec139442dc3414e8","modified":1701326969059},{"_id":"node_modules/hexo-theme-icarus/include/migration/v4_v5.js","hash":"6342310892d113763b5544789b45d44c0ccf2854","modified":1701326968891},{"_id":"node_modules/hexo-theme-icarus/source/css/cyberpunk.styl","hash":"ae17d3528df0c3f089df14a06b7bd82f1bc5fed9","modified":1701326969255},{"_id":"node_modules/hexo-theme-icarus/source/css/default.styl","hash":"b01da3028e5a1267a40aaae5c86a11187a2259e3","modified":1701326969255},{"_id":"node_modules/hexo-theme-icarus/source/img/avatar.png","hash":"0d8236dcca871735500e9d06bbdbe0853ed6775b","modified":1701326969233},{"_id":"node_modules/hexo-theme-icarus/source/img/favicon.svg","hash":"16fd847265845063a16596761cddb32926073dd2","modified":1701326969270},{"_id":"node_modules/hexo-theme-icarus/source/css/style.styl","hash":"5b9815586e993a6ccbe8cdcfc0c65ea38fc315ac","modified":1701326969267},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-bottom-black.svg","hash":"a3eda07b1c605b456da9cdf335a1075db5e5d72c","modified":1701326969272},{"_id":"node_modules/hexo-theme-icarus/source/img/logo.svg","hash":"e9b5c1438ddb576693a15d0713b2a1d9ceda4be9","modified":1701326969271},{"_id":"node_modules/hexo-theme-icarus/source/img/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1701326969243},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-top-black.svg","hash":"201f1171a43ce667a39091fe47c0f278857f18f0","modified":1701326969273},{"_id":"node_modules/hexo-theme-icarus/include/style/article.styl","hash":"105c983871b6c9148d97a0f756886e56411572bd","modified":1701326969244},{"_id":"node_modules/hexo-theme-icarus/include/style/base.styl","hash":"2bca6ad099949d52236c87db8db1002ffb99774c","modified":1701326969245},{"_id":"node_modules/hexo-theme-icarus/include/style/button.styl","hash":"0fb35b4786be1b387c751fa2849bc71523fcedd4","modified":1701326969247},{"_id":"node_modules/hexo-theme-icarus/include/style/codeblock.styl","hash":"ec54dc24eb4d9802d8fefc44c210558bc1641109","modified":1701326969251},{"_id":"node_modules/hexo-theme-icarus/include/style/footer.styl","hash":"a4ad715dee38b249538ac6cce94efc9b355a904b","modified":1701326969258},{"_id":"node_modules/hexo-theme-icarus/include/style/donate.styl","hash":"8d0af00628c13134b5f30a558608e7bebf18c2ec","modified":1701326969256},{"_id":"node_modules/hexo-theme-icarus/include/style/card.styl","hash":"f78674422eb408cd17c17bbdc3ee1ebe4a453e05","modified":1701326969251},{"_id":"node_modules/hexo-theme-icarus/include/style/search.styl","hash":"416737e1da4e7e907bd03609b0fee9e2aacfe56c","modified":1701326969266},{"_id":"node_modules/hexo-theme-icarus/include/style/helper.styl","hash":"9f3393e6122cc9f351091bfab960674e962da343","modified":1701326969259},{"_id":"node_modules/hexo-theme-icarus/include/style/navbar.styl","hash":"34f09b144cb46a25ec2cc7260a6c207dd34ff1fe","modified":1701326969260},{"_id":"node_modules/hexo-theme-icarus/include/style/plugin.styl","hash":"084843d5a522029e0f84a4fe791fbcb2cabd4c36","modified":1701326969263},{"_id":"node_modules/hexo-theme-icarus/include/style/responsive.styl","hash":"207083fe287612cddee6608b541861b14ac8de81","modified":1701326969265},{"_id":"node_modules/hexo-theme-icarus/include/style/pagination.styl","hash":"b81bcd7ff915b4e9299533addc01bc4575ec35e3","modified":1701326969261},{"_id":"node_modules/hexo-theme-icarus/include/style/widget.styl","hash":"c746902251136544eb3fe523235b3183f4189460","modified":1701326969269},{"_id":"node_modules/hexo-theme-icarus/include/style/timeline.styl","hash":"ea61798a09bffdda07efb93c2ff800b63bddc4c4","modified":1701326969269},{"_id":"node_modules/hexo-theme-icarus/source/js/animation.js","hash":"12cedd5caaf9109eed97e50eeab8f883f6e49be3","modified":1701326968811},{"_id":"node_modules/hexo-theme-icarus/include/util/console.js","hash":"59cf9d277d3ac85a496689bd811b1c316001641d","modified":1701326968843},{"_id":"node_modules/hexo-theme-icarus/source/js/back_to_top.js","hash":"d91f10c08c726135a13dfa1f422c49d8764ef03f","modified":1701326968814},{"_id":"node_modules/hexo-theme-icarus/source/js/column.js","hash":"0baee024ab67474c073a4c41b495f3e7f0df4505","modified":1701326968824},{"_id":"node_modules/hexo-theme-icarus/source/js/main.js","hash":"08a2641765eeaf712157ad134dd675e3f7708ae2","modified":1701326968866},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/article.json","hash":"e2502c39045c6a26ccd8e880858f93e78c7bda35","modified":1701326968930},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/comment.json","hash":"f49270b619f5d2c3decde6b0b5a0c3bbab4b54a5","modified":1701326968937},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/footer.json","hash":"09d706cbb94d6da9a0d15c719ce7139325cae1c7","modified":1701326968962},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/donates.json","hash":"ae86e6f177bedf4afbe638502c12635027539305","modified":1701326968961},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/head.json","hash":"98889f059c635e6bdbd51effd04cf1cf44968a66","modified":1701326968963},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/navbar.json","hash":"6691e587284c4cf450e0288680d5ff0f3565f090","modified":1701326968964},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/plugins.json","hash":"6036a805749816416850d944f7d64aaae62e5e75","modified":1701326968974},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/providers.json","hash":"97ec953d497fb53594227ae98acaef8a8baa91da","modified":1701326968994},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/share.json","hash":"cf4f9ff4fb27c3541b35f57db355c228fa6873e4","modified":1701326968996},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/sidebar.json","hash":"eb241beaec4c73e3085dfb3139ce72e827e20549","modified":1701326969000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/search.json","hash":"985fbcbf47054af714ead1a124869d54f2a8b607","modified":1701326968995},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/widgets.json","hash":"cadd9dc942740ecd5037d3943e72f8b6a8399bbe","modified":1701326969002},{"_id":"node_modules/hexo-theme-icarus/include/schema/widget/profile.json","hash":"690ee1b0791cab47ea03cf42b5b4932ed2aa5675","modified":1701326968988},{"_id":"node_modules/hexo-theme-icarus/include/schema/plugin/animejs.json","hash":"e62ab6e20bd8862efa1ed32e7c0db0f8acbcfdec","modified":1701326968904},{"_id":"node_modules/hexo-theme-icarus/include/schema/plugin/back_to_top.json","hash":"dc0febab7e7b67075d0ad3f80f5ec8b798b68dea","modified":1701326968935},{"_id":"source/images/TextCNN/table2.png","hash":"35f2aff4257a5eb02941650ed4e37b52e75daa40","modified":1672661628434},{"_id":"source/images/R_Rollout/class_specific.png","hash":"94f3f04a35dffe5515ae0866160bc16c39b62dee","modified":1688632922831},{"_id":"source/images/R_Rollout/sample_results.png","hash":"838d1b4212e8e777242eb61a59e7f5dbfcede966","modified":1688632865547},{"_id":"source/_posts/DashBot-Insight-Driven-Dashboard-Generation-Based-on-Deep-Reinforcement-Learning/VIS2022.pptx","hash":"a4e7869fa817986bae201b159810ec03d8316c7f","modified":1679629268023},{"_id":"source/ppt/DashBot/VIS2022.pptx","hash":"a4e7869fa817986bae201b159810ec03d8316c7f","modified":1679636128553}],"Category":[{"name":"机器学习","_id":"clpku7kp1000gqnwyeym019np"},{"name":"正则化","_id":"clpku7kp1000jqnwy16asc1ar"},{"name":"无监督学习","_id":"clpku7kp4000uqnwyfra4a7tb"}],"Data":[],"Page":[{"title":"first post test","date":"2022-10-16T07:00:00.000Z","_content":"","source":"text/first.md","raw":"---\ntitle: first post test\ndate: 2022-10-16 15:00:00\n---\n","updated":"2022-10-16T07:00:00.897Z","path":"text/first.html","comments":1,"layout":"page","_id":"clpku7kou0000qnwy8z0z08eb","content":"","site":{"data":{}},"excerpt":"","more":""},{"_content":"","source":"about/index.md","raw":"","date":"2023-11-30T06:48:07.333Z","updated":"2022-11-02T04:31:13.677Z","path":"about/index.html","title":"","comments":1,"layout":"page","_id":"clpku7kow0002qnwy6mfrgov1","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"BILSTM-CRF 原理及 Pytorch 实现","date":"2023-01-11T11:27:29.000Z","_content":"## 动态和静态的深度学习工具\n\n动态神经网络工具有 Pytorch 和 Dynet。静态工具包包括 Theano、Keras 和 TensorFlow。他们的核心区别是：\n- 在静态工具包中，只需定义一次计算图，编译他，然后将实例传输给它\n- 在动态工具包中，需要为每个实例定义一个计算图。它永远不会编译，而是动态执行\n\n如果没有丰富的经验，很难领会到其中的差别。这里我们举一个深度成分分解器的例子，假设我们的模型大致包括下面的步骤：\n- 我们自底向上构建树\n- 标记根节点（句子中的单词）\n- 在此基础上，使用神经网络和单词嵌入来找到构成成分的组合。每当网络形成一个新的组成部分，使用某种方法来得到一个组成部分的嵌入表示。在这种情况下，我们的网络架构将完全依赖于组成部分的嵌入。比如 “the green cat scratch the wall” 这句话中，我们将组合表示为 $(i, j, r) = (1, 3, NP)$（也就是说，一个 $NP$ 成分横跨单词1 到单词 3，也就是本例中的 \"the green cat\"）\n\n然而，对于另一个句子 “Somewhere, the big fat cat scratched the wall” 。在执行过程中，我们可能会想这样定义成分 $(2, 4, NP)$。成分的定义依赖于实例。在静态工具中通过计算图的一次编译很难或者说基本不可能编程这样的逻辑。然而，在动态工具中并不只有一个预定义的计算图，每一个实例都会有一个新的计算图，所以不存在这个问题。\n\n动态工具的另一个优点是，更容易调试而且更加接近宿主语言（也就是说 Pytorch 和 Dynet 相比 Keras 或者 Theano 更接近 Python 的写法）\n\n## BiLSTM-CRF 决策\n\n在本节中，我们将看到一个用于命名实体识别的 BiLSTM-CRF 复杂示例。","source":"_posts/BILSTM-CRF-原理及-pytorch-实现.md","raw":"---\ntitle: BILSTM-CRF 原理及 Pytorch 实现\ndate: 2023-01-11 19:27:29\ntags:\n---\n## 动态和静态的深度学习工具\n\n动态神经网络工具有 Pytorch 和 Dynet。静态工具包包括 Theano、Keras 和 TensorFlow。他们的核心区别是：\n- 在静态工具包中，只需定义一次计算图，编译他，然后将实例传输给它\n- 在动态工具包中，需要为每个实例定义一个计算图。它永远不会编译，而是动态执行\n\n如果没有丰富的经验，很难领会到其中的差别。这里我们举一个深度成分分解器的例子，假设我们的模型大致包括下面的步骤：\n- 我们自底向上构建树\n- 标记根节点（句子中的单词）\n- 在此基础上，使用神经网络和单词嵌入来找到构成成分的组合。每当网络形成一个新的组成部分，使用某种方法来得到一个组成部分的嵌入表示。在这种情况下，我们的网络架构将完全依赖于组成部分的嵌入。比如 “the green cat scratch the wall” 这句话中，我们将组合表示为 $(i, j, r) = (1, 3, NP)$（也就是说，一个 $NP$ 成分横跨单词1 到单词 3，也就是本例中的 \"the green cat\"）\n\n然而，对于另一个句子 “Somewhere, the big fat cat scratched the wall” 。在执行过程中，我们可能会想这样定义成分 $(2, 4, NP)$。成分的定义依赖于实例。在静态工具中通过计算图的一次编译很难或者说基本不可能编程这样的逻辑。然而，在动态工具中并不只有一个预定义的计算图，每一个实例都会有一个新的计算图，所以不存在这个问题。\n\n动态工具的另一个优点是，更容易调试而且更加接近宿主语言（也就是说 Pytorch 和 Dynet 相比 Keras 或者 Theano 更接近 Python 的写法）\n\n## BiLSTM-CRF 决策\n\n在本节中，我们将看到一个用于命名实体识别的 BiLSTM-CRF 复杂示例。","slug":"BILSTM-CRF-原理及-pytorch-实现","published":1,"updated":"2023-01-11T12:42:27.317Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kov0001qnwyh4tu82do","content":"<h2 id=\"动态和静态的深度学习工具\"><a href=\"#动态和静态的深度学习工具\" class=\"headerlink\" title=\"动态和静态的深度学习工具\"></a>动态和静态的深度学习工具</h2><p>动态神经网络工具有 Pytorch 和 Dynet。静态工具包包括 Theano、Keras 和 TensorFlow。他们的核心区别是：</p>\n<ul>\n<li>在静态工具包中，只需定义一次计算图，编译他，然后将实例传输给它</li>\n<li>在动态工具包中，需要为每个实例定义一个计算图。它永远不会编译，而是动态执行</li>\n</ul>\n<p>如果没有丰富的经验，很难领会到其中的差别。这里我们举一个深度成分分解器的例子，假设我们的模型大致包括下面的步骤：</p>\n<ul>\n<li>我们自底向上构建树</li>\n<li>标记根节点（句子中的单词）</li>\n<li>在此基础上，使用神经网络和单词嵌入来找到构成成分的组合。每当网络形成一个新的组成部分，使用某种方法来得到一个组成部分的嵌入表示。在这种情况下，我们的网络架构将完全依赖于组成部分的嵌入。比如 “the green cat scratch the wall” 这句话中，我们将组合表示为 $(i, j, r) &#x3D; (1, 3, NP)$（也就是说，一个 $NP$ 成分横跨单词1 到单词 3，也就是本例中的 “the green cat”）</li>\n</ul>\n<p>然而，对于另一个句子 “Somewhere, the big fat cat scratched the wall” 。在执行过程中，我们可能会想这样定义成分 $(2, 4, NP)$。成分的定义依赖于实例。在静态工具中通过计算图的一次编译很难或者说基本不可能编程这样的逻辑。然而，在动态工具中并不只有一个预定义的计算图，每一个实例都会有一个新的计算图，所以不存在这个问题。</p>\n<p>动态工具的另一个优点是，更容易调试而且更加接近宿主语言（也就是说 Pytorch 和 Dynet 相比 Keras 或者 Theano 更接近 Python 的写法）</p>\n<h2 id=\"BiLSTM-CRF-决策\"><a href=\"#BiLSTM-CRF-决策\" class=\"headerlink\" title=\"BiLSTM-CRF 决策\"></a>BiLSTM-CRF 决策</h2><p>在本节中，我们将看到一个用于命名实体识别的 BiLSTM-CRF 复杂示例。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"动态和静态的深度学习工具\"><a href=\"#动态和静态的深度学习工具\" class=\"headerlink\" title=\"动态和静态的深度学习工具\"></a>动态和静态的深度学习工具</h2><p>动态神经网络工具有 Pytorch 和 Dynet。静态工具包包括 Theano、Keras 和 TensorFlow。他们的核心区别是：</p>\n<ul>\n<li>在静态工具包中，只需定义一次计算图，编译他，然后将实例传输给它</li>\n<li>在动态工具包中，需要为每个实例定义一个计算图。它永远不会编译，而是动态执行</li>\n</ul>\n<p>如果没有丰富的经验，很难领会到其中的差别。这里我们举一个深度成分分解器的例子，假设我们的模型大致包括下面的步骤：</p>\n<ul>\n<li>我们自底向上构建树</li>\n<li>标记根节点（句子中的单词）</li>\n<li>在此基础上，使用神经网络和单词嵌入来找到构成成分的组合。每当网络形成一个新的组成部分，使用某种方法来得到一个组成部分的嵌入表示。在这种情况下，我们的网络架构将完全依赖于组成部分的嵌入。比如 “the green cat scratch the wall” 这句话中，我们将组合表示为 $(i, j, r) &#x3D; (1, 3, NP)$（也就是说，一个 $NP$ 成分横跨单词1 到单词 3，也就是本例中的 “the green cat”）</li>\n</ul>\n<p>然而，对于另一个句子 “Somewhere, the big fat cat scratched the wall” 。在执行过程中，我们可能会想这样定义成分 $(2, 4, NP)$。成分的定义依赖于实例。在静态工具中通过计算图的一次编译很难或者说基本不可能编程这样的逻辑。然而，在动态工具中并不只有一个预定义的计算图，每一个实例都会有一个新的计算图，所以不存在这个问题。</p>\n<p>动态工具的另一个优点是，更容易调试而且更加接近宿主语言（也就是说 Pytorch 和 Dynet 相比 Keras 或者 Theano 更接近 Python 的写法）</p>\n<h2 id=\"BiLSTM-CRF-决策\"><a href=\"#BiLSTM-CRF-决策\" class=\"headerlink\" title=\"BiLSTM-CRF 决策\"></a>BiLSTM-CRF 决策</h2><p>在本节中，我们将看到一个用于命名实体识别的 BiLSTM-CRF 复杂示例。</p>\n"},{"title":"All Things ViT: 理解并解释分析视觉中的注意力机制 CVPR2023 Tutorial","date":"2023-07-10T06:44:10.000Z","_content":"\n一共六个 topic:\n\n- introdutions\n- Attention as a jiffy\n- Probing Vision Transformers\n- Explaining Transformers' Predictions\n- Attention as a (visual) explanation\n- Attention to aid downstream applications\n\n\n## Introdution\n- From RNNs to Transformers\n- Attention - Intuition\n- The Beast with Many Heads\n- Positional Encoding\n- Cross-Attention\n\n### From RNNs to Transformers\n\n在 2017 年以前，NLP 的研究任务的模型架构中 RNN 一直是主流，RNN 的模型架构如下图所示。他采用序列的方式去处理文本，即一个 token 一个 token 得去处理。\n\n但是这种架构存在着三种问题：\n- 序列处理文本时间消耗太高（你需要一个 token 一个 token 得去处理文本序列）\n- 模型感受野太小，这一点之前已经有人做过实验发现 RNN 的隐状态主要是受到邻近 token 的影响。\n- 模型只能从做向右单向处理文本（但是 Bi-LSTM 一定程度上解决了这个问题）\n\n![Figure 1: RNN 模型架构](/images/All_Things_Vit/RNN.png)\n\n但是，有了 transformer 之后上述的三个问题都得到了很好的解决：\n- 序列处理 vs. 编码过程完全并行处理\n- 感受野太小 vs. 上下文从所有的 tokens 中同步获得\n- 单向的上下文 vs. 从整个文本序列中得到上下文\n\n但是 transformer 的伟大之处不仅仅是解决了上述问题。随着 transformer 的出现实现了各个领域模型的大一统！\n\n\n![Figure 2: transformer 出现前各领域主流模型架构](/images/All_Things_Vit/B_transformer.png)\n![Figure 3: transformer 出现后各领域主流模型架构](/images/All_Things_Vit/A_transformer.png)\n\n","source":"_posts/All-Things-ViT-理解并解释分析视觉中的注意力机制-CVPR2023-Tutorial.md","raw":"---\ntitle: 'All Things ViT: 理解并解释分析视觉中的注意力机制 CVPR2023 Tutorial'\ndate: 2023-07-10 14:44:10\ntags:\n---\n\n一共六个 topic:\n\n- introdutions\n- Attention as a jiffy\n- Probing Vision Transformers\n- Explaining Transformers' Predictions\n- Attention as a (visual) explanation\n- Attention to aid downstream applications\n\n\n## Introdution\n- From RNNs to Transformers\n- Attention - Intuition\n- The Beast with Many Heads\n- Positional Encoding\n- Cross-Attention\n\n### From RNNs to Transformers\n\n在 2017 年以前，NLP 的研究任务的模型架构中 RNN 一直是主流，RNN 的模型架构如下图所示。他采用序列的方式去处理文本，即一个 token 一个 token 得去处理。\n\n但是这种架构存在着三种问题：\n- 序列处理文本时间消耗太高（你需要一个 token 一个 token 得去处理文本序列）\n- 模型感受野太小，这一点之前已经有人做过实验发现 RNN 的隐状态主要是受到邻近 token 的影响。\n- 模型只能从做向右单向处理文本（但是 Bi-LSTM 一定程度上解决了这个问题）\n\n![Figure 1: RNN 模型架构](/images/All_Things_Vit/RNN.png)\n\n但是，有了 transformer 之后上述的三个问题都得到了很好的解决：\n- 序列处理 vs. 编码过程完全并行处理\n- 感受野太小 vs. 上下文从所有的 tokens 中同步获得\n- 单向的上下文 vs. 从整个文本序列中得到上下文\n\n但是 transformer 的伟大之处不仅仅是解决了上述问题。随着 transformer 的出现实现了各个领域模型的大一统！\n\n\n![Figure 2: transformer 出现前各领域主流模型架构](/images/All_Things_Vit/B_transformer.png)\n![Figure 3: transformer 出现后各领域主流模型架构](/images/All_Things_Vit/A_transformer.png)\n\n","slug":"All-Things-ViT-理解并解释分析视觉中的注意力机制-CVPR2023-Tutorial","published":1,"updated":"2023-07-11T02:16:21.898Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kox0003qnwy391dbiwt","content":"<p>一共六个 topic:</p>\n<ul>\n<li>introdutions</li>\n<li>Attention as a jiffy</li>\n<li>Probing Vision Transformers</li>\n<li>Explaining Transformers’ Predictions</li>\n<li>Attention as a (visual) explanation</li>\n<li>Attention to aid downstream applications</li>\n</ul>\n<h2 id=\"Introdution\"><a href=\"#Introdution\" class=\"headerlink\" title=\"Introdution\"></a>Introdution</h2><ul>\n<li>From RNNs to Transformers</li>\n<li>Attention - Intuition</li>\n<li>The Beast with Many Heads</li>\n<li>Positional Encoding</li>\n<li>Cross-Attention</li>\n</ul>\n<h3 id=\"From-RNNs-to-Transformers\"><a href=\"#From-RNNs-to-Transformers\" class=\"headerlink\" title=\"From RNNs to Transformers\"></a>From RNNs to Transformers</h3><p>在 2017 年以前，NLP 的研究任务的模型架构中 RNN 一直是主流，RNN 的模型架构如下图所示。他采用序列的方式去处理文本，即一个 token 一个 token 得去处理。</p>\n<p>但是这种架构存在着三种问题：</p>\n<ul>\n<li>序列处理文本时间消耗太高（你需要一个 token 一个 token 得去处理文本序列）</li>\n<li>模型感受野太小，这一点之前已经有人做过实验发现 RNN 的隐状态主要是受到邻近 token 的影响。</li>\n<li>模型只能从做向右单向处理文本（但是 Bi-LSTM 一定程度上解决了这个问题）</li>\n</ul>\n<p><img src=\"/images/All_Things_Vit/RNN.png\" alt=\"Figure 1: RNN 模型架构\"></p>\n<p>但是，有了 transformer 之后上述的三个问题都得到了很好的解决：</p>\n<ul>\n<li>序列处理 vs. 编码过程完全并行处理</li>\n<li>感受野太小 vs. 上下文从所有的 tokens 中同步获得</li>\n<li>单向的上下文 vs. 从整个文本序列中得到上下文</li>\n</ul>\n<p>但是 transformer 的伟大之处不仅仅是解决了上述问题。随着 transformer 的出现实现了各个领域模型的大一统！</p>\n<p><img src=\"/images/All_Things_Vit/B_transformer.png\" alt=\"Figure 2: transformer 出现前各领域主流模型架构\"><br><img src=\"/images/All_Things_Vit/A_transformer.png\" alt=\"Figure 3: transformer 出现后各领域主流模型架构\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>一共六个 topic:</p>\n<ul>\n<li>introdutions</li>\n<li>Attention as a jiffy</li>\n<li>Probing Vision Transformers</li>\n<li>Explaining Transformers’ Predictions</li>\n<li>Attention as a (visual) explanation</li>\n<li>Attention to aid downstream applications</li>\n</ul>\n<h2 id=\"Introdution\"><a href=\"#Introdution\" class=\"headerlink\" title=\"Introdution\"></a>Introdution</h2><ul>\n<li>From RNNs to Transformers</li>\n<li>Attention - Intuition</li>\n<li>The Beast with Many Heads</li>\n<li>Positional Encoding</li>\n<li>Cross-Attention</li>\n</ul>\n<h3 id=\"From-RNNs-to-Transformers\"><a href=\"#From-RNNs-to-Transformers\" class=\"headerlink\" title=\"From RNNs to Transformers\"></a>From RNNs to Transformers</h3><p>在 2017 年以前，NLP 的研究任务的模型架构中 RNN 一直是主流，RNN 的模型架构如下图所示。他采用序列的方式去处理文本，即一个 token 一个 token 得去处理。</p>\n<p>但是这种架构存在着三种问题：</p>\n<ul>\n<li>序列处理文本时间消耗太高（你需要一个 token 一个 token 得去处理文本序列）</li>\n<li>模型感受野太小，这一点之前已经有人做过实验发现 RNN 的隐状态主要是受到邻近 token 的影响。</li>\n<li>模型只能从做向右单向处理文本（但是 Bi-LSTM 一定程度上解决了这个问题）</li>\n</ul>\n<p><img src=\"/images/All_Things_Vit/RNN.png\" alt=\"Figure 1: RNN 模型架构\"></p>\n<p>但是，有了 transformer 之后上述的三个问题都得到了很好的解决：</p>\n<ul>\n<li>序列处理 vs. 编码过程完全并行处理</li>\n<li>感受野太小 vs. 上下文从所有的 tokens 中同步获得</li>\n<li>单向的上下文 vs. 从整个文本序列中得到上下文</li>\n</ul>\n<p>但是 transformer 的伟大之处不仅仅是解决了上述问题。随着 transformer 的出现实现了各个领域模型的大一统！</p>\n<p><img src=\"/images/All_Things_Vit/B_transformer.png\" alt=\"Figure 2: transformer 出现前各领域主流模型架构\"><br><img src=\"/images/All_Things_Vit/A_transformer.png\" alt=\"Figure 3: transformer 出现后各领域主流模型架构\"></p>\n"},{"title":"Convolutional Neural Networks for Sentence Classification(TextCNN)","date":"2023-01-02T08:43:36.000Z","_content":"\n## 摘要\n\n我们报告了一系列的卷积神经网络实验（CNN），这些实验室都是基于预训练的词向量做句子级别的分类预测任务。我们展示了一个简单的 CNN 基于很少的超参数和静态向量在多个 benchmark 上取得了很好的结果。通过微调学习具体任务的向量表示能够得到更好的性能。我们对架构进行了简单的修改使其能够支持具体任务的向量表示和静态向量表示。我们通过 CNN 模型在 4 个任务上取得了 sota （一共测试了 7 个任务），其中包括情感分析和问题分类。\n\n<!--more-->\n\n## 引言\n\n在这篇文章发表之前 CNN 模型就已经在图像领域和语音识别领域取得了非常好的成效。在过去的一些 NLP 的工作也有一些使用了深度学习的方法，用于将词向量的表示（将 V 个单词通过隐藏层编码到低维空间，其中 V 为词表大小，而隐藏层本质上是对单词进行特征提取）。通过这样的表示，语义接近的单词更可能在低维的向量空间中有更近的距离表示可以是欧几里得距离也可以是余弦距离。\n\n在第二段作者又对 CNN 进行了介绍，结构非常清晰。在此篇工作之前，CNN 主要用于图像处理、语音识别以及一些传统的 NLP 任务。\n\n第三段中，作者对于自己的工作进行了简要介绍：我们对于 Google News 数据集的单词使用了一个简单的 CNN 结构去得到他们的词向量。我们首先使用这些静态的词向量只对模型的一些超参数进行微调，我们的结果表明这种结构只需要很少的超参数微调操作就能够在多个 benchmark 上取得很好的结果。这个结果表明我们的 CNN 结构词向量表示方法能够进行通用特征抽取，这些特征能够用于多种不同的分类任务。而通过基于结果进行微调学习具体任务的向量表示能够进一步提升效果。所以我们最后对我们的模型结构进行了简单的更新是他能够同时使用预训练的词向量和具体任务的词向量表示。\n\n## 2 模型\n\n我们使用 $x_i\\in \\mathbb{R}^k$ 表示句子中第 i 个单词的 k 维向量。长度为 n 的句子（因为句子长度可变，所以q需要使用填充）表示为：\n$$\n\\begin{equation}\nx_{1:n} = x_1 \\oplus x_2 \\oplus ... \\oplus x_n,\n\\end{equation}\n$$\n\n其中 $\\oplus$ 表示连结操作符。$x_{i:i+j}$ 表示单词$x_i, x_i+1, ..., x_{i+j}$的连接。卷积操作涉及到一个滤波器 $w \\in \\mathbb{R}^{hk}$，这个滤波器用于对窗口大小 h 内的单词构建新的特征。比如，一个特征 $c_i$ 由单词窗口 $x_{i:i+h-1}$ 生成：\n\n$$\n\\begin{equation}\nc_i = f(w \\cdot x_{i:i+h-1} + b).\n\\end{equation}\n$$\n\n$b \\in \\mathbb{R}$ 是一个偏置项而 $f$ 是一个非线性的激活函数（文中作者使用的是双向正切函数，tanh）。这个滤波器将对句子中所有窗口${x_{1:h}, x_{2:h+1}, ..., x_{n-h+1}}$生成一个特征映射：\n$$\n\\begin{equation}\n\\bold{c} = [c_1, c_2, ..., c_n-h+1],\n\\end{equation}\n$$\n\n其中 $c \\in \\mathbb{R}^{n-h+1}$。我们接下来使用一个最大池化操作基于特征映射选取最大值 $\\hat{c} = max{\\bold{c}}$。这个操作的目的是捕获到最重要的特征。这种池化模式能够很自然地用于处理可变的句子长度。\n\n上面介绍了单滤波器构建单特征的过程。实际上，这个模型使用了多个滤波器（不同的窗口大小）构建了多个特征。这些特征构成了倒数第二层，最后一层是一个全连接层加上 softmax 输出每个类别的概率。\n\n作为模型的一个变体，我们实验了双通道的词向量——一个是训练好的静态向量和一个通过反向传播微调的向量（3.2 章节中会详细介绍）。多通道架构，如图1所示，每个通道共用一套滤波器。\n\n![Figure 1:双通道样例结构](/images/TextCNN/fig1.png)\n\n### 2.1 正则化\n\n这里作者主要介绍了 dropout 的用法和原理，dropout 现在已经相当成熟了这里就不过多介绍了。\n\n## 3 数据集和实验配置\n\n作者使用的 benchmark 在表 1 中进行了总结\n\n![Table 1:令牌化后的数据集统计数据](/images/TextCNN/table1.png)\n\n### 3.1 超参数和训练\n\n一些常规的超参数和实验数据集配置的介绍，这里不过多介绍了。\n\n### 3.2 预训练的词向量\n\n这里其实就是使用了 word2vec 基于 Google News 的预训练结果\n\n### 3.3 模型变种\n\n有点类似消融实验，这里作者给出了四种模型变种：\n\n- CNN-rand：baseline 模型，所有的词向量是随机初始化的再通过训练进行更新\n\n- CNN-static：基于 word2vec 预训练词向量的模型。所有的单词不在训练过程中修改，训练过程中只更新模型学习到的参数\n\n- CNN-non-static：与上一个模型相同，但是预训练向量会基于每一个任务微调\n\n- CNN-multichannel：即图 1 介绍的模型结构\n\n## 实验结果和讨论\n\n实验结果如表 2 所示\n![](/images/TextCNN/table1.png)\n\n","source":"_posts/Convolutional-Neural-Networks-for-Sentence-Classification-TextCNN.md","raw":"---\ntitle: Convolutional Neural Networks for Sentence Classification(TextCNN)\ndate: 2023-01-02 16:43:36\ntags:\n---\n\n## 摘要\n\n我们报告了一系列的卷积神经网络实验（CNN），这些实验室都是基于预训练的词向量做句子级别的分类预测任务。我们展示了一个简单的 CNN 基于很少的超参数和静态向量在多个 benchmark 上取得了很好的结果。通过微调学习具体任务的向量表示能够得到更好的性能。我们对架构进行了简单的修改使其能够支持具体任务的向量表示和静态向量表示。我们通过 CNN 模型在 4 个任务上取得了 sota （一共测试了 7 个任务），其中包括情感分析和问题分类。\n\n<!--more-->\n\n## 引言\n\n在这篇文章发表之前 CNN 模型就已经在图像领域和语音识别领域取得了非常好的成效。在过去的一些 NLP 的工作也有一些使用了深度学习的方法，用于将词向量的表示（将 V 个单词通过隐藏层编码到低维空间，其中 V 为词表大小，而隐藏层本质上是对单词进行特征提取）。通过这样的表示，语义接近的单词更可能在低维的向量空间中有更近的距离表示可以是欧几里得距离也可以是余弦距离。\n\n在第二段作者又对 CNN 进行了介绍，结构非常清晰。在此篇工作之前，CNN 主要用于图像处理、语音识别以及一些传统的 NLP 任务。\n\n第三段中，作者对于自己的工作进行了简要介绍：我们对于 Google News 数据集的单词使用了一个简单的 CNN 结构去得到他们的词向量。我们首先使用这些静态的词向量只对模型的一些超参数进行微调，我们的结果表明这种结构只需要很少的超参数微调操作就能够在多个 benchmark 上取得很好的结果。这个结果表明我们的 CNN 结构词向量表示方法能够进行通用特征抽取，这些特征能够用于多种不同的分类任务。而通过基于结果进行微调学习具体任务的向量表示能够进一步提升效果。所以我们最后对我们的模型结构进行了简单的更新是他能够同时使用预训练的词向量和具体任务的词向量表示。\n\n## 2 模型\n\n我们使用 $x_i\\in \\mathbb{R}^k$ 表示句子中第 i 个单词的 k 维向量。长度为 n 的句子（因为句子长度可变，所以q需要使用填充）表示为：\n$$\n\\begin{equation}\nx_{1:n} = x_1 \\oplus x_2 \\oplus ... \\oplus x_n,\n\\end{equation}\n$$\n\n其中 $\\oplus$ 表示连结操作符。$x_{i:i+j}$ 表示单词$x_i, x_i+1, ..., x_{i+j}$的连接。卷积操作涉及到一个滤波器 $w \\in \\mathbb{R}^{hk}$，这个滤波器用于对窗口大小 h 内的单词构建新的特征。比如，一个特征 $c_i$ 由单词窗口 $x_{i:i+h-1}$ 生成：\n\n$$\n\\begin{equation}\nc_i = f(w \\cdot x_{i:i+h-1} + b).\n\\end{equation}\n$$\n\n$b \\in \\mathbb{R}$ 是一个偏置项而 $f$ 是一个非线性的激活函数（文中作者使用的是双向正切函数，tanh）。这个滤波器将对句子中所有窗口${x_{1:h}, x_{2:h+1}, ..., x_{n-h+1}}$生成一个特征映射：\n$$\n\\begin{equation}\n\\bold{c} = [c_1, c_2, ..., c_n-h+1],\n\\end{equation}\n$$\n\n其中 $c \\in \\mathbb{R}^{n-h+1}$。我们接下来使用一个最大池化操作基于特征映射选取最大值 $\\hat{c} = max{\\bold{c}}$。这个操作的目的是捕获到最重要的特征。这种池化模式能够很自然地用于处理可变的句子长度。\n\n上面介绍了单滤波器构建单特征的过程。实际上，这个模型使用了多个滤波器（不同的窗口大小）构建了多个特征。这些特征构成了倒数第二层，最后一层是一个全连接层加上 softmax 输出每个类别的概率。\n\n作为模型的一个变体，我们实验了双通道的词向量——一个是训练好的静态向量和一个通过反向传播微调的向量（3.2 章节中会详细介绍）。多通道架构，如图1所示，每个通道共用一套滤波器。\n\n![Figure 1:双通道样例结构](/images/TextCNN/fig1.png)\n\n### 2.1 正则化\n\n这里作者主要介绍了 dropout 的用法和原理，dropout 现在已经相当成熟了这里就不过多介绍了。\n\n## 3 数据集和实验配置\n\n作者使用的 benchmark 在表 1 中进行了总结\n\n![Table 1:令牌化后的数据集统计数据](/images/TextCNN/table1.png)\n\n### 3.1 超参数和训练\n\n一些常规的超参数和实验数据集配置的介绍，这里不过多介绍了。\n\n### 3.2 预训练的词向量\n\n这里其实就是使用了 word2vec 基于 Google News 的预训练结果\n\n### 3.3 模型变种\n\n有点类似消融实验，这里作者给出了四种模型变种：\n\n- CNN-rand：baseline 模型，所有的词向量是随机初始化的再通过训练进行更新\n\n- CNN-static：基于 word2vec 预训练词向量的模型。所有的单词不在训练过程中修改，训练过程中只更新模型学习到的参数\n\n- CNN-non-static：与上一个模型相同，但是预训练向量会基于每一个任务微调\n\n- CNN-multichannel：即图 1 介绍的模型结构\n\n## 实验结果和讨论\n\n实验结果如表 2 所示\n![](/images/TextCNN/table1.png)\n\n","slug":"Convolutional-Neural-Networks-for-Sentence-Classification-TextCNN","published":1,"updated":"2023-01-11T12:42:27.336Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kox0004qnwy9vgveowa","content":"<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>我们报告了一系列的卷积神经网络实验（CNN），这些实验室都是基于预训练的词向量做句子级别的分类预测任务。我们展示了一个简单的 CNN 基于很少的超参数和静态向量在多个 benchmark 上取得了很好的结果。通过微调学习具体任务的向量表示能够得到更好的性能。我们对架构进行了简单的修改使其能够支持具体任务的向量表示和静态向量表示。我们通过 CNN 模型在 4 个任务上取得了 sota （一共测试了 7 个任务），其中包括情感分析和问题分类。</p>\n<span id=\"more\"></span>\n\n<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p>在这篇文章发表之前 CNN 模型就已经在图像领域和语音识别领域取得了非常好的成效。在过去的一些 NLP 的工作也有一些使用了深度学习的方法，用于将词向量的表示（将 V 个单词通过隐藏层编码到低维空间，其中 V 为词表大小，而隐藏层本质上是对单词进行特征提取）。通过这样的表示，语义接近的单词更可能在低维的向量空间中有更近的距离表示可以是欧几里得距离也可以是余弦距离。</p>\n<p>在第二段作者又对 CNN 进行了介绍，结构非常清晰。在此篇工作之前，CNN 主要用于图像处理、语音识别以及一些传统的 NLP 任务。</p>\n<p>第三段中，作者对于自己的工作进行了简要介绍：我们对于 Google News 数据集的单词使用了一个简单的 CNN 结构去得到他们的词向量。我们首先使用这些静态的词向量只对模型的一些超参数进行微调，我们的结果表明这种结构只需要很少的超参数微调操作就能够在多个 benchmark 上取得很好的结果。这个结果表明我们的 CNN 结构词向量表示方法能够进行通用特征抽取，这些特征能够用于多种不同的分类任务。而通过基于结果进行微调学习具体任务的向量表示能够进一步提升效果。所以我们最后对我们的模型结构进行了简单的更新是他能够同时使用预训练的词向量和具体任务的词向量表示。</p>\n<h2 id=\"2-模型\"><a href=\"#2-模型\" class=\"headerlink\" title=\"2 模型\"></a>2 模型</h2><p>我们使用 $x_i\\in \\mathbb{R}^k$ 表示句子中第 i 个单词的 k 维向量。长度为 n 的句子（因为句子长度可变，所以q需要使用填充）表示为：<br>$$<br>\\begin{equation}<br>x_{1:n} &#x3D; x_1 \\oplus x_2 \\oplus … \\oplus x_n,<br>\\end{equation}<br>$$</p>\n<p>其中 $\\oplus$ 表示连结操作符。$x_{i:i+j}$ 表示单词$x_i, x_i+1, …, x_{i+j}$的连接。卷积操作涉及到一个滤波器 $w \\in \\mathbb{R}^{hk}$，这个滤波器用于对窗口大小 h 内的单词构建新的特征。比如，一个特征 $c_i$ 由单词窗口 $x_{i:i+h-1}$ 生成：</p>\n<p>$$<br>\\begin{equation}<br>c_i &#x3D; f(w \\cdot x_{i:i+h-1} + b).<br>\\end{equation}<br>$$</p>\n<p>$b \\in \\mathbb{R}$ 是一个偏置项而 $f$ 是一个非线性的激活函数（文中作者使用的是双向正切函数，tanh）。这个滤波器将对句子中所有窗口${x_{1:h}, x_{2:h+1}, …, x_{n-h+1}}$生成一个特征映射：<br>$$<br>\\begin{equation}<br>\\bold{c} &#x3D; [c_1, c_2, …, c_n-h+1],<br>\\end{equation}<br>$$</p>\n<p>其中 $c \\in \\mathbb{R}^{n-h+1}$。我们接下来使用一个最大池化操作基于特征映射选取最大值 $\\hat{c} &#x3D; max{\\bold{c}}$。这个操作的目的是捕获到最重要的特征。这种池化模式能够很自然地用于处理可变的句子长度。</p>\n<p>上面介绍了单滤波器构建单特征的过程。实际上，这个模型使用了多个滤波器（不同的窗口大小）构建了多个特征。这些特征构成了倒数第二层，最后一层是一个全连接层加上 softmax 输出每个类别的概率。</p>\n<p>作为模型的一个变体，我们实验了双通道的词向量——一个是训练好的静态向量和一个通过反向传播微调的向量（3.2 章节中会详细介绍）。多通道架构，如图1所示，每个通道共用一套滤波器。</p>\n<p><img src=\"/images/TextCNN/fig1.png\" alt=\"Figure 1:双通道样例结构\"></p>\n<h3 id=\"2-1-正则化\"><a href=\"#2-1-正则化\" class=\"headerlink\" title=\"2.1 正则化\"></a>2.1 正则化</h3><p>这里作者主要介绍了 dropout 的用法和原理，dropout 现在已经相当成熟了这里就不过多介绍了。</p>\n<h2 id=\"3-数据集和实验配置\"><a href=\"#3-数据集和实验配置\" class=\"headerlink\" title=\"3 数据集和实验配置\"></a>3 数据集和实验配置</h2><p>作者使用的 benchmark 在表 1 中进行了总结</p>\n<p><img src=\"/images/TextCNN/table1.png\" alt=\"Table 1:令牌化后的数据集统计数据\"></p>\n<h3 id=\"3-1-超参数和训练\"><a href=\"#3-1-超参数和训练\" class=\"headerlink\" title=\"3.1 超参数和训练\"></a>3.1 超参数和训练</h3><p>一些常规的超参数和实验数据集配置的介绍，这里不过多介绍了。</p>\n<h3 id=\"3-2-预训练的词向量\"><a href=\"#3-2-预训练的词向量\" class=\"headerlink\" title=\"3.2 预训练的词向量\"></a>3.2 预训练的词向量</h3><p>这里其实就是使用了 word2vec 基于 Google News 的预训练结果</p>\n<h3 id=\"3-3-模型变种\"><a href=\"#3-3-模型变种\" class=\"headerlink\" title=\"3.3 模型变种\"></a>3.3 模型变种</h3><p>有点类似消融实验，这里作者给出了四种模型变种：</p>\n<ul>\n<li><p>CNN-rand：baseline 模型，所有的词向量是随机初始化的再通过训练进行更新</p>\n</li>\n<li><p>CNN-static：基于 word2vec 预训练词向量的模型。所有的单词不在训练过程中修改，训练过程中只更新模型学习到的参数</p>\n</li>\n<li><p>CNN-non-static：与上一个模型相同，但是预训练向量会基于每一个任务微调</p>\n</li>\n<li><p>CNN-multichannel：即图 1 介绍的模型结构</p>\n</li>\n</ul>\n<h2 id=\"实验结果和讨论\"><a href=\"#实验结果和讨论\" class=\"headerlink\" title=\"实验结果和讨论\"></a>实验结果和讨论</h2><p>实验结果如表 2 所示<br><img src=\"/images/TextCNN/table1.png\"></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>我们报告了一系列的卷积神经网络实验（CNN），这些实验室都是基于预训练的词向量做句子级别的分类预测任务。我们展示了一个简单的 CNN 基于很少的超参数和静态向量在多个 benchmark 上取得了很好的结果。通过微调学习具体任务的向量表示能够得到更好的性能。我们对架构进行了简单的修改使其能够支持具体任务的向量表示和静态向量表示。我们通过 CNN 模型在 4 个任务上取得了 sota （一共测试了 7 个任务），其中包括情感分析和问题分类。</p>","more":"<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p>在这篇文章发表之前 CNN 模型就已经在图像领域和语音识别领域取得了非常好的成效。在过去的一些 NLP 的工作也有一些使用了深度学习的方法，用于将词向量的表示（将 V 个单词通过隐藏层编码到低维空间，其中 V 为词表大小，而隐藏层本质上是对单词进行特征提取）。通过这样的表示，语义接近的单词更可能在低维的向量空间中有更近的距离表示可以是欧几里得距离也可以是余弦距离。</p>\n<p>在第二段作者又对 CNN 进行了介绍，结构非常清晰。在此篇工作之前，CNN 主要用于图像处理、语音识别以及一些传统的 NLP 任务。</p>\n<p>第三段中，作者对于自己的工作进行了简要介绍：我们对于 Google News 数据集的单词使用了一个简单的 CNN 结构去得到他们的词向量。我们首先使用这些静态的词向量只对模型的一些超参数进行微调，我们的结果表明这种结构只需要很少的超参数微调操作就能够在多个 benchmark 上取得很好的结果。这个结果表明我们的 CNN 结构词向量表示方法能够进行通用特征抽取，这些特征能够用于多种不同的分类任务。而通过基于结果进行微调学习具体任务的向量表示能够进一步提升效果。所以我们最后对我们的模型结构进行了简单的更新是他能够同时使用预训练的词向量和具体任务的词向量表示。</p>\n<h2 id=\"2-模型\"><a href=\"#2-模型\" class=\"headerlink\" title=\"2 模型\"></a>2 模型</h2><p>我们使用 $x_i\\in \\mathbb{R}^k$ 表示句子中第 i 个单词的 k 维向量。长度为 n 的句子（因为句子长度可变，所以q需要使用填充）表示为：<br>$$<br>\\begin{equation}<br>x_{1:n} &#x3D; x_1 \\oplus x_2 \\oplus … \\oplus x_n,<br>\\end{equation}<br>$$</p>\n<p>其中 $\\oplus$ 表示连结操作符。$x_{i:i+j}$ 表示单词$x_i, x_i+1, …, x_{i+j}$的连接。卷积操作涉及到一个滤波器 $w \\in \\mathbb{R}^{hk}$，这个滤波器用于对窗口大小 h 内的单词构建新的特征。比如，一个特征 $c_i$ 由单词窗口 $x_{i:i+h-1}$ 生成：</p>\n<p>$$<br>\\begin{equation}<br>c_i &#x3D; f(w \\cdot x_{i:i+h-1} + b).<br>\\end{equation}<br>$$</p>\n<p>$b \\in \\mathbb{R}$ 是一个偏置项而 $f$ 是一个非线性的激活函数（文中作者使用的是双向正切函数，tanh）。这个滤波器将对句子中所有窗口${x_{1:h}, x_{2:h+1}, …, x_{n-h+1}}$生成一个特征映射：<br>$$<br>\\begin{equation}<br>\\bold{c} &#x3D; [c_1, c_2, …, c_n-h+1],<br>\\end{equation}<br>$$</p>\n<p>其中 $c \\in \\mathbb{R}^{n-h+1}$。我们接下来使用一个最大池化操作基于特征映射选取最大值 $\\hat{c} &#x3D; max{\\bold{c}}$。这个操作的目的是捕获到最重要的特征。这种池化模式能够很自然地用于处理可变的句子长度。</p>\n<p>上面介绍了单滤波器构建单特征的过程。实际上，这个模型使用了多个滤波器（不同的窗口大小）构建了多个特征。这些特征构成了倒数第二层，最后一层是一个全连接层加上 softmax 输出每个类别的概率。</p>\n<p>作为模型的一个变体，我们实验了双通道的词向量——一个是训练好的静态向量和一个通过反向传播微调的向量（3.2 章节中会详细介绍）。多通道架构，如图1所示，每个通道共用一套滤波器。</p>\n<p><img src=\"/images/TextCNN/fig1.png\" alt=\"Figure 1:双通道样例结构\"></p>\n<h3 id=\"2-1-正则化\"><a href=\"#2-1-正则化\" class=\"headerlink\" title=\"2.1 正则化\"></a>2.1 正则化</h3><p>这里作者主要介绍了 dropout 的用法和原理，dropout 现在已经相当成熟了这里就不过多介绍了。</p>\n<h2 id=\"3-数据集和实验配置\"><a href=\"#3-数据集和实验配置\" class=\"headerlink\" title=\"3 数据集和实验配置\"></a>3 数据集和实验配置</h2><p>作者使用的 benchmark 在表 1 中进行了总结</p>\n<p><img src=\"/images/TextCNN/table1.png\" alt=\"Table 1:令牌化后的数据集统计数据\"></p>\n<h3 id=\"3-1-超参数和训练\"><a href=\"#3-1-超参数和训练\" class=\"headerlink\" title=\"3.1 超参数和训练\"></a>3.1 超参数和训练</h3><p>一些常规的超参数和实验数据集配置的介绍，这里不过多介绍了。</p>\n<h3 id=\"3-2-预训练的词向量\"><a href=\"#3-2-预训练的词向量\" class=\"headerlink\" title=\"3.2 预训练的词向量\"></a>3.2 预训练的词向量</h3><p>这里其实就是使用了 word2vec 基于 Google News 的预训练结果</p>\n<h3 id=\"3-3-模型变种\"><a href=\"#3-3-模型变种\" class=\"headerlink\" title=\"3.3 模型变种\"></a>3.3 模型变种</h3><p>有点类似消融实验，这里作者给出了四种模型变种：</p>\n<ul>\n<li><p>CNN-rand：baseline 模型，所有的词向量是随机初始化的再通过训练进行更新</p>\n</li>\n<li><p>CNN-static：基于 word2vec 预训练词向量的模型。所有的单词不在训练过程中修改，训练过程中只更新模型学习到的参数</p>\n</li>\n<li><p>CNN-non-static：与上一个模型相同，但是预训练向量会基于每一个任务微调</p>\n</li>\n<li><p>CNN-multichannel：即图 1 介绍的模型结构</p>\n</li>\n</ul>\n<h2 id=\"实验结果和讨论\"><a href=\"#实验结果和讨论\" class=\"headerlink\" title=\"实验结果和讨论\"></a>实验结果和讨论</h2><p>实验结果如表 2 所示<br><img src=\"/images/TextCNN/table1.png\"></p>"},{"title":"DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning","date":"2022-12-20T08:55:04.000Z","_content":"\n## 摘要\n多图表的分析 dashboard 常用于为商业智能知识发现与洞察赋能。然而，创建一个高效的 dashborad 非常需要使用者有着扎实的数据分析基础并且非常熟悉专业的 dashboard 创作工具，比如 Power BI。用户需要在构建 dashboard 的过程中通过选择数据列配置图表并且探索不同图表之间的组合以更好地洞察数据背后的规律，这个过程就是一个试错的过程。最近的研究已经开始尝试通过深度学习的方法去减少创作成本。但是，这种方法非常依赖于大规模高质量的 dashboard 数据集。在这篇工作中，我们能够运用优秀的可视化知识创作的强化学习技术去构建 dashboard 并且评估了强化学习的能力。特别的，我们使用可视化的知识去构建了一个训练环境和奖励，并且通过一个精心设计的代理网络代替学习人类的探索行为。我们通过消融实验和用户案例验证了强化学习模型的有效性。总之，我们的工作为基于机器学习但缺失训练集的可视化工作打开了一个新的有效方向。\n\n## 引言\n传统的多视图分析可视化（multiple-view visualizations, MVs）需要借助于专业的分析工具和扎实的数据分析能力。而且，构建的过程是一个逐渐试错的过程。\n\n有许多研究尝试通过不同的方法去减少中间成本，主要分为两种基于规则的方法和基于机器学习的方法。基于规则的方法通过将优秀的可视设计规则转换成可编程的限制用于推荐。而基于机器学习的方法使用各种 sota 的模型比如树模型或者深度神经网络模型，从大规模的图表数据中去学习公共的模式。虽然能够生成非常好的图表，但是这些方法聚焦于单视图而不是多视图，但是图表之间的关系是非常重要的一环。\n\n面向多视图生成的方法：也大体分为两种，一种是手写规则的方法，通过统计一些指标来总结出设计模式，另一种是基于深度学习的方法，训练一个单目标的模型并通过自定义生成多视图的度量函数来生成多个图表。这种方法非常依赖高质量的数据集，否则就会影响训练过程和目标。\n\n在这篇文章中，我们提出了使用强化学习去生成分析 dashboard 的技术。我们认为通过强化学习有以下几个优势：\n1. 直觉建模。强化学习代理的通过从环境中探索学习的方式与探索可视分析的机制类似。所以，通过强化学习建模的过程就很自然。\n2. 自玩训练。通过精心设计环境，行为空间，评分函数，代理可以被不同的数据集持续训练，而不需要提前标签的数据集。\n3. 在线推荐。当用户选择完偏好后，强化学习代理会依据选择更新后续的提示。\n\n然而，为分析 dashboard 生成过程设计一个强化学习模型是一个具有挑战的过程：\n1. 缺少精心构建的可视化代理环境去探索和训练。不同于游戏训练环境可以简单的通过获胜失败去度量代理行为的好坏，在 dashboard 生成环境中没有精确的度量方式去决定生成结果的好坏。\n2. 设计一个代理去模仿人类在生成 dashboard 的过程中复杂的行为非常困难。因为在人类构建 dashboard 中会有通过多个参数配置图表的行为和探索大规模的图表连接空间的行为，但这些行为很难建模。\n\n为了解决这些问题，本文中我们开发了 DashBot，一个基于强化学习的分析 dashboard 的推荐系统。对于第一个挑战，我们调查了 \n\n本文的四个主要贡献：\n\n## 3 DASHBOT 的设计\n\n### 3.1 预先研究\n现有的可视化研究主要集中在可视化类型的研究。很少有关于 dashboard 设计模式的研究。所以，我们为了得到设计实践的一个概览开展了预先研究。\n\n我们从 Tableau 和 Power BI 中搜集了一些 dashboard 的模版。40 个来自于 Tableau 50 个来自于 PowerBI。\n\n然后，我们从视觉和数据呈现的方面两个角度分析了 dashboard。基于 MVs 的设计指南，一个好的设计应该遵循多样性、互补性、分解性和简约性四个原则。\n\n### 3.2 设计思考\n基于预先研究衍生出了 dashborad 推荐系统的设计思考。\n\nDC1 自动生成有效的 dashboard\n\n## 4 问题定义\n\n### 4.1 背景 马尔可夫决策过程\n\n{% pdf VIS2022.pptx %}","source":"_posts/DashBot-Insight-Driven-Dashboard-Generation-Based-on-Deep-Reinforcement-Learning.md","raw":"---\ntitle: >-\n  DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement\n  Learning\ndate: 2022-12-20 16:55:04\ntags:\n- [强化学习]\n- [可视化]\n---\n\n## 摘要\n多图表的分析 dashboard 常用于为商业智能知识发现与洞察赋能。然而，创建一个高效的 dashborad 非常需要使用者有着扎实的数据分析基础并且非常熟悉专业的 dashboard 创作工具，比如 Power BI。用户需要在构建 dashboard 的过程中通过选择数据列配置图表并且探索不同图表之间的组合以更好地洞察数据背后的规律，这个过程就是一个试错的过程。最近的研究已经开始尝试通过深度学习的方法去减少创作成本。但是，这种方法非常依赖于大规模高质量的 dashboard 数据集。在这篇工作中，我们能够运用优秀的可视化知识创作的强化学习技术去构建 dashboard 并且评估了强化学习的能力。特别的，我们使用可视化的知识去构建了一个训练环境和奖励，并且通过一个精心设计的代理网络代替学习人类的探索行为。我们通过消融实验和用户案例验证了强化学习模型的有效性。总之，我们的工作为基于机器学习但缺失训练集的可视化工作打开了一个新的有效方向。\n\n## 引言\n传统的多视图分析可视化（multiple-view visualizations, MVs）需要借助于专业的分析工具和扎实的数据分析能力。而且，构建的过程是一个逐渐试错的过程。\n\n有许多研究尝试通过不同的方法去减少中间成本，主要分为两种基于规则的方法和基于机器学习的方法。基于规则的方法通过将优秀的可视设计规则转换成可编程的限制用于推荐。而基于机器学习的方法使用各种 sota 的模型比如树模型或者深度神经网络模型，从大规模的图表数据中去学习公共的模式。虽然能够生成非常好的图表，但是这些方法聚焦于单视图而不是多视图，但是图表之间的关系是非常重要的一环。\n\n面向多视图生成的方法：也大体分为两种，一种是手写规则的方法，通过统计一些指标来总结出设计模式，另一种是基于深度学习的方法，训练一个单目标的模型并通过自定义生成多视图的度量函数来生成多个图表。这种方法非常依赖高质量的数据集，否则就会影响训练过程和目标。\n\n在这篇文章中，我们提出了使用强化学习去生成分析 dashboard 的技术。我们认为通过强化学习有以下几个优势：\n1. 直觉建模。强化学习代理的通过从环境中探索学习的方式与探索可视分析的机制类似。所以，通过强化学习建模的过程就很自然。\n2. 自玩训练。通过精心设计环境，行为空间，评分函数，代理可以被不同的数据集持续训练，而不需要提前标签的数据集。\n3. 在线推荐。当用户选择完偏好后，强化学习代理会依据选择更新后续的提示。\n\n然而，为分析 dashboard 生成过程设计一个强化学习模型是一个具有挑战的过程：\n1. 缺少精心构建的可视化代理环境去探索和训练。不同于游戏训练环境可以简单的通过获胜失败去度量代理行为的好坏，在 dashboard 生成环境中没有精确的度量方式去决定生成结果的好坏。\n2. 设计一个代理去模仿人类在生成 dashboard 的过程中复杂的行为非常困难。因为在人类构建 dashboard 中会有通过多个参数配置图表的行为和探索大规模的图表连接空间的行为，但这些行为很难建模。\n\n为了解决这些问题，本文中我们开发了 DashBot，一个基于强化学习的分析 dashboard 的推荐系统。对于第一个挑战，我们调查了 \n\n本文的四个主要贡献：\n\n## 3 DASHBOT 的设计\n\n### 3.1 预先研究\n现有的可视化研究主要集中在可视化类型的研究。很少有关于 dashboard 设计模式的研究。所以，我们为了得到设计实践的一个概览开展了预先研究。\n\n我们从 Tableau 和 Power BI 中搜集了一些 dashboard 的模版。40 个来自于 Tableau 50 个来自于 PowerBI。\n\n然后，我们从视觉和数据呈现的方面两个角度分析了 dashboard。基于 MVs 的设计指南，一个好的设计应该遵循多样性、互补性、分解性和简约性四个原则。\n\n### 3.2 设计思考\n基于预先研究衍生出了 dashborad 推荐系统的设计思考。\n\nDC1 自动生成有效的 dashboard\n\n## 4 问题定义\n\n### 4.1 背景 马尔可夫决策过程\n\n{% pdf VIS2022.pptx %}","slug":"DashBot-Insight-Driven-Dashboard-Generation-Based-on-Deep-Reinforcement-Learning","published":1,"updated":"2023-03-24T05:36:11.405Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7koy0005qnwy05sg611e","content":"<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>多图表的分析 dashboard 常用于为商业智能知识发现与洞察赋能。然而，创建一个高效的 dashborad 非常需要使用者有着扎实的数据分析基础并且非常熟悉专业的 dashboard 创作工具，比如 Power BI。用户需要在构建 dashboard 的过程中通过选择数据列配置图表并且探索不同图表之间的组合以更好地洞察数据背后的规律，这个过程就是一个试错的过程。最近的研究已经开始尝试通过深度学习的方法去减少创作成本。但是，这种方法非常依赖于大规模高质量的 dashboard 数据集。在这篇工作中，我们能够运用优秀的可视化知识创作的强化学习技术去构建 dashboard 并且评估了强化学习的能力。特别的，我们使用可视化的知识去构建了一个训练环境和奖励，并且通过一个精心设计的代理网络代替学习人类的探索行为。我们通过消融实验和用户案例验证了强化学习模型的有效性。总之，我们的工作为基于机器学习但缺失训练集的可视化工作打开了一个新的有效方向。</p>\n<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p>传统的多视图分析可视化（multiple-view visualizations, MVs）需要借助于专业的分析工具和扎实的数据分析能力。而且，构建的过程是一个逐渐试错的过程。</p>\n<p>有许多研究尝试通过不同的方法去减少中间成本，主要分为两种基于规则的方法和基于机器学习的方法。基于规则的方法通过将优秀的可视设计规则转换成可编程的限制用于推荐。而基于机器学习的方法使用各种 sota 的模型比如树模型或者深度神经网络模型，从大规模的图表数据中去学习公共的模式。虽然能够生成非常好的图表，但是这些方法聚焦于单视图而不是多视图，但是图表之间的关系是非常重要的一环。</p>\n<p>面向多视图生成的方法：也大体分为两种，一种是手写规则的方法，通过统计一些指标来总结出设计模式，另一种是基于深度学习的方法，训练一个单目标的模型并通过自定义生成多视图的度量函数来生成多个图表。这种方法非常依赖高质量的数据集，否则就会影响训练过程和目标。</p>\n<p>在这篇文章中，我们提出了使用强化学习去生成分析 dashboard 的技术。我们认为通过强化学习有以下几个优势：</p>\n<ol>\n<li>直觉建模。强化学习代理的通过从环境中探索学习的方式与探索可视分析的机制类似。所以，通过强化学习建模的过程就很自然。</li>\n<li>自玩训练。通过精心设计环境，行为空间，评分函数，代理可以被不同的数据集持续训练，而不需要提前标签的数据集。</li>\n<li>在线推荐。当用户选择完偏好后，强化学习代理会依据选择更新后续的提示。</li>\n</ol>\n<p>然而，为分析 dashboard 生成过程设计一个强化学习模型是一个具有挑战的过程：</p>\n<ol>\n<li>缺少精心构建的可视化代理环境去探索和训练。不同于游戏训练环境可以简单的通过获胜失败去度量代理行为的好坏，在 dashboard 生成环境中没有精确的度量方式去决定生成结果的好坏。</li>\n<li>设计一个代理去模仿人类在生成 dashboard 的过程中复杂的行为非常困难。因为在人类构建 dashboard 中会有通过多个参数配置图表的行为和探索大规模的图表连接空间的行为，但这些行为很难建模。</li>\n</ol>\n<p>为了解决这些问题，本文中我们开发了 DashBot，一个基于强化学习的分析 dashboard 的推荐系统。对于第一个挑战，我们调查了 </p>\n<p>本文的四个主要贡献：</p>\n<h2 id=\"3-DASHBOT-的设计\"><a href=\"#3-DASHBOT-的设计\" class=\"headerlink\" title=\"3 DASHBOT 的设计\"></a>3 DASHBOT 的设计</h2><h3 id=\"3-1-预先研究\"><a href=\"#3-1-预先研究\" class=\"headerlink\" title=\"3.1 预先研究\"></a>3.1 预先研究</h3><p>现有的可视化研究主要集中在可视化类型的研究。很少有关于 dashboard 设计模式的研究。所以，我们为了得到设计实践的一个概览开展了预先研究。</p>\n<p>我们从 Tableau 和 Power BI 中搜集了一些 dashboard 的模版。40 个来自于 Tableau 50 个来自于 PowerBI。</p>\n<p>然后，我们从视觉和数据呈现的方面两个角度分析了 dashboard。基于 MVs 的设计指南，一个好的设计应该遵循多样性、互补性、分解性和简约性四个原则。</p>\n<h3 id=\"3-2-设计思考\"><a href=\"#3-2-设计思考\" class=\"headerlink\" title=\"3.2 设计思考\"></a>3.2 设计思考</h3><p>基于预先研究衍生出了 dashborad 推荐系统的设计思考。</p>\n<p>DC1 自动生成有效的 dashboard</p>\n<h2 id=\"4-问题定义\"><a href=\"#4-问题定义\" class=\"headerlink\" title=\"4 问题定义\"></a>4 问题定义</h2><h3 id=\"4-1-背景-马尔可夫决策过程\"><a href=\"#4-1-背景-马尔可夫决策过程\" class=\"headerlink\" title=\"4.1 背景 马尔可夫决策过程\"></a>4.1 背景 马尔可夫决策过程</h3>\n\n\t<div class=\"row\">\n    <embed src=\"VIS2022.pptx\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>多图表的分析 dashboard 常用于为商业智能知识发现与洞察赋能。然而，创建一个高效的 dashborad 非常需要使用者有着扎实的数据分析基础并且非常熟悉专业的 dashboard 创作工具，比如 Power BI。用户需要在构建 dashboard 的过程中通过选择数据列配置图表并且探索不同图表之间的组合以更好地洞察数据背后的规律，这个过程就是一个试错的过程。最近的研究已经开始尝试通过深度学习的方法去减少创作成本。但是，这种方法非常依赖于大规模高质量的 dashboard 数据集。在这篇工作中，我们能够运用优秀的可视化知识创作的强化学习技术去构建 dashboard 并且评估了强化学习的能力。特别的，我们使用可视化的知识去构建了一个训练环境和奖励，并且通过一个精心设计的代理网络代替学习人类的探索行为。我们通过消融实验和用户案例验证了强化学习模型的有效性。总之，我们的工作为基于机器学习但缺失训练集的可视化工作打开了一个新的有效方向。</p>\n<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p>传统的多视图分析可视化（multiple-view visualizations, MVs）需要借助于专业的分析工具和扎实的数据分析能力。而且，构建的过程是一个逐渐试错的过程。</p>\n<p>有许多研究尝试通过不同的方法去减少中间成本，主要分为两种基于规则的方法和基于机器学习的方法。基于规则的方法通过将优秀的可视设计规则转换成可编程的限制用于推荐。而基于机器学习的方法使用各种 sota 的模型比如树模型或者深度神经网络模型，从大规模的图表数据中去学习公共的模式。虽然能够生成非常好的图表，但是这些方法聚焦于单视图而不是多视图，但是图表之间的关系是非常重要的一环。</p>\n<p>面向多视图生成的方法：也大体分为两种，一种是手写规则的方法，通过统计一些指标来总结出设计模式，另一种是基于深度学习的方法，训练一个单目标的模型并通过自定义生成多视图的度量函数来生成多个图表。这种方法非常依赖高质量的数据集，否则就会影响训练过程和目标。</p>\n<p>在这篇文章中，我们提出了使用强化学习去生成分析 dashboard 的技术。我们认为通过强化学习有以下几个优势：</p>\n<ol>\n<li>直觉建模。强化学习代理的通过从环境中探索学习的方式与探索可视分析的机制类似。所以，通过强化学习建模的过程就很自然。</li>\n<li>自玩训练。通过精心设计环境，行为空间，评分函数，代理可以被不同的数据集持续训练，而不需要提前标签的数据集。</li>\n<li>在线推荐。当用户选择完偏好后，强化学习代理会依据选择更新后续的提示。</li>\n</ol>\n<p>然而，为分析 dashboard 生成过程设计一个强化学习模型是一个具有挑战的过程：</p>\n<ol>\n<li>缺少精心构建的可视化代理环境去探索和训练。不同于游戏训练环境可以简单的通过获胜失败去度量代理行为的好坏，在 dashboard 生成环境中没有精确的度量方式去决定生成结果的好坏。</li>\n<li>设计一个代理去模仿人类在生成 dashboard 的过程中复杂的行为非常困难。因为在人类构建 dashboard 中会有通过多个参数配置图表的行为和探索大规模的图表连接空间的行为，但这些行为很难建模。</li>\n</ol>\n<p>为了解决这些问题，本文中我们开发了 DashBot，一个基于强化学习的分析 dashboard 的推荐系统。对于第一个挑战，我们调查了 </p>\n<p>本文的四个主要贡献：</p>\n<h2 id=\"3-DASHBOT-的设计\"><a href=\"#3-DASHBOT-的设计\" class=\"headerlink\" title=\"3 DASHBOT 的设计\"></a>3 DASHBOT 的设计</h2><h3 id=\"3-1-预先研究\"><a href=\"#3-1-预先研究\" class=\"headerlink\" title=\"3.1 预先研究\"></a>3.1 预先研究</h3><p>现有的可视化研究主要集中在可视化类型的研究。很少有关于 dashboard 设计模式的研究。所以，我们为了得到设计实践的一个概览开展了预先研究。</p>\n<p>我们从 Tableau 和 Power BI 中搜集了一些 dashboard 的模版。40 个来自于 Tableau 50 个来自于 PowerBI。</p>\n<p>然后，我们从视觉和数据呈现的方面两个角度分析了 dashboard。基于 MVs 的设计指南，一个好的设计应该遵循多样性、互补性、分解性和简约性四个原则。</p>\n<h3 id=\"3-2-设计思考\"><a href=\"#3-2-设计思考\" class=\"headerlink\" title=\"3.2 设计思考\"></a>3.2 设计思考</h3><p>基于预先研究衍生出了 dashborad 推荐系统的设计思考。</p>\n<p>DC1 自动生成有效的 dashboard</p>\n<h2 id=\"4-问题定义\"><a href=\"#4-问题定义\" class=\"headerlink\" title=\"4 问题定义\"></a>4 问题定义</h2><h3 id=\"4-1-背景-马尔可夫决策过程\"><a href=\"#4-1-背景-马尔可夫决策过程\" class=\"headerlink\" title=\"4.1 背景 马尔可夫决策过程\"></a>4.1 背景 马尔可夫决策过程</h3>\n\n\t<div class=\"row\">\n    <embed src=\"VIS2022.pptx\" width=\"100%\" height=\"550\" type=\"application/pdf\">\n\t</div>\n\n\n"},{"title":"Target Encoding: 算法比赛提分利器","date":"2023-01-17T02:08:48.000Z","_content":"\nhttps://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69\nhttps://www.kaggle.com/code/ryanholbrook/target-encoding","source":"_posts/Target-Encoding-算法比赛提分利器.md","raw":"---\ntitle: 'Target Encoding: 算法比赛提分利器'\ndate: 2023-01-17 10:08:48\ntags:\n---\n\nhttps://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69\nhttps://www.kaggle.com/code/ryanholbrook/target-encoding","slug":"Target-Encoding-算法比赛提分利器","published":1,"updated":"2023-02-08T13:56:39.374Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7koy0006qnwyde0h843p","content":"<p><a href=\"https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69\">https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69</a><br><a href=\"https://www.kaggle.com/code/ryanholbrook/target-encoding\">https://www.kaggle.com/code/ryanholbrook/target-encoding</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69\">https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69</a><br><a href=\"https://www.kaggle.com/code/ryanholbrook/target-encoding\">https://www.kaggle.com/code/ryanholbrook/target-encoding</a></p>\n"},{"title":"Generative Adversarial Nets(GAN) 论文精读","date":"2022-11-23T02:41:55.000Z","_content":"\nGAN 自提出之后近几年一直是一个充满热度的话题，而这篇论文的一作也是大名鼎鼎的花书的作者。\n\n## 摘要\n\n我们提出了一个通过对抗过程估计生成模型的新框架，其中我们同时训练两个模型：一个生成模型 G 捕获数据分布，另一个判别模型 D 估计样本来自训练数据而不是由 G 生成的概率。 G 的训练过程是最大化 D 出错的概率。该框架对应于极小极大两人博弈。在任意函数 G 和 D 的空间中，存在一个唯一解，G 恢复训练数据分布，D 处处等于 $\\frac{1}{2}$。在 G 和 D 由多层感知器定义的情况下，整个系统可以通过反向传播进行训练。在训练或生成样本期间，不需要任何马尔可夫链或展开的近似推理网络。实验通过对生成样本的定性和定量评估来证明该框架的潜力。\n\n## 引言\n\n在过去深度学习模型在判别模型上取得了显著的成功，但是在生成模型上受制于逼近最大似然估计等策略使得生成模型难以复现并借鉴判别模型的成功经验。在本文中，作者提出了一种新的生成模型框架规避了这些问题。\n\n作者举了一个非常形象的例子介绍生成模型与判别模型：生成模型可以被认为是造假币的团队，并且试图制造假币并且在不被发现的情况下使用它，而判别模型类似于警察，试图检测假币。在对抗过程中驱使这两个模型不断改进他们的方法，直到假币和真币无法区分。\n\n通过这种框架能够基于反向传播算法和 dropout 训练这两个模型即这两个 MLP。并不需要近似推理或者马尔可夫链。\n\n## 相关工作\n\n4相关工作\n\n﻿\n10:42\n﻿\n\n版本：NIPS final version，arxiv 早期版本\n\n相关工作 的 区别，未参考别人工作，大佬！高级高级！\n\n\n\nRelated work 第一段：先说别人的问题，我们怎么做？\n\n\n\n别人的问题在哪里呢？\n\n构造 有参数的 概率分布函数，provide a parametric specification of a probability distribution function \n\n\n\n参数构造了，怎么学习呢？\n\nmaximizing the log-likelihood，但采样分布计算难， esp 维度高， i.e., DBM。\n\n\n\n怎么解决这个计算难的问题？当然是不要 分布 啦\n\n直接学习一个模型，近似数据分布。\n\n\n\nDBM 和 我们的 generative models 有什么区别？\n\nDBM：一定要学 分布，知道均值、方差等一系列参数\n\nGAN：用一个模型学习 你想要的结果，答对题就行。\n\n\n\nGAN 的好处和坏处是什么呢？\n\npros: 计算容易；\n               \n\ncons: 即使答对了题，GAN 不知道正确的分布长什么样。\n\n\nRelated work 第二段：误差反向传递对 GAN 进行求解\n\n\n\nWhy error BP 有效？\n\n对 f 的期望求导 == 对 F 自己求导\n\n\n\n\n\nRelated work 第三段：unaware 别人的类似工作，VAE\n\n\n\nRelated work 第四段：从 discriminative 角度 to train a generative model 的相关工作\n\n\n\nGAN 和 Noise-contrastive estimation (NCE) 的区别？\n\nNCE 的损失函数复杂一点 --> 求解性能不如 GAN\n\n\n\nRelated work 第五段：和 predictability minimization 1992的区别\n\n\n\nJürgen (LSTM作者) 有非常超前的工作，LSTM在当年的计算量和数据量的条件，很难大展身手。现在，LSTM 效果被发现了。\n\n\n\nJürgen predictability minimization算法被埋没了，Jürgen 认为 GAN 是 reverse PM。\n\n\n\n轶事：Jürgen 在GAN 的 NIPS 汇报，也提出质疑。Ian 回绝，邮件交流已经很清楚了，你在耽搁大家学习 GAN\n\n\n\nRemark：一个真实有用的技术，会在不同领域、不断的被人重新发现，给予新的名词。大家会把功劳归给那一个教会了大家这个算法的人，而不是最早发明它的人。\n\n\n\n\n\nRelated work 第六段：和 adversarial examples 的区别\n\n﻿\n15:25\n﻿\n\nadversarial examples 测试算法的稳定性，how？\n\n构造一些假的、跟真的样本长得很像的 样本，能糊弄到分类器。\n\n\n## GAN 的目标函数\n\n生成器的目标是学习训练数据 $x$ 的数据分布情况，生成器的概率分布用 $p_g$ 表示。生成器的输入是一个噪声分布 $p_z(z)$，然后将其映射到数据空间表示为 $G(z;\\theta_g)$，其中 $G$ 是一个可微的函数，由参数为 $\\theta_g$ 的多层感知机表示。输出单个标量的判别模型被表示为$D(x;\\theta_d)$。$D(x)$ 表示 $x$ 来自于数据分布而不是 $p_g$ 的概率。我们训练 $D$ 以最大化将正确标签分配给来自 $G$ 的样本的概率。我们同时训练 $G$ 以最小化 $log(1-D(G(z)))$。换句话说，这个过程可以被视为 $G$ 和 $D$ 在玩具有价值函数 $V(G, D)$ 的两人极小极大游戏：\n\n$\\mathop{min}\\limits_{G}\\mathop{max}\\limits_{D}V(D, G) = \\mathbb{E}_{x \\sim p_{data(x)}}[logD(x)] + \\mathbb{E}_{z \\sim p_{z(z)}}[logD(1 - logD(G(z)))]$\n\n在训练的过程中优化 $D$ 非常容易过拟合而且计算量很大。所以通过超参数 $k$ 控制 $D$ 的优化，并且优化 $D$ 的 $k$ 步和优化 $G$ 的一步交替进行。\n\n\n![](/images/GAN/GAN.png)\n\n上图中蓝色的虚线表示判别模型的分布情况，绿色的实现表示生成模型的分布，黑色的点线表示真实数据的分布。下方的 $z$ 轴表示均匀采样的噪声点，向上的线表示映射 $x = G(z)$ 即如果对噪声点施加生成模型的分布 $p_g$。(a) 表示奖将近收敛的对抗模型情况：$p_g$与 $p_{data}$ 相似，$D$ 是准确的分类器(b)$D$模型收敛到$D(x) = \\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}$。(c)在更新$G$ 后，$D$的梯度引导$G(z)$流入更有可能被归类为数据分布的区域。(d)经过几个步骤的训练，如果 $G$ 和 $D$ 有足够的能力，他们将达到一个目标点，此时 $p_g=p_{data}$。鉴别器无法分类两个分布$D(x)=\\frac{1}{2}$\n\n## 理论推导\n\n### 全局的最优目标是 $p_g = p_{data}$\n\nProposition: 我们首先定义对于给定的 $G$，判别器 $D$ 可以被表示为 $D^*_{G}(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$\n\nProof:不考虑生成器 $G$的情况下，判别器$D$的训练目标是最大化 $V(G, D)$\n\n$V(D, G) = \\int_xp_{data}(x)logD(x) dx + \\int_z p_z(z)log(1 - D(G(z)))dz\n\\\\=\\int_xp_{data}(x)logD(x)  +  p_g(x)log(1 - D(x))dx$\n\n现在给出上式的解释，首先第一步就是将期望转换为积分的形式；第二步，因为我们这里考虑的是任意生成器 $G$ 所以可以使用换元替换。上式的最终形式可以被表示为 $y \\rightarrow alog(y) + blog(1-y)$，而它的最大值为 $\\frac{a}{a+b}$\n\n基于上面的推论，我们的生成器 $G$ 的目标函数可以被表示为 $C(G)$，\n\n$C(G) = \\mathbb{E}_{x \\sim p_{data(x)}}[D_G^*(x)] + \\mathbb{E}_{z \\sim p_{z(z)}}[log(1 - D_G^*(x))]$\n\nTheorem: 当且仅当 $p_g = p_{data}$ 时， $C(G)$ 才能达到全局的最小值。此时，$C(G)$ 的值为 $-log4$。\n\nProof: 通过上面的证明，当 $p_g = p_{data}$ 时，$D_G^*(x) = \\frac{1}{2}$，即\n\n$\\mathbb{E}_{x \\sim p_{data(x)}}[-log2] + \\mathbb{E}_{z \\sim p_{z(z)}}[-log2] = -log4$\n\n因此我们的目标函数可以被表示为\n\n$C(G) = -log4 + KL(p_{data} || \\frac{p_{data} + p_g}{2}) + KL(p_{g} || \\frac{p_{data} + p_g}{2})$\n\n### GAN 所提出的算法具有收敛性\n\nProposition：当 G 和 D 有足够的能力的时候，在 GAN 算法的每一步，判别器 D 是可以达到最优解的，而 $p_g$ 的训练目标是提升\n\n$\\mathbb{E}_{x \\sim p_{data(x)}}[D_G^*(x)] + \\mathbb{E}_{z \\sim p_{z(z)}}[log(1 - D_G^*(x))]$\n\n直到 $p_g$ 收敛到 $p_{data}$\n\nProof：将 $V(G, D)$ 看成 $U(p_g, D)$，是一个关于 $p_g$ 的函数。\n\n之前：在一个高维的值空间做迭代\n\n现在：在一个函数空间做梯度下降\n\n","source":"_posts/Generative-Adversarial-Nets-GAN-论文精读.md","raw":"---\ntitle: Generative Adversarial Nets(GAN) 论文精读\ndate: 2022-11-23 10:41:55\ntags:\n---\n\nGAN 自提出之后近几年一直是一个充满热度的话题，而这篇论文的一作也是大名鼎鼎的花书的作者。\n\n## 摘要\n\n我们提出了一个通过对抗过程估计生成模型的新框架，其中我们同时训练两个模型：一个生成模型 G 捕获数据分布，另一个判别模型 D 估计样本来自训练数据而不是由 G 生成的概率。 G 的训练过程是最大化 D 出错的概率。该框架对应于极小极大两人博弈。在任意函数 G 和 D 的空间中，存在一个唯一解，G 恢复训练数据分布，D 处处等于 $\\frac{1}{2}$。在 G 和 D 由多层感知器定义的情况下，整个系统可以通过反向传播进行训练。在训练或生成样本期间，不需要任何马尔可夫链或展开的近似推理网络。实验通过对生成样本的定性和定量评估来证明该框架的潜力。\n\n## 引言\n\n在过去深度学习模型在判别模型上取得了显著的成功，但是在生成模型上受制于逼近最大似然估计等策略使得生成模型难以复现并借鉴判别模型的成功经验。在本文中，作者提出了一种新的生成模型框架规避了这些问题。\n\n作者举了一个非常形象的例子介绍生成模型与判别模型：生成模型可以被认为是造假币的团队，并且试图制造假币并且在不被发现的情况下使用它，而判别模型类似于警察，试图检测假币。在对抗过程中驱使这两个模型不断改进他们的方法，直到假币和真币无法区分。\n\n通过这种框架能够基于反向传播算法和 dropout 训练这两个模型即这两个 MLP。并不需要近似推理或者马尔可夫链。\n\n## 相关工作\n\n4相关工作\n\n﻿\n10:42\n﻿\n\n版本：NIPS final version，arxiv 早期版本\n\n相关工作 的 区别，未参考别人工作，大佬！高级高级！\n\n\n\nRelated work 第一段：先说别人的问题，我们怎么做？\n\n\n\n别人的问题在哪里呢？\n\n构造 有参数的 概率分布函数，provide a parametric specification of a probability distribution function \n\n\n\n参数构造了，怎么学习呢？\n\nmaximizing the log-likelihood，但采样分布计算难， esp 维度高， i.e., DBM。\n\n\n\n怎么解决这个计算难的问题？当然是不要 分布 啦\n\n直接学习一个模型，近似数据分布。\n\n\n\nDBM 和 我们的 generative models 有什么区别？\n\nDBM：一定要学 分布，知道均值、方差等一系列参数\n\nGAN：用一个模型学习 你想要的结果，答对题就行。\n\n\n\nGAN 的好处和坏处是什么呢？\n\npros: 计算容易；\n               \n\ncons: 即使答对了题，GAN 不知道正确的分布长什么样。\n\n\nRelated work 第二段：误差反向传递对 GAN 进行求解\n\n\n\nWhy error BP 有效？\n\n对 f 的期望求导 == 对 F 自己求导\n\n\n\n\n\nRelated work 第三段：unaware 别人的类似工作，VAE\n\n\n\nRelated work 第四段：从 discriminative 角度 to train a generative model 的相关工作\n\n\n\nGAN 和 Noise-contrastive estimation (NCE) 的区别？\n\nNCE 的损失函数复杂一点 --> 求解性能不如 GAN\n\n\n\nRelated work 第五段：和 predictability minimization 1992的区别\n\n\n\nJürgen (LSTM作者) 有非常超前的工作，LSTM在当年的计算量和数据量的条件，很难大展身手。现在，LSTM 效果被发现了。\n\n\n\nJürgen predictability minimization算法被埋没了，Jürgen 认为 GAN 是 reverse PM。\n\n\n\n轶事：Jürgen 在GAN 的 NIPS 汇报，也提出质疑。Ian 回绝，邮件交流已经很清楚了，你在耽搁大家学习 GAN\n\n\n\nRemark：一个真实有用的技术，会在不同领域、不断的被人重新发现，给予新的名词。大家会把功劳归给那一个教会了大家这个算法的人，而不是最早发明它的人。\n\n\n\n\n\nRelated work 第六段：和 adversarial examples 的区别\n\n﻿\n15:25\n﻿\n\nadversarial examples 测试算法的稳定性，how？\n\n构造一些假的、跟真的样本长得很像的 样本，能糊弄到分类器。\n\n\n## GAN 的目标函数\n\n生成器的目标是学习训练数据 $x$ 的数据分布情况，生成器的概率分布用 $p_g$ 表示。生成器的输入是一个噪声分布 $p_z(z)$，然后将其映射到数据空间表示为 $G(z;\\theta_g)$，其中 $G$ 是一个可微的函数，由参数为 $\\theta_g$ 的多层感知机表示。输出单个标量的判别模型被表示为$D(x;\\theta_d)$。$D(x)$ 表示 $x$ 来自于数据分布而不是 $p_g$ 的概率。我们训练 $D$ 以最大化将正确标签分配给来自 $G$ 的样本的概率。我们同时训练 $G$ 以最小化 $log(1-D(G(z)))$。换句话说，这个过程可以被视为 $G$ 和 $D$ 在玩具有价值函数 $V(G, D)$ 的两人极小极大游戏：\n\n$\\mathop{min}\\limits_{G}\\mathop{max}\\limits_{D}V(D, G) = \\mathbb{E}_{x \\sim p_{data(x)}}[logD(x)] + \\mathbb{E}_{z \\sim p_{z(z)}}[logD(1 - logD(G(z)))]$\n\n在训练的过程中优化 $D$ 非常容易过拟合而且计算量很大。所以通过超参数 $k$ 控制 $D$ 的优化，并且优化 $D$ 的 $k$ 步和优化 $G$ 的一步交替进行。\n\n\n![](/images/GAN/GAN.png)\n\n上图中蓝色的虚线表示判别模型的分布情况，绿色的实现表示生成模型的分布，黑色的点线表示真实数据的分布。下方的 $z$ 轴表示均匀采样的噪声点，向上的线表示映射 $x = G(z)$ 即如果对噪声点施加生成模型的分布 $p_g$。(a) 表示奖将近收敛的对抗模型情况：$p_g$与 $p_{data}$ 相似，$D$ 是准确的分类器(b)$D$模型收敛到$D(x) = \\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}$。(c)在更新$G$ 后，$D$的梯度引导$G(z)$流入更有可能被归类为数据分布的区域。(d)经过几个步骤的训练，如果 $G$ 和 $D$ 有足够的能力，他们将达到一个目标点，此时 $p_g=p_{data}$。鉴别器无法分类两个分布$D(x)=\\frac{1}{2}$\n\n## 理论推导\n\n### 全局的最优目标是 $p_g = p_{data}$\n\nProposition: 我们首先定义对于给定的 $G$，判别器 $D$ 可以被表示为 $D^*_{G}(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$\n\nProof:不考虑生成器 $G$的情况下，判别器$D$的训练目标是最大化 $V(G, D)$\n\n$V(D, G) = \\int_xp_{data}(x)logD(x) dx + \\int_z p_z(z)log(1 - D(G(z)))dz\n\\\\=\\int_xp_{data}(x)logD(x)  +  p_g(x)log(1 - D(x))dx$\n\n现在给出上式的解释，首先第一步就是将期望转换为积分的形式；第二步，因为我们这里考虑的是任意生成器 $G$ 所以可以使用换元替换。上式的最终形式可以被表示为 $y \\rightarrow alog(y) + blog(1-y)$，而它的最大值为 $\\frac{a}{a+b}$\n\n基于上面的推论，我们的生成器 $G$ 的目标函数可以被表示为 $C(G)$，\n\n$C(G) = \\mathbb{E}_{x \\sim p_{data(x)}}[D_G^*(x)] + \\mathbb{E}_{z \\sim p_{z(z)}}[log(1 - D_G^*(x))]$\n\nTheorem: 当且仅当 $p_g = p_{data}$ 时， $C(G)$ 才能达到全局的最小值。此时，$C(G)$ 的值为 $-log4$。\n\nProof: 通过上面的证明，当 $p_g = p_{data}$ 时，$D_G^*(x) = \\frac{1}{2}$，即\n\n$\\mathbb{E}_{x \\sim p_{data(x)}}[-log2] + \\mathbb{E}_{z \\sim p_{z(z)}}[-log2] = -log4$\n\n因此我们的目标函数可以被表示为\n\n$C(G) = -log4 + KL(p_{data} || \\frac{p_{data} + p_g}{2}) + KL(p_{g} || \\frac{p_{data} + p_g}{2})$\n\n### GAN 所提出的算法具有收敛性\n\nProposition：当 G 和 D 有足够的能力的时候，在 GAN 算法的每一步，判别器 D 是可以达到最优解的，而 $p_g$ 的训练目标是提升\n\n$\\mathbb{E}_{x \\sim p_{data(x)}}[D_G^*(x)] + \\mathbb{E}_{z \\sim p_{z(z)}}[log(1 - D_G^*(x))]$\n\n直到 $p_g$ 收敛到 $p_{data}$\n\nProof：将 $V(G, D)$ 看成 $U(p_g, D)$，是一个关于 $p_g$ 的函数。\n\n之前：在一个高维的值空间做迭代\n\n现在：在一个函数空间做梯度下降\n\n","slug":"Generative-Adversarial-Nets-GAN-论文精读","published":1,"updated":"2022-11-23T12:20:38.189Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7koz0008qnwy0uks0cjx","content":"<p>GAN 自提出之后近几年一直是一个充满热度的话题，而这篇论文的一作也是大名鼎鼎的花书的作者。</p>\n<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>我们提出了一个通过对抗过程估计生成模型的新框架，其中我们同时训练两个模型：一个生成模型 G 捕获数据分布，另一个判别模型 D 估计样本来自训练数据而不是由 G 生成的概率。 G 的训练过程是最大化 D 出错的概率。该框架对应于极小极大两人博弈。在任意函数 G 和 D 的空间中，存在一个唯一解，G 恢复训练数据分布，D 处处等于 $\\frac{1}{2}$。在 G 和 D 由多层感知器定义的情况下，整个系统可以通过反向传播进行训练。在训练或生成样本期间，不需要任何马尔可夫链或展开的近似推理网络。实验通过对生成样本的定性和定量评估来证明该框架的潜力。</p>\n<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p>在过去深度学习模型在判别模型上取得了显著的成功，但是在生成模型上受制于逼近最大似然估计等策略使得生成模型难以复现并借鉴判别模型的成功经验。在本文中，作者提出了一种新的生成模型框架规避了这些问题。</p>\n<p>作者举了一个非常形象的例子介绍生成模型与判别模型：生成模型可以被认为是造假币的团队，并且试图制造假币并且在不被发现的情况下使用它，而判别模型类似于警察，试图检测假币。在对抗过程中驱使这两个模型不断改进他们的方法，直到假币和真币无法区分。</p>\n<p>通过这种框架能够基于反向传播算法和 dropout 训练这两个模型即这两个 MLP。并不需要近似推理或者马尔可夫链。</p>\n<h2 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h2><p>4相关工作</p>\n<p>﻿<br>10:42\n﻿</p>\n<p>版本：NIPS final version，arxiv 早期版本</p>\n<p>相关工作 的 区别，未参考别人工作，大佬！高级高级！</p>\n<p>Related work 第一段：先说别人的问题，我们怎么做？</p>\n<p>别人的问题在哪里呢？</p>\n<p>构造 有参数的 概率分布函数，provide a parametric specification of a probability distribution function </p>\n<p>参数构造了，怎么学习呢？</p>\n<p>maximizing the log-likelihood，但采样分布计算难， esp 维度高， i.e., DBM。</p>\n<p>怎么解决这个计算难的问题？当然是不要 分布 啦</p>\n<p>直接学习一个模型，近似数据分布。</p>\n<p>DBM 和 我们的 generative models 有什么区别？</p>\n<p>DBM：一定要学 分布，知道均值、方差等一系列参数</p>\n<p>GAN：用一个模型学习 你想要的结果，答对题就行。</p>\n<p>GAN 的好处和坏处是什么呢？</p>\n<p>pros: 计算容易；</p>\n<p>cons: 即使答对了题，GAN 不知道正确的分布长什么样。</p>\n<p>Related work 第二段：误差反向传递对 GAN 进行求解</p>\n<p>Why error BP 有效？</p>\n<p>对 f 的期望求导 &#x3D;&#x3D; 对 F 自己求导</p>\n<p>Related work 第三段：unaware 别人的类似工作，VAE</p>\n<p>Related work 第四段：从 discriminative 角度 to train a generative model 的相关工作</p>\n<p>GAN 和 Noise-contrastive estimation (NCE) 的区别？</p>\n<p>NCE 的损失函数复杂一点 –&gt; 求解性能不如 GAN</p>\n<p>Related work 第五段：和 predictability minimization 1992的区别</p>\n<p>Jürgen (LSTM作者) 有非常超前的工作，LSTM在当年的计算量和数据量的条件，很难大展身手。现在，LSTM 效果被发现了。</p>\n<p>Jürgen predictability minimization算法被埋没了，Jürgen 认为 GAN 是 reverse PM。</p>\n<p>轶事：Jürgen 在GAN 的 NIPS 汇报，也提出质疑。Ian 回绝，邮件交流已经很清楚了，你在耽搁大家学习 GAN</p>\n<p>Remark：一个真实有用的技术，会在不同领域、不断的被人重新发现，给予新的名词。大家会把功劳归给那一个教会了大家这个算法的人，而不是最早发明它的人。</p>\n<p>Related work 第六段：和 adversarial examples 的区别</p>\n<p>﻿<br>15:25\n﻿</p>\n<p>adversarial examples 测试算法的稳定性，how？</p>\n<p>构造一些假的、跟真的样本长得很像的 样本，能糊弄到分类器。</p>\n<h2 id=\"GAN-的目标函数\"><a href=\"#GAN-的目标函数\" class=\"headerlink\" title=\"GAN 的目标函数\"></a>GAN 的目标函数</h2><p>生成器的目标是学习训练数据 $x$ 的数据分布情况，生成器的概率分布用 $p_g$ 表示。生成器的输入是一个噪声分布 $p_z(z)$，然后将其映射到数据空间表示为 $G(z;\\theta_g)$，其中 $G$ 是一个可微的函数，由参数为 $\\theta_g$ 的多层感知机表示。输出单个标量的判别模型被表示为$D(x;\\theta_d)$。$D(x)$ 表示 $x$ 来自于数据分布而不是 $p_g$ 的概率。我们训练 $D$ 以最大化将正确标签分配给来自 $G$ 的样本的概率。我们同时训练 $G$ 以最小化 $log(1-D(G(z)))$。换句话说，这个过程可以被视为 $G$ 和 $D$ 在玩具有价值函数 $V(G, D)$ 的两人极小极大游戏：</p>\n<p>$\\mathop{min}\\limits_{G}\\mathop{max}\\limits_{D}V(D, G) &#x3D; \\mathbb{E}<em>{x \\sim p</em>{data(x)}}[logD(x)] + \\mathbb{E}<em>{z \\sim p</em>{z(z)}}[logD(1 - logD(G(z)))]$</p>\n<p>在训练的过程中优化 $D$ 非常容易过拟合而且计算量很大。所以通过超参数 $k$ 控制 $D$ 的优化，并且优化 $D$ 的 $k$ 步和优化 $G$ 的一步交替进行。</p>\n<p><img src=\"/images/GAN/GAN.png\"></p>\n<p>上图中蓝色的虚线表示判别模型的分布情况，绿色的实现表示生成模型的分布，黑色的点线表示真实数据的分布。下方的 $z$ 轴表示均匀采样的噪声点，向上的线表示映射 $x &#x3D; G(z)$ 即如果对噪声点施加生成模型的分布 $p_g$。(a) 表示奖将近收敛的对抗模型情况：$p_g$与 $p_{data}$ 相似，$D$ 是准确的分类器(b)$D$模型收敛到$D(x) &#x3D; \\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}$。(c)在更新$G$ 后，$D$的梯度引导$G(z)$流入更有可能被归类为数据分布的区域。(d)经过几个步骤的训练，如果 $G$ 和 $D$ 有足够的能力，他们将达到一个目标点，此时 $p_g&#x3D;p_{data}$。鉴别器无法分类两个分布$D(x)&#x3D;\\frac{1}{2}$</p>\n<h2 id=\"理论推导\"><a href=\"#理论推导\" class=\"headerlink\" title=\"理论推导\"></a>理论推导</h2><h3 id=\"全局的最优目标是-p-g-x3D-p-data\"><a href=\"#全局的最优目标是-p-g-x3D-p-data\" class=\"headerlink\" title=\"全局的最优目标是 $p_g &#x3D; p_{data}$\"></a>全局的最优目标是 $p_g &#x3D; p_{data}$</h3><p>Proposition: 我们首先定义对于给定的 $G$，判别器 $D$ 可以被表示为 $D^*_{G}(x) &#x3D; \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$</p>\n<p>Proof:不考虑生成器 $G$的情况下，判别器$D$的训练目标是最大化 $V(G, D)$</p>\n<p>$V(D, G) &#x3D; \\int_xp_{data}(x)logD(x) dx + \\int_z p_z(z)log(1 - D(G(z)))dz<br>\\&#x3D;\\int_xp_{data}(x)logD(x)  +  p_g(x)log(1 - D(x))dx$</p>\n<p>现在给出上式的解释，首先第一步就是将期望转换为积分的形式；第二步，因为我们这里考虑的是任意生成器 $G$ 所以可以使用换元替换。上式的最终形式可以被表示为 $y \\rightarrow alog(y) + blog(1-y)$，而它的最大值为 $\\frac{a}{a+b}$</p>\n<p>基于上面的推论，我们的生成器 $G$ 的目标函数可以被表示为 $C(G)$，</p>\n<p>$C(G) &#x3D; \\mathbb{E}<em>{x \\sim p</em>{data(x)}}[D_G^*(x)] + \\mathbb{E}<em>{z \\sim p</em>{z(z)}}[log(1 - D_G^*(x))]$</p>\n<p>Theorem: 当且仅当 $p_g &#x3D; p_{data}$ 时， $C(G)$ 才能达到全局的最小值。此时，$C(G)$ 的值为 $-log4$。</p>\n<p>Proof: 通过上面的证明，当 $p_g &#x3D; p_{data}$ 时，$D_G^*(x) &#x3D; \\frac{1}{2}$，即</p>\n<p>$\\mathbb{E}<em>{x \\sim p</em>{data(x)}}[-log2] + \\mathbb{E}<em>{z \\sim p</em>{z(z)}}[-log2] &#x3D; -log4$</p>\n<p>因此我们的目标函数可以被表示为</p>\n<p>$C(G) &#x3D; -log4 + KL(p_{data} || \\frac{p_{data} + p_g}{2}) + KL(p_{g} || \\frac{p_{data} + p_g}{2})$</p>\n<h3 id=\"GAN-所提出的算法具有收敛性\"><a href=\"#GAN-所提出的算法具有收敛性\" class=\"headerlink\" title=\"GAN 所提出的算法具有收敛性\"></a>GAN 所提出的算法具有收敛性</h3><p>Proposition：当 G 和 D 有足够的能力的时候，在 GAN 算法的每一步，判别器 D 是可以达到最优解的，而 $p_g$ 的训练目标是提升</p>\n<p>$\\mathbb{E}<em>{x \\sim p</em>{data(x)}}[D_G^*(x)] + \\mathbb{E}<em>{z \\sim p</em>{z(z)}}[log(1 - D_G^*(x))]$</p>\n<p>直到 $p_g$ 收敛到 $p_{data}$</p>\n<p>Proof：将 $V(G, D)$ 看成 $U(p_g, D)$，是一个关于 $p_g$ 的函数。</p>\n<p>之前：在一个高维的值空间做迭代</p>\n<p>现在：在一个函数空间做梯度下降</p>\n","site":{"data":{}},"excerpt":"","more":"<p>GAN 自提出之后近几年一直是一个充满热度的话题，而这篇论文的一作也是大名鼎鼎的花书的作者。</p>\n<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>我们提出了一个通过对抗过程估计生成模型的新框架，其中我们同时训练两个模型：一个生成模型 G 捕获数据分布，另一个判别模型 D 估计样本来自训练数据而不是由 G 生成的概率。 G 的训练过程是最大化 D 出错的概率。该框架对应于极小极大两人博弈。在任意函数 G 和 D 的空间中，存在一个唯一解，G 恢复训练数据分布，D 处处等于 $\\frac{1}{2}$。在 G 和 D 由多层感知器定义的情况下，整个系统可以通过反向传播进行训练。在训练或生成样本期间，不需要任何马尔可夫链或展开的近似推理网络。实验通过对生成样本的定性和定量评估来证明该框架的潜力。</p>\n<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p>在过去深度学习模型在判别模型上取得了显著的成功，但是在生成模型上受制于逼近最大似然估计等策略使得生成模型难以复现并借鉴判别模型的成功经验。在本文中，作者提出了一种新的生成模型框架规避了这些问题。</p>\n<p>作者举了一个非常形象的例子介绍生成模型与判别模型：生成模型可以被认为是造假币的团队，并且试图制造假币并且在不被发现的情况下使用它，而判别模型类似于警察，试图检测假币。在对抗过程中驱使这两个模型不断改进他们的方法，直到假币和真币无法区分。</p>\n<p>通过这种框架能够基于反向传播算法和 dropout 训练这两个模型即这两个 MLP。并不需要近似推理或者马尔可夫链。</p>\n<h2 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h2><p>4相关工作</p>\n<p>﻿<br>10:42\n﻿</p>\n<p>版本：NIPS final version，arxiv 早期版本</p>\n<p>相关工作 的 区别，未参考别人工作，大佬！高级高级！</p>\n<p>Related work 第一段：先说别人的问题，我们怎么做？</p>\n<p>别人的问题在哪里呢？</p>\n<p>构造 有参数的 概率分布函数，provide a parametric specification of a probability distribution function </p>\n<p>参数构造了，怎么学习呢？</p>\n<p>maximizing the log-likelihood，但采样分布计算难， esp 维度高， i.e., DBM。</p>\n<p>怎么解决这个计算难的问题？当然是不要 分布 啦</p>\n<p>直接学习一个模型，近似数据分布。</p>\n<p>DBM 和 我们的 generative models 有什么区别？</p>\n<p>DBM：一定要学 分布，知道均值、方差等一系列参数</p>\n<p>GAN：用一个模型学习 你想要的结果，答对题就行。</p>\n<p>GAN 的好处和坏处是什么呢？</p>\n<p>pros: 计算容易；</p>\n<p>cons: 即使答对了题，GAN 不知道正确的分布长什么样。</p>\n<p>Related work 第二段：误差反向传递对 GAN 进行求解</p>\n<p>Why error BP 有效？</p>\n<p>对 f 的期望求导 &#x3D;&#x3D; 对 F 自己求导</p>\n<p>Related work 第三段：unaware 别人的类似工作，VAE</p>\n<p>Related work 第四段：从 discriminative 角度 to train a generative model 的相关工作</p>\n<p>GAN 和 Noise-contrastive estimation (NCE) 的区别？</p>\n<p>NCE 的损失函数复杂一点 –&gt; 求解性能不如 GAN</p>\n<p>Related work 第五段：和 predictability minimization 1992的区别</p>\n<p>Jürgen (LSTM作者) 有非常超前的工作，LSTM在当年的计算量和数据量的条件，很难大展身手。现在，LSTM 效果被发现了。</p>\n<p>Jürgen predictability minimization算法被埋没了，Jürgen 认为 GAN 是 reverse PM。</p>\n<p>轶事：Jürgen 在GAN 的 NIPS 汇报，也提出质疑。Ian 回绝，邮件交流已经很清楚了，你在耽搁大家学习 GAN</p>\n<p>Remark：一个真实有用的技术，会在不同领域、不断的被人重新发现，给予新的名词。大家会把功劳归给那一个教会了大家这个算法的人，而不是最早发明它的人。</p>\n<p>Related work 第六段：和 adversarial examples 的区别</p>\n<p>﻿<br>15:25\n﻿</p>\n<p>adversarial examples 测试算法的稳定性，how？</p>\n<p>构造一些假的、跟真的样本长得很像的 样本，能糊弄到分类器。</p>\n<h2 id=\"GAN-的目标函数\"><a href=\"#GAN-的目标函数\" class=\"headerlink\" title=\"GAN 的目标函数\"></a>GAN 的目标函数</h2><p>生成器的目标是学习训练数据 $x$ 的数据分布情况，生成器的概率分布用 $p_g$ 表示。生成器的输入是一个噪声分布 $p_z(z)$，然后将其映射到数据空间表示为 $G(z;\\theta_g)$，其中 $G$ 是一个可微的函数，由参数为 $\\theta_g$ 的多层感知机表示。输出单个标量的判别模型被表示为$D(x;\\theta_d)$。$D(x)$ 表示 $x$ 来自于数据分布而不是 $p_g$ 的概率。我们训练 $D$ 以最大化将正确标签分配给来自 $G$ 的样本的概率。我们同时训练 $G$ 以最小化 $log(1-D(G(z)))$。换句话说，这个过程可以被视为 $G$ 和 $D$ 在玩具有价值函数 $V(G, D)$ 的两人极小极大游戏：</p>\n<p>$\\mathop{min}\\limits_{G}\\mathop{max}\\limits_{D}V(D, G) &#x3D; \\mathbb{E}<em>{x \\sim p</em>{data(x)}}[logD(x)] + \\mathbb{E}<em>{z \\sim p</em>{z(z)}}[logD(1 - logD(G(z)))]$</p>\n<p>在训练的过程中优化 $D$ 非常容易过拟合而且计算量很大。所以通过超参数 $k$ 控制 $D$ 的优化，并且优化 $D$ 的 $k$ 步和优化 $G$ 的一步交替进行。</p>\n<p><img src=\"/images/GAN/GAN.png\"></p>\n<p>上图中蓝色的虚线表示判别模型的分布情况，绿色的实现表示生成模型的分布，黑色的点线表示真实数据的分布。下方的 $z$ 轴表示均匀采样的噪声点，向上的线表示映射 $x &#x3D; G(z)$ 即如果对噪声点施加生成模型的分布 $p_g$。(a) 表示奖将近收敛的对抗模型情况：$p_g$与 $p_{data}$ 相似，$D$ 是准确的分类器(b)$D$模型收敛到$D(x) &#x3D; \\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}$。(c)在更新$G$ 后，$D$的梯度引导$G(z)$流入更有可能被归类为数据分布的区域。(d)经过几个步骤的训练，如果 $G$ 和 $D$ 有足够的能力，他们将达到一个目标点，此时 $p_g&#x3D;p_{data}$。鉴别器无法分类两个分布$D(x)&#x3D;\\frac{1}{2}$</p>\n<h2 id=\"理论推导\"><a href=\"#理论推导\" class=\"headerlink\" title=\"理论推导\"></a>理论推导</h2><h3 id=\"全局的最优目标是-p-g-x3D-p-data\"><a href=\"#全局的最优目标是-p-g-x3D-p-data\" class=\"headerlink\" title=\"全局的最优目标是 $p_g &#x3D; p_{data}$\"></a>全局的最优目标是 $p_g &#x3D; p_{data}$</h3><p>Proposition: 我们首先定义对于给定的 $G$，判别器 $D$ 可以被表示为 $D^*_{G}(x) &#x3D; \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$</p>\n<p>Proof:不考虑生成器 $G$的情况下，判别器$D$的训练目标是最大化 $V(G, D)$</p>\n<p>$V(D, G) &#x3D; \\int_xp_{data}(x)logD(x) dx + \\int_z p_z(z)log(1 - D(G(z)))dz<br>\\&#x3D;\\int_xp_{data}(x)logD(x)  +  p_g(x)log(1 - D(x))dx$</p>\n<p>现在给出上式的解释，首先第一步就是将期望转换为积分的形式；第二步，因为我们这里考虑的是任意生成器 $G$ 所以可以使用换元替换。上式的最终形式可以被表示为 $y \\rightarrow alog(y) + blog(1-y)$，而它的最大值为 $\\frac{a}{a+b}$</p>\n<p>基于上面的推论，我们的生成器 $G$ 的目标函数可以被表示为 $C(G)$，</p>\n<p>$C(G) &#x3D; \\mathbb{E}<em>{x \\sim p</em>{data(x)}}[D_G^*(x)] + \\mathbb{E}<em>{z \\sim p</em>{z(z)}}[log(1 - D_G^*(x))]$</p>\n<p>Theorem: 当且仅当 $p_g &#x3D; p_{data}$ 时， $C(G)$ 才能达到全局的最小值。此时，$C(G)$ 的值为 $-log4$。</p>\n<p>Proof: 通过上面的证明，当 $p_g &#x3D; p_{data}$ 时，$D_G^*(x) &#x3D; \\frac{1}{2}$，即</p>\n<p>$\\mathbb{E}<em>{x \\sim p</em>{data(x)}}[-log2] + \\mathbb{E}<em>{z \\sim p</em>{z(z)}}[-log2] &#x3D; -log4$</p>\n<p>因此我们的目标函数可以被表示为</p>\n<p>$C(G) &#x3D; -log4 + KL(p_{data} || \\frac{p_{data} + p_g}{2}) + KL(p_{g} || \\frac{p_{data} + p_g}{2})$</p>\n<h3 id=\"GAN-所提出的算法具有收敛性\"><a href=\"#GAN-所提出的算法具有收敛性\" class=\"headerlink\" title=\"GAN 所提出的算法具有收敛性\"></a>GAN 所提出的算法具有收敛性</h3><p>Proposition：当 G 和 D 有足够的能力的时候，在 GAN 算法的每一步，判别器 D 是可以达到最优解的，而 $p_g$ 的训练目标是提升</p>\n<p>$\\mathbb{E}<em>{x \\sim p</em>{data(x)}}[D_G^*(x)] + \\mathbb{E}<em>{z \\sim p</em>{z(z)}}[log(1 - D_G^*(x))]$</p>\n<p>直到 $p_g$ 收敛到 $p_{data}$</p>\n<p>Proof：将 $V(G, D)$ 看成 $U(p_g, D)$，是一个关于 $p_g$ 的函数。</p>\n<p>之前：在一个高维的值空间做迭代</p>\n<p>现在：在一个函数空间做梯度下降</p>\n"},{"title":"Relevance Rollout，比 attention 更具可解释性的 transformer 解释方法","date":"2023-07-04T06:42:41.000Z","_content":"\n今天给大家分享一篇文章 《Transformer Interpretability Beyond Attention Visualization》。这篇文章是 2021 年 CVPR 的文章，这篇工作中提出了一套新的可解释性方法来对 transformer 进行可解释性分析。\n\ntransformer 的 attention 机制本身就能够输出具有一定可解释性的可视化结果，但是这套方案只能看到单个注意力块的结果，因此 rollout 应运而生。他能够累积所有注意力块的归因结果得到一个全局的归因，ViT 的解释分析方法使用的就是 rollout，但是 rollout 是一个静态的归因结果，因此作者在本文中提出一种 class-specific 的解释方法。在标题中，作者并没有明确得给自己的方法命名，所以这篇文章中为了书写方便将这个方法暂且称为 Relevance Rollout。\n\n<!--more-->\n\n## 摘要\n\n这篇文章与 ViT 是同年发布的文章， ViT 一经发布后就席卷了 CV 领域的各个子任务，自然有人好奇 ViT 这个黑箱模型其中的机制到底是什么。\n\n而之前针对 transformer 进行可解释的方法只有 transformer 文章中提及的注意力图也可称为显著性图、以及 2020 年 ACL 的一篇文章 Rollout 采用启发式的方法传播注意力图得到最终的归因结果。\n\n作者提出的方法其实可以总结为两点，一是在 transformer 的可解释性工作中如何对注意力层做好解释分析，二是层与层之间残差连接结构的相关性做好分析是当前任务的难点，而作者提出的方法旨在解决这两个问题的。\n\n最终，作者在一些在一些 transformer 网络以及一些任务中验证了其方法的有效性。\n\n## 引言\n\n现在 NLP 和 CV 领域的 sota 方法基本上都被 transformer based model 刷了一遍，而且今年大火的大模型的背后也是 transformer。因此，关于可视化 transformer 的决策过程的研究对于我们后续进一步开展相关研究至关重要。\n\ntransformer 的核心结构就是 self-attention layers，在自注意力层中会为每两个 token 之间分配一个注意力值。在 NLP 中，这个 token 就是一个单词或者单词的一部分，在 CV 领域 这个 token 可以是图像的一个 patch。\n\n可视化 Transformer 模型有两种常见的方法，一种是将注意力作为相关性分数进行可视化，这种方法通常用于单个注意力层。另一种是连接多个层。也就是平均所有层的注意力值。然而这种方法假设层与层之间是某种简单的关系进行累加，会导致一些不相关的区域被高亮，例如 rollout 方法其实就是假设层与层之间是线性关系。\n\n本文提出了一套可解释性方法。\n\n第一步，引入能够适用于正值和负值的相关性传播规则\n\n第二步，为非参数层提出了一个归一项\n\n第三步，集成了注意力和相关性分数并连接了多个注意力块的集成结果\n\n## 方法\n\n在本文方法的第 3.1 和 3.2 章节都是在做一些定理的推导和证明，这里就不过多赘述和推导了。3.1 章节相关性和梯度，通过深度泰勒分解推导出相关性值的计算方式，以及非线性层正负激活值的处理问题。3.2 章节非参相关性传播，主要介绍了如果对于非参结构进行相关性值的传播以及通过乘法传播的过程可能会带来的相关性值爆炸的问题及归一化解决方式的形式化推导。\n\n### 3.3 相关性和梯度扩散\n\n在 ViT 中，采用的是多头自注意力结构，注意力头的数量即为 $h$，每一个头的特征维度为 $d_h$ 因此一个注意力块的总维度 $d = hd_h$。一个自注意力块被定义为下面的结构：\n\n$A^{(b)} = softmax(\\frac{ {Q^{(b)}} \\cdot {K^{(b)}} }{\\sqrt{d_h}})$\n\n$O^{(b)} = A^{(b)} \\cdot V^{(b)}$\n\n其中，$Q^{(b)}, K^{(b)}, V^{(b)} \\in \\mathbb{R}^{h \\times s \\times d_h}$ 分别代表 query,key 和 value。\n\n按照如上的定义， rollout 方法的计算为如下过程：\n\n$\\hat{A}^{(b)} = I + \\mathbb{E_h}A^{(b)}$\n\n$rollout = \\hat{A}^{(1)} \\cdot \\hat{A}^{(2)} \\cdot ... \\cdot \\hat{A}^{(B)}$\n\n我们可以观察到 rollout 的结果是固定的，不会因目标类别的变化而产生影响。\n\n本文的过程为如下形式：\n\n$\\overline{A}^{(b)} = I + \\mathbb{E_h}(\\bigtriangledown A^{(b)} \\odot R^{(n_b)})^{+}$\n\n$C = \\overline{A}^{(1)} \\cdot \\overline{A}^{(2)} \\cdot ... \\cdot \\overline{A}^{(B)}$\n\n其中 $\\mathbb{E_h}$ 是多个头输出结果的求平均得到的结果，而 $\\bigtriangledown A^{(b)}$ 为注意力映射 $A^{(b)}$ 的梯度，$R^{(n_b)}$ 为相关值，其计算方式在 3.1 章节中有详细的介绍和推导，其实就是通过泰勒分解进行计算得到的值。\n\n### 3.4 得到图片的归因映射\n\n通过上述的方法得到的矩阵 $C$ 的大小为 $s \\times s$。在常规的 ViT 结构中，$s$ 为 197，这里进行一个简要的介绍，想要更加详细了解的话可以读一下 ViT 和 Bert 的论文。在 ViT 中是将一个 $16 \\times 16$ 的图像块作为一个 patch，因此对于一个 $224 \\times 224$ 的图像输入进来，就会得到 $(224 \\times 224) \\div (16 \\times 16)$ 也就是 196 个 token。在 ViT 中，参考了 Bert 的设计，在这 196 个 token 前加了一个 [CLS] token，用于下游的分类任务。从下图即 ViT 的架构图中就能清晰得看出来这一设计。\n\n![](/images/R_Rollout/ViT.png)\n\n\n因此我们最后得到的矩阵 $C$ 的大小为 $197 \\times 197$，下一步就是实现从这个相关性矩阵到图像归因结果的映射了。这个步骤，在原文的代码中是通过 $C[0, 1:]$ 这个操作来实现的。在理解了，ViT 结构的 token 的内涵后，对于这一设计也很容易理解。首先，这个相关性矩阵的每一行就是每一个 token 与别的 token 的相关性映射。而只有第一个 [CLS] token 中包含类别信息，因此我们需要取出他，然而 [CLS] token 与他本身的映射关系在图像中是没有映射关系的，所以只取出了后 196 个值，也即 [CLS] token 与各个 patch token 的映射。最后，我们得到一个 $14 \\times 14$ 的矩阵，再通过双线性插值上采样到原始图像尺寸。\n\n整个算法完整的过程如下图所示：\n\n![](/images/R_Rollout/R_Rollout.png)\n\n## 4 实验\n\n首先是定性实验，也就是提供了一些示例的不同方法的归因结果对比，分别是无类别归因和类别归因结果。\n\n![](/images/R_Rollout/sample_results.png)\n\n![](/images/R_Rollout/class_specific.png)\n\n然后是扰动实验，即基于归因结果按照相关值从小到大和从大到小的顺序去进行 mask 得出对于预测的影响计算得到 AUC。\n\n![](/images/R_Rollout/perturbation.png)\n\n接下来分别是分割实验和 Bert 归因实验的结果\n\n![](/images/R_Rollout/segment.png)\n\n![](/images/R_Rollout/Bert.png)\n\n在消融实验中，作者测试了不使用梯度信息，只对第一个注意力块和最后一个注意力块进行计算 这三个消融实验设置下的不同结果。\n\n![](/images/R_Rollout/alation.png)\n\n\n## 5 结语\n\n在结语中，作者主要提到了几个点值得关注：\n\n- 与 CNN 相比， Transformer 中的非正激活函数，跳跃连接以及对自注意力矩阵乘法的建模等问题对于模型可解释来说都是很大的挑战。（黑盒模型则不需要关注于这些挑战，但是黑盒模型的时间效率太低了\n\n- 可能是受限于注意力本身机制的问题，通过本文方法得到的解释结果依然存在碎片化解释的问题","source":"_posts/Transformer-Interpretability-Beyond-Attention-Visualization-论文精度及代码复现.md","raw":"---\ntitle: Relevance Rollout，比 attention 更具可解释性的 transformer 解释方法\ndate: 2023-07-04 14:42:41\ntags:\n---\n\n今天给大家分享一篇文章 《Transformer Interpretability Beyond Attention Visualization》。这篇文章是 2021 年 CVPR 的文章，这篇工作中提出了一套新的可解释性方法来对 transformer 进行可解释性分析。\n\ntransformer 的 attention 机制本身就能够输出具有一定可解释性的可视化结果，但是这套方案只能看到单个注意力块的结果，因此 rollout 应运而生。他能够累积所有注意力块的归因结果得到一个全局的归因，ViT 的解释分析方法使用的就是 rollout，但是 rollout 是一个静态的归因结果，因此作者在本文中提出一种 class-specific 的解释方法。在标题中，作者并没有明确得给自己的方法命名，所以这篇文章中为了书写方便将这个方法暂且称为 Relevance Rollout。\n\n<!--more-->\n\n## 摘要\n\n这篇文章与 ViT 是同年发布的文章， ViT 一经发布后就席卷了 CV 领域的各个子任务，自然有人好奇 ViT 这个黑箱模型其中的机制到底是什么。\n\n而之前针对 transformer 进行可解释的方法只有 transformer 文章中提及的注意力图也可称为显著性图、以及 2020 年 ACL 的一篇文章 Rollout 采用启发式的方法传播注意力图得到最终的归因结果。\n\n作者提出的方法其实可以总结为两点，一是在 transformer 的可解释性工作中如何对注意力层做好解释分析，二是层与层之间残差连接结构的相关性做好分析是当前任务的难点，而作者提出的方法旨在解决这两个问题的。\n\n最终，作者在一些在一些 transformer 网络以及一些任务中验证了其方法的有效性。\n\n## 引言\n\n现在 NLP 和 CV 领域的 sota 方法基本上都被 transformer based model 刷了一遍，而且今年大火的大模型的背后也是 transformer。因此，关于可视化 transformer 的决策过程的研究对于我们后续进一步开展相关研究至关重要。\n\ntransformer 的核心结构就是 self-attention layers，在自注意力层中会为每两个 token 之间分配一个注意力值。在 NLP 中，这个 token 就是一个单词或者单词的一部分，在 CV 领域 这个 token 可以是图像的一个 patch。\n\n可视化 Transformer 模型有两种常见的方法，一种是将注意力作为相关性分数进行可视化，这种方法通常用于单个注意力层。另一种是连接多个层。也就是平均所有层的注意力值。然而这种方法假设层与层之间是某种简单的关系进行累加，会导致一些不相关的区域被高亮，例如 rollout 方法其实就是假设层与层之间是线性关系。\n\n本文提出了一套可解释性方法。\n\n第一步，引入能够适用于正值和负值的相关性传播规则\n\n第二步，为非参数层提出了一个归一项\n\n第三步，集成了注意力和相关性分数并连接了多个注意力块的集成结果\n\n## 方法\n\n在本文方法的第 3.1 和 3.2 章节都是在做一些定理的推导和证明，这里就不过多赘述和推导了。3.1 章节相关性和梯度，通过深度泰勒分解推导出相关性值的计算方式，以及非线性层正负激活值的处理问题。3.2 章节非参相关性传播，主要介绍了如果对于非参结构进行相关性值的传播以及通过乘法传播的过程可能会带来的相关性值爆炸的问题及归一化解决方式的形式化推导。\n\n### 3.3 相关性和梯度扩散\n\n在 ViT 中，采用的是多头自注意力结构，注意力头的数量即为 $h$，每一个头的特征维度为 $d_h$ 因此一个注意力块的总维度 $d = hd_h$。一个自注意力块被定义为下面的结构：\n\n$A^{(b)} = softmax(\\frac{ {Q^{(b)}} \\cdot {K^{(b)}} }{\\sqrt{d_h}})$\n\n$O^{(b)} = A^{(b)} \\cdot V^{(b)}$\n\n其中，$Q^{(b)}, K^{(b)}, V^{(b)} \\in \\mathbb{R}^{h \\times s \\times d_h}$ 分别代表 query,key 和 value。\n\n按照如上的定义， rollout 方法的计算为如下过程：\n\n$\\hat{A}^{(b)} = I + \\mathbb{E_h}A^{(b)}$\n\n$rollout = \\hat{A}^{(1)} \\cdot \\hat{A}^{(2)} \\cdot ... \\cdot \\hat{A}^{(B)}$\n\n我们可以观察到 rollout 的结果是固定的，不会因目标类别的变化而产生影响。\n\n本文的过程为如下形式：\n\n$\\overline{A}^{(b)} = I + \\mathbb{E_h}(\\bigtriangledown A^{(b)} \\odot R^{(n_b)})^{+}$\n\n$C = \\overline{A}^{(1)} \\cdot \\overline{A}^{(2)} \\cdot ... \\cdot \\overline{A}^{(B)}$\n\n其中 $\\mathbb{E_h}$ 是多个头输出结果的求平均得到的结果，而 $\\bigtriangledown A^{(b)}$ 为注意力映射 $A^{(b)}$ 的梯度，$R^{(n_b)}$ 为相关值，其计算方式在 3.1 章节中有详细的介绍和推导，其实就是通过泰勒分解进行计算得到的值。\n\n### 3.4 得到图片的归因映射\n\n通过上述的方法得到的矩阵 $C$ 的大小为 $s \\times s$。在常规的 ViT 结构中，$s$ 为 197，这里进行一个简要的介绍，想要更加详细了解的话可以读一下 ViT 和 Bert 的论文。在 ViT 中是将一个 $16 \\times 16$ 的图像块作为一个 patch，因此对于一个 $224 \\times 224$ 的图像输入进来，就会得到 $(224 \\times 224) \\div (16 \\times 16)$ 也就是 196 个 token。在 ViT 中，参考了 Bert 的设计，在这 196 个 token 前加了一个 [CLS] token，用于下游的分类任务。从下图即 ViT 的架构图中就能清晰得看出来这一设计。\n\n![](/images/R_Rollout/ViT.png)\n\n\n因此我们最后得到的矩阵 $C$ 的大小为 $197 \\times 197$，下一步就是实现从这个相关性矩阵到图像归因结果的映射了。这个步骤，在原文的代码中是通过 $C[0, 1:]$ 这个操作来实现的。在理解了，ViT 结构的 token 的内涵后，对于这一设计也很容易理解。首先，这个相关性矩阵的每一行就是每一个 token 与别的 token 的相关性映射。而只有第一个 [CLS] token 中包含类别信息，因此我们需要取出他，然而 [CLS] token 与他本身的映射关系在图像中是没有映射关系的，所以只取出了后 196 个值，也即 [CLS] token 与各个 patch token 的映射。最后，我们得到一个 $14 \\times 14$ 的矩阵，再通过双线性插值上采样到原始图像尺寸。\n\n整个算法完整的过程如下图所示：\n\n![](/images/R_Rollout/R_Rollout.png)\n\n## 4 实验\n\n首先是定性实验，也就是提供了一些示例的不同方法的归因结果对比，分别是无类别归因和类别归因结果。\n\n![](/images/R_Rollout/sample_results.png)\n\n![](/images/R_Rollout/class_specific.png)\n\n然后是扰动实验，即基于归因结果按照相关值从小到大和从大到小的顺序去进行 mask 得出对于预测的影响计算得到 AUC。\n\n![](/images/R_Rollout/perturbation.png)\n\n接下来分别是分割实验和 Bert 归因实验的结果\n\n![](/images/R_Rollout/segment.png)\n\n![](/images/R_Rollout/Bert.png)\n\n在消融实验中，作者测试了不使用梯度信息，只对第一个注意力块和最后一个注意力块进行计算 这三个消融实验设置下的不同结果。\n\n![](/images/R_Rollout/alation.png)\n\n\n## 5 结语\n\n在结语中，作者主要提到了几个点值得关注：\n\n- 与 CNN 相比， Transformer 中的非正激活函数，跳跃连接以及对自注意力矩阵乘法的建模等问题对于模型可解释来说都是很大的挑战。（黑盒模型则不需要关注于这些挑战，但是黑盒模型的时间效率太低了\n\n- 可能是受限于注意力本身机制的问题，通过本文方法得到的解释结果依然存在碎片化解释的问题","slug":"Transformer-Interpretability-Beyond-Attention-Visualization-论文精度及代码复现","published":1,"updated":"2023-07-06T08:59:16.486Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7koz0009qnwy0z2t4ny5","content":"<p>今天给大家分享一篇文章 《Transformer Interpretability Beyond Attention Visualization》。这篇文章是 2021 年 CVPR 的文章，这篇工作中提出了一套新的可解释性方法来对 transformer 进行可解释性分析。</p>\n<p>transformer 的 attention 机制本身就能够输出具有一定可解释性的可视化结果，但是这套方案只能看到单个注意力块的结果，因此 rollout 应运而生。他能够累积所有注意力块的归因结果得到一个全局的归因，ViT 的解释分析方法使用的就是 rollout，但是 rollout 是一个静态的归因结果，因此作者在本文中提出一种 class-specific 的解释方法。在标题中，作者并没有明确得给自己的方法命名，所以这篇文章中为了书写方便将这个方法暂且称为 Relevance Rollout。</p>\n<span id=\"more\"></span>\n\n<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>这篇文章与 ViT 是同年发布的文章， ViT 一经发布后就席卷了 CV 领域的各个子任务，自然有人好奇 ViT 这个黑箱模型其中的机制到底是什么。</p>\n<p>而之前针对 transformer 进行可解释的方法只有 transformer 文章中提及的注意力图也可称为显著性图、以及 2020 年 ACL 的一篇文章 Rollout 采用启发式的方法传播注意力图得到最终的归因结果。</p>\n<p>作者提出的方法其实可以总结为两点，一是在 transformer 的可解释性工作中如何对注意力层做好解释分析，二是层与层之间残差连接结构的相关性做好分析是当前任务的难点，而作者提出的方法旨在解决这两个问题的。</p>\n<p>最终，作者在一些在一些 transformer 网络以及一些任务中验证了其方法的有效性。</p>\n<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p>现在 NLP 和 CV 领域的 sota 方法基本上都被 transformer based model 刷了一遍，而且今年大火的大模型的背后也是 transformer。因此，关于可视化 transformer 的决策过程的研究对于我们后续进一步开展相关研究至关重要。</p>\n<p>transformer 的核心结构就是 self-attention layers，在自注意力层中会为每两个 token 之间分配一个注意力值。在 NLP 中，这个 token 就是一个单词或者单词的一部分，在 CV 领域 这个 token 可以是图像的一个 patch。</p>\n<p>可视化 Transformer 模型有两种常见的方法，一种是将注意力作为相关性分数进行可视化，这种方法通常用于单个注意力层。另一种是连接多个层。也就是平均所有层的注意力值。然而这种方法假设层与层之间是某种简单的关系进行累加，会导致一些不相关的区域被高亮，例如 rollout 方法其实就是假设层与层之间是线性关系。</p>\n<p>本文提出了一套可解释性方法。</p>\n<p>第一步，引入能够适用于正值和负值的相关性传播规则</p>\n<p>第二步，为非参数层提出了一个归一项</p>\n<p>第三步，集成了注意力和相关性分数并连接了多个注意力块的集成结果</p>\n<h2 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h2><p>在本文方法的第 3.1 和 3.2 章节都是在做一些定理的推导和证明，这里就不过多赘述和推导了。3.1 章节相关性和梯度，通过深度泰勒分解推导出相关性值的计算方式，以及非线性层正负激活值的处理问题。3.2 章节非参相关性传播，主要介绍了如果对于非参结构进行相关性值的传播以及通过乘法传播的过程可能会带来的相关性值爆炸的问题及归一化解决方式的形式化推导。</p>\n<h3 id=\"3-3-相关性和梯度扩散\"><a href=\"#3-3-相关性和梯度扩散\" class=\"headerlink\" title=\"3.3 相关性和梯度扩散\"></a>3.3 相关性和梯度扩散</h3><p>在 ViT 中，采用的是多头自注意力结构，注意力头的数量即为 $h$，每一个头的特征维度为 $d_h$ 因此一个注意力块的总维度 $d &#x3D; hd_h$。一个自注意力块被定义为下面的结构：</p>\n<p>$A^{(b)} &#x3D; softmax(\\frac{ {Q^{(b)}} \\cdot {K^{(b)}} }{\\sqrt{d_h}})$</p>\n<p>$O^{(b)} &#x3D; A^{(b)} \\cdot V^{(b)}$</p>\n<p>其中，$Q^{(b)}, K^{(b)}, V^{(b)} \\in \\mathbb{R}^{h \\times s \\times d_h}$ 分别代表 query,key 和 value。</p>\n<p>按照如上的定义， rollout 方法的计算为如下过程：</p>\n<p>$\\hat{A}^{(b)} &#x3D; I + \\mathbb{E_h}A^{(b)}$</p>\n<p>$rollout &#x3D; \\hat{A}^{(1)} \\cdot \\hat{A}^{(2)} \\cdot … \\cdot \\hat{A}^{(B)}$</p>\n<p>我们可以观察到 rollout 的结果是固定的，不会因目标类别的变化而产生影响。</p>\n<p>本文的过程为如下形式：</p>\n<p>$\\overline{A}^{(b)} &#x3D; I + \\mathbb{E_h}(\\bigtriangledown A^{(b)} \\odot R^{(n_b)})^{+}$</p>\n<p>$C &#x3D; \\overline{A}^{(1)} \\cdot \\overline{A}^{(2)} \\cdot … \\cdot \\overline{A}^{(B)}$</p>\n<p>其中 $\\mathbb{E_h}$ 是多个头输出结果的求平均得到的结果，而 $\\bigtriangledown A^{(b)}$ 为注意力映射 $A^{(b)}$ 的梯度，$R^{(n_b)}$ 为相关值，其计算方式在 3.1 章节中有详细的介绍和推导，其实就是通过泰勒分解进行计算得到的值。</p>\n<h3 id=\"3-4-得到图片的归因映射\"><a href=\"#3-4-得到图片的归因映射\" class=\"headerlink\" title=\"3.4 得到图片的归因映射\"></a>3.4 得到图片的归因映射</h3><p>通过上述的方法得到的矩阵 $C$ 的大小为 $s \\times s$。在常规的 ViT 结构中，$s$ 为 197，这里进行一个简要的介绍，想要更加详细了解的话可以读一下 ViT 和 Bert 的论文。在 ViT 中是将一个 $16 \\times 16$ 的图像块作为一个 patch，因此对于一个 $224 \\times 224$ 的图像输入进来，就会得到 $(224 \\times 224) \\div (16 \\times 16)$ 也就是 196 个 token。在 ViT 中，参考了 Bert 的设计，在这 196 个 token 前加了一个 [CLS] token，用于下游的分类任务。从下图即 ViT 的架构图中就能清晰得看出来这一设计。</p>\n<p><img src=\"/images/R_Rollout/ViT.png\"></p>\n<p>因此我们最后得到的矩阵 $C$ 的大小为 $197 \\times 197$，下一步就是实现从这个相关性矩阵到图像归因结果的映射了。这个步骤，在原文的代码中是通过 $C[0, 1:]$ 这个操作来实现的。在理解了，ViT 结构的 token 的内涵后，对于这一设计也很容易理解。首先，这个相关性矩阵的每一行就是每一个 token 与别的 token 的相关性映射。而只有第一个 [CLS] token 中包含类别信息，因此我们需要取出他，然而 [CLS] token 与他本身的映射关系在图像中是没有映射关系的，所以只取出了后 196 个值，也即 [CLS] token 与各个 patch token 的映射。最后，我们得到一个 $14 \\times 14$ 的矩阵，再通过双线性插值上采样到原始图像尺寸。</p>\n<p>整个算法完整的过程如下图所示：</p>\n<p><img src=\"/images/R_Rollout/R_Rollout.png\"></p>\n<h2 id=\"4-实验\"><a href=\"#4-实验\" class=\"headerlink\" title=\"4 实验\"></a>4 实验</h2><p>首先是定性实验，也就是提供了一些示例的不同方法的归因结果对比，分别是无类别归因和类别归因结果。</p>\n<p><img src=\"/images/R_Rollout/sample_results.png\"></p>\n<p><img src=\"/images/R_Rollout/class_specific.png\"></p>\n<p>然后是扰动实验，即基于归因结果按照相关值从小到大和从大到小的顺序去进行 mask 得出对于预测的影响计算得到 AUC。</p>\n<p><img src=\"/images/R_Rollout/perturbation.png\"></p>\n<p>接下来分别是分割实验和 Bert 归因实验的结果</p>\n<p><img src=\"/images/R_Rollout/segment.png\"></p>\n<p><img src=\"/images/R_Rollout/Bert.png\"></p>\n<p>在消融实验中，作者测试了不使用梯度信息，只对第一个注意力块和最后一个注意力块进行计算 这三个消融实验设置下的不同结果。</p>\n<p><img src=\"/images/R_Rollout/alation.png\"></p>\n<h2 id=\"5-结语\"><a href=\"#5-结语\" class=\"headerlink\" title=\"5 结语\"></a>5 结语</h2><p>在结语中，作者主要提到了几个点值得关注：</p>\n<ul>\n<li><p>与 CNN 相比， Transformer 中的非正激活函数，跳跃连接以及对自注意力矩阵乘法的建模等问题对于模型可解释来说都是很大的挑战。（黑盒模型则不需要关注于这些挑战，但是黑盒模型的时间效率太低了</p>\n</li>\n<li><p>可能是受限于注意力本身机制的问题，通过本文方法得到的解释结果依然存在碎片化解释的问题</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>今天给大家分享一篇文章 《Transformer Interpretability Beyond Attention Visualization》。这篇文章是 2021 年 CVPR 的文章，这篇工作中提出了一套新的可解释性方法来对 transformer 进行可解释性分析。</p>\n<p>transformer 的 attention 机制本身就能够输出具有一定可解释性的可视化结果，但是这套方案只能看到单个注意力块的结果，因此 rollout 应运而生。他能够累积所有注意力块的归因结果得到一个全局的归因，ViT 的解释分析方法使用的就是 rollout，但是 rollout 是一个静态的归因结果，因此作者在本文中提出一种 class-specific 的解释方法。在标题中，作者并没有明确得给自己的方法命名，所以这篇文章中为了书写方便将这个方法暂且称为 Relevance Rollout。</p>","more":"<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>这篇文章与 ViT 是同年发布的文章， ViT 一经发布后就席卷了 CV 领域的各个子任务，自然有人好奇 ViT 这个黑箱模型其中的机制到底是什么。</p>\n<p>而之前针对 transformer 进行可解释的方法只有 transformer 文章中提及的注意力图也可称为显著性图、以及 2020 年 ACL 的一篇文章 Rollout 采用启发式的方法传播注意力图得到最终的归因结果。</p>\n<p>作者提出的方法其实可以总结为两点，一是在 transformer 的可解释性工作中如何对注意力层做好解释分析，二是层与层之间残差连接结构的相关性做好分析是当前任务的难点，而作者提出的方法旨在解决这两个问题的。</p>\n<p>最终，作者在一些在一些 transformer 网络以及一些任务中验证了其方法的有效性。</p>\n<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><p>现在 NLP 和 CV 领域的 sota 方法基本上都被 transformer based model 刷了一遍，而且今年大火的大模型的背后也是 transformer。因此，关于可视化 transformer 的决策过程的研究对于我们后续进一步开展相关研究至关重要。</p>\n<p>transformer 的核心结构就是 self-attention layers，在自注意力层中会为每两个 token 之间分配一个注意力值。在 NLP 中，这个 token 就是一个单词或者单词的一部分，在 CV 领域 这个 token 可以是图像的一个 patch。</p>\n<p>可视化 Transformer 模型有两种常见的方法，一种是将注意力作为相关性分数进行可视化，这种方法通常用于单个注意力层。另一种是连接多个层。也就是平均所有层的注意力值。然而这种方法假设层与层之间是某种简单的关系进行累加，会导致一些不相关的区域被高亮，例如 rollout 方法其实就是假设层与层之间是线性关系。</p>\n<p>本文提出了一套可解释性方法。</p>\n<p>第一步，引入能够适用于正值和负值的相关性传播规则</p>\n<p>第二步，为非参数层提出了一个归一项</p>\n<p>第三步，集成了注意力和相关性分数并连接了多个注意力块的集成结果</p>\n<h2 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h2><p>在本文方法的第 3.1 和 3.2 章节都是在做一些定理的推导和证明，这里就不过多赘述和推导了。3.1 章节相关性和梯度，通过深度泰勒分解推导出相关性值的计算方式，以及非线性层正负激活值的处理问题。3.2 章节非参相关性传播，主要介绍了如果对于非参结构进行相关性值的传播以及通过乘法传播的过程可能会带来的相关性值爆炸的问题及归一化解决方式的形式化推导。</p>\n<h3 id=\"3-3-相关性和梯度扩散\"><a href=\"#3-3-相关性和梯度扩散\" class=\"headerlink\" title=\"3.3 相关性和梯度扩散\"></a>3.3 相关性和梯度扩散</h3><p>在 ViT 中，采用的是多头自注意力结构，注意力头的数量即为 $h$，每一个头的特征维度为 $d_h$ 因此一个注意力块的总维度 $d &#x3D; hd_h$。一个自注意力块被定义为下面的结构：</p>\n<p>$A^{(b)} &#x3D; softmax(\\frac{ {Q^{(b)}} \\cdot {K^{(b)}} }{\\sqrt{d_h}})$</p>\n<p>$O^{(b)} &#x3D; A^{(b)} \\cdot V^{(b)}$</p>\n<p>其中，$Q^{(b)}, K^{(b)}, V^{(b)} \\in \\mathbb{R}^{h \\times s \\times d_h}$ 分别代表 query,key 和 value。</p>\n<p>按照如上的定义， rollout 方法的计算为如下过程：</p>\n<p>$\\hat{A}^{(b)} &#x3D; I + \\mathbb{E_h}A^{(b)}$</p>\n<p>$rollout &#x3D; \\hat{A}^{(1)} \\cdot \\hat{A}^{(2)} \\cdot … \\cdot \\hat{A}^{(B)}$</p>\n<p>我们可以观察到 rollout 的结果是固定的，不会因目标类别的变化而产生影响。</p>\n<p>本文的过程为如下形式：</p>\n<p>$\\overline{A}^{(b)} &#x3D; I + \\mathbb{E_h}(\\bigtriangledown A^{(b)} \\odot R^{(n_b)})^{+}$</p>\n<p>$C &#x3D; \\overline{A}^{(1)} \\cdot \\overline{A}^{(2)} \\cdot … \\cdot \\overline{A}^{(B)}$</p>\n<p>其中 $\\mathbb{E_h}$ 是多个头输出结果的求平均得到的结果，而 $\\bigtriangledown A^{(b)}$ 为注意力映射 $A^{(b)}$ 的梯度，$R^{(n_b)}$ 为相关值，其计算方式在 3.1 章节中有详细的介绍和推导，其实就是通过泰勒分解进行计算得到的值。</p>\n<h3 id=\"3-4-得到图片的归因映射\"><a href=\"#3-4-得到图片的归因映射\" class=\"headerlink\" title=\"3.4 得到图片的归因映射\"></a>3.4 得到图片的归因映射</h3><p>通过上述的方法得到的矩阵 $C$ 的大小为 $s \\times s$。在常规的 ViT 结构中，$s$ 为 197，这里进行一个简要的介绍，想要更加详细了解的话可以读一下 ViT 和 Bert 的论文。在 ViT 中是将一个 $16 \\times 16$ 的图像块作为一个 patch，因此对于一个 $224 \\times 224$ 的图像输入进来，就会得到 $(224 \\times 224) \\div (16 \\times 16)$ 也就是 196 个 token。在 ViT 中，参考了 Bert 的设计，在这 196 个 token 前加了一个 [CLS] token，用于下游的分类任务。从下图即 ViT 的架构图中就能清晰得看出来这一设计。</p>\n<p><img src=\"/images/R_Rollout/ViT.png\"></p>\n<p>因此我们最后得到的矩阵 $C$ 的大小为 $197 \\times 197$，下一步就是实现从这个相关性矩阵到图像归因结果的映射了。这个步骤，在原文的代码中是通过 $C[0, 1:]$ 这个操作来实现的。在理解了，ViT 结构的 token 的内涵后，对于这一设计也很容易理解。首先，这个相关性矩阵的每一行就是每一个 token 与别的 token 的相关性映射。而只有第一个 [CLS] token 中包含类别信息，因此我们需要取出他，然而 [CLS] token 与他本身的映射关系在图像中是没有映射关系的，所以只取出了后 196 个值，也即 [CLS] token 与各个 patch token 的映射。最后，我们得到一个 $14 \\times 14$ 的矩阵，再通过双线性插值上采样到原始图像尺寸。</p>\n<p>整个算法完整的过程如下图所示：</p>\n<p><img src=\"/images/R_Rollout/R_Rollout.png\"></p>\n<h2 id=\"4-实验\"><a href=\"#4-实验\" class=\"headerlink\" title=\"4 实验\"></a>4 实验</h2><p>首先是定性实验，也就是提供了一些示例的不同方法的归因结果对比，分别是无类别归因和类别归因结果。</p>\n<p><img src=\"/images/R_Rollout/sample_results.png\"></p>\n<p><img src=\"/images/R_Rollout/class_specific.png\"></p>\n<p>然后是扰动实验，即基于归因结果按照相关值从小到大和从大到小的顺序去进行 mask 得出对于预测的影响计算得到 AUC。</p>\n<p><img src=\"/images/R_Rollout/perturbation.png\"></p>\n<p>接下来分别是分割实验和 Bert 归因实验的结果</p>\n<p><img src=\"/images/R_Rollout/segment.png\"></p>\n<p><img src=\"/images/R_Rollout/Bert.png\"></p>\n<p>在消融实验中，作者测试了不使用梯度信息，只对第一个注意力块和最后一个注意力块进行计算 这三个消融实验设置下的不同结果。</p>\n<p><img src=\"/images/R_Rollout/alation.png\"></p>\n<h2 id=\"5-结语\"><a href=\"#5-结语\" class=\"headerlink\" title=\"5 结语\"></a>5 结语</h2><p>在结语中，作者主要提到了几个点值得关注：</p>\n<ul>\n<li><p>与 CNN 相比， Transformer 中的非正激活函数，跳跃连接以及对自注意力矩阵乘法的建模等问题对于模型可解释来说都是很大的挑战。（黑盒模型则不需要关注于这些挑战，但是黑盒模型的时间效率太低了</p>\n</li>\n<li><p>可能是受限于注意力本身机制的问题，通过本文方法得到的解释结果依然存在碎片化解释的问题</p>\n</li>\n</ul>"},{"title":"VAE 原理及实现","date":"2023-02-07T02:47:38.000Z","_content":"\n在过去的几年，Variational Autoencoders(VAE)一直是无监督学习中学习复杂分布的常用方法。VAE 之所以如此具有吸引力，是因为他们是建立在标准函数估计器（神经网络）之上的，并且可以通过随机梯度下降方法进行训练。在这篇文章中将会介绍 VAE 背后的原理。\n\n## 1 引言\n生成模型就是为了处理模型的分布函数 $P(X)$。其中 $P(X)$ 定义了数据点 $X$ 在潜在高维空间的分布情况。举个例子，我们以图片为例，每一个“数据点”（图片）有数以万计的维度（像素），并且生成模型的任务就是去捕捉像素之间的依赖，例如，相邻的像素有相似的颜色。捕捉这些依赖的确切含义取决于我们想要模型去做什么。","source":"_posts/VAE-原理及实现.md","raw":"---\ntitle: VAE 原理及实现\ndate: 2023-02-07 10:47:38\ntags:\n---\n\n在过去的几年，Variational Autoencoders(VAE)一直是无监督学习中学习复杂分布的常用方法。VAE 之所以如此具有吸引力，是因为他们是建立在标准函数估计器（神经网络）之上的，并且可以通过随机梯度下降方法进行训练。在这篇文章中将会介绍 VAE 背后的原理。\n\n## 1 引言\n生成模型就是为了处理模型的分布函数 $P(X)$。其中 $P(X)$ 定义了数据点 $X$ 在潜在高维空间的分布情况。举个例子，我们以图片为例，每一个“数据点”（图片）有数以万计的维度（像素），并且生成模型的任务就是去捕捉像素之间的依赖，例如，相邻的像素有相似的颜色。捕捉这些依赖的确切含义取决于我们想要模型去做什么。","slug":"VAE-原理及实现","published":1,"updated":"2023-02-08T13:56:39.374Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp0000aqnwy376whvfy","content":"<p>在过去的几年，Variational Autoencoders(VAE)一直是无监督学习中学习复杂分布的常用方法。VAE 之所以如此具有吸引力，是因为他们是建立在标准函数估计器（神经网络）之上的，并且可以通过随机梯度下降方法进行训练。在这篇文章中将会介绍 VAE 背后的原理。</p>\n<h2 id=\"1-引言\"><a href=\"#1-引言\" class=\"headerlink\" title=\"1 引言\"></a>1 引言</h2><p>生成模型就是为了处理模型的分布函数 $P(X)$。其中 $P(X)$ 定义了数据点 $X$ 在潜在高维空间的分布情况。举个例子，我们以图片为例，每一个“数据点”（图片）有数以万计的维度（像素），并且生成模型的任务就是去捕捉像素之间的依赖，例如，相邻的像素有相似的颜色。捕捉这些依赖的确切含义取决于我们想要模型去做什么。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在过去的几年，Variational Autoencoders(VAE)一直是无监督学习中学习复杂分布的常用方法。VAE 之所以如此具有吸引力，是因为他们是建立在标准函数估计器（神经网络）之上的，并且可以通过随机梯度下降方法进行训练。在这篇文章中将会介绍 VAE 背后的原理。</p>\n<h2 id=\"1-引言\"><a href=\"#1-引言\" class=\"headerlink\" title=\"1 引言\"></a>1 引言</h2><p>生成模型就是为了处理模型的分布函数 $P(X)$。其中 $P(X)$ 定义了数据点 $X$ 在潜在高维空间的分布情况。举个例子，我们以图片为例，每一个“数据点”（图片）有数以万计的维度（像素），并且生成模型的任务就是去捕捉像素之间的依赖，例如，相邻的像素有相似的颜色。捕捉这些依赖的确切含义取决于我们想要模型去做什么。</p>\n"},{"title":"diffusion 原理及代码","date":"2023-06-21T06:34:01.000Z","_content":"\n## Referrence\nhttps://huggingface.co/blog/annotated-diffusion","source":"_posts/diffusion-原理及代码.md","raw":"---\ntitle: diffusion 原理及代码\ndate: 2023-06-21 14:34:01\ntags:\n---\n\n## Referrence\nhttps://huggingface.co/blog/annotated-diffusion","slug":"diffusion-原理及代码","published":1,"updated":"2023-06-27T08:30:51.907Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp0000cqnwyczwhag2n","content":"<h2 id=\"Referrence\"><a href=\"#Referrence\" class=\"headerlink\" title=\"Referrence\"></a>Referrence</h2><p><a href=\"https://huggingface.co/blog/annotated-diffusion\">https://huggingface.co/blog/annotated-diffusion</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Referrence\"><a href=\"#Referrence\" class=\"headerlink\" title=\"Referrence\"></a>Referrence</h2><p><a href=\"https://huggingface.co/blog/annotated-diffusion\">https://huggingface.co/blog/annotated-diffusion</a></p>\n"},{"title":"一问读懂 L1 正则化(Lasso)、L2 正则化(Ridge)、权重衰减","date":"2022-10-16T10:31:52.000Z","toc":true,"_content":"正则化\n在训练模型的过程中经常会遇到两个问题：过拟合和欠拟合\n正则化技术则是用于防止过拟合的一大利器。\n那么正则化技术是怎么样使得模型不倾向于过拟合的呢？\n通过缩减系数项使得模型对于自变量的敏感度降低，进一步降低模型的整体方差。\n<!-- more -->\n在实践中主要的正则化技术包括：\nL2 正则化\nL1 正则化\n数据增强\nDropout\nEarly Stopping\n本文中主要介绍， L1 与 L2 正则化，其中他们二者分别也可以被称为 lasso 回归和 ridge 回归。\nlasso 回归和 ridge 回归有哪些区别？为什么 lasso 回归可以用于特征选择而后者不能？L2 正则化和权重衰减又有这哪些细微差别？\n这些问题看完本文后都能够告诉你答案！\nL1 正则化 (Lasso) 和 L2 正则化 (Ridge)\n在介绍 L1 正则化和 L2 正则化之前，先对于范数概念进行介绍。\n在数学上，针对于 p-norm 的严格定义是：\n$$ ||x||p:=(∑i=1n|xi|p)(1/p) $$\n其中，当 p 取 1 时被称为 1-norm，也就是提到的 L1-norm 同理 L2-norm。\n而我们的 L1 正则化与 L2 正则化实际上也就是在损失函数中加入 L1/L2 范数项，并且使用 λ 作为惩罚系数，防止过拟合。\n二者的公式如下：\n$$ Lasso(w^)=∑i=1n(yi−xiw^)2+λ∑j=1m|wi| $$\n \n关于这两个正则项的主要的不同，包括以下几点：\nL2 计算起来更方便，而 L1 特别是在非稀疏向量上的计算效率就很低；\nL1 正则项的输出稀疏，会把不重要的特征直接置0，而 L2 不会；\nL2 有唯一解，而 L1 不是。\n关于计算方便与唯一解，是因为 L2 为两次项，求最小值则可以直接通过求导后计算最小值即可。但是 L1 为一个绝对值项，对于一个绝对值项去计算最小值是非常麻烦的。\n关于输出稀疏这一点，也是 L1 正则项能够被用于特征选择的原因。\n这里简要的介绍一下，因为 L1 不管大小为多少梯度都为 1 或者 -1，所以每次更新都是稳步向 0 前进。而反观 L2 就会发现，它的梯度越靠近 0，就会变得越小。\n也就是说 L1 正则经过一定步数很可能变为0，而 L2 则不太可能，因为在其值小的时候梯度也会变小。这就造就了 L1 的输出稀疏的特性。\n正则化的原理\n首先，我们推导一下 L2 正则化产生作用的原理。\n针对损失函数：\n \n我们对  求偏导数得到\n \n基于反向传播算法的  的学习规则变为：\n \n通过上式可以看出，正则化过程中，会使得网络的权重更小。而权重更小意味着，如果我们在这里或那里改变一些随机的输入，网络的行为不会有太大的变化，这反过来使正则化很难学习到数据中的局部噪声。这也是 L2 正则化被称为权重衰减的原因，因为它是权重减小。\n权重衰减与 L2 正则化\nL2 正则化和权重衰减并不是一回事，但是在 SGD 中两个方法是等价的。\n以  为衰减因子，给出权值的衰减方程\n \n与上面 L2 正则化的式子进行对比，可以看出在SGD中将 L2 正则化的更新方程重新参数化后（  ）二者之间是等价的\n结语\n在最后，我们将上文讲述的内容进行总结：\nL1 正则化与 L2 正则化的损失函数是及其相似的，二者使用的 范数项不同\nL1 正则化具有输出稀疏的特点，并且能够用于特征选择；L2 正则化的计算较快\n权重衰减和 L2 正则化概念上比较相似，但是不能够完全等价\nReferrence\nLasso and Ridge Regression Tutorial\n权值衰减和L2正则化傻傻分不清楚？\nl1正则与l2正则的特点是什么，各有什么优势？","source":"_posts/一问读懂-L1-正则化-Lasso-、L2-正则化-Ridge-、权重衰减.md","raw":"---\ntitle: 一问读懂 L1 正则化(Lasso)、L2 正则化(Ridge)、权重衰减\ndate: 2022-10-16 18:31:52\ntags:\n- 涨知识\ncategories:\n- [机器学习]\n- [正则化]\ntoc: true\n---\n正则化\n在训练模型的过程中经常会遇到两个问题：过拟合和欠拟合\n正则化技术则是用于防止过拟合的一大利器。\n那么正则化技术是怎么样使得模型不倾向于过拟合的呢？\n通过缩减系数项使得模型对于自变量的敏感度降低，进一步降低模型的整体方差。\n<!-- more -->\n在实践中主要的正则化技术包括：\nL2 正则化\nL1 正则化\n数据增强\nDropout\nEarly Stopping\n本文中主要介绍， L1 与 L2 正则化，其中他们二者分别也可以被称为 lasso 回归和 ridge 回归。\nlasso 回归和 ridge 回归有哪些区别？为什么 lasso 回归可以用于特征选择而后者不能？L2 正则化和权重衰减又有这哪些细微差别？\n这些问题看完本文后都能够告诉你答案！\nL1 正则化 (Lasso) 和 L2 正则化 (Ridge)\n在介绍 L1 正则化和 L2 正则化之前，先对于范数概念进行介绍。\n在数学上，针对于 p-norm 的严格定义是：\n$$ ||x||p:=(∑i=1n|xi|p)(1/p) $$\n其中，当 p 取 1 时被称为 1-norm，也就是提到的 L1-norm 同理 L2-norm。\n而我们的 L1 正则化与 L2 正则化实际上也就是在损失函数中加入 L1/L2 范数项，并且使用 λ 作为惩罚系数，防止过拟合。\n二者的公式如下：\n$$ Lasso(w^)=∑i=1n(yi−xiw^)2+λ∑j=1m|wi| $$\n \n关于这两个正则项的主要的不同，包括以下几点：\nL2 计算起来更方便，而 L1 特别是在非稀疏向量上的计算效率就很低；\nL1 正则项的输出稀疏，会把不重要的特征直接置0，而 L2 不会；\nL2 有唯一解，而 L1 不是。\n关于计算方便与唯一解，是因为 L2 为两次项，求最小值则可以直接通过求导后计算最小值即可。但是 L1 为一个绝对值项，对于一个绝对值项去计算最小值是非常麻烦的。\n关于输出稀疏这一点，也是 L1 正则项能够被用于特征选择的原因。\n这里简要的介绍一下，因为 L1 不管大小为多少梯度都为 1 或者 -1，所以每次更新都是稳步向 0 前进。而反观 L2 就会发现，它的梯度越靠近 0，就会变得越小。\n也就是说 L1 正则经过一定步数很可能变为0，而 L2 则不太可能，因为在其值小的时候梯度也会变小。这就造就了 L1 的输出稀疏的特性。\n正则化的原理\n首先，我们推导一下 L2 正则化产生作用的原理。\n针对损失函数：\n \n我们对  求偏导数得到\n \n基于反向传播算法的  的学习规则变为：\n \n通过上式可以看出，正则化过程中，会使得网络的权重更小。而权重更小意味着，如果我们在这里或那里改变一些随机的输入，网络的行为不会有太大的变化，这反过来使正则化很难学习到数据中的局部噪声。这也是 L2 正则化被称为权重衰减的原因，因为它是权重减小。\n权重衰减与 L2 正则化\nL2 正则化和权重衰减并不是一回事，但是在 SGD 中两个方法是等价的。\n以  为衰减因子，给出权值的衰减方程\n \n与上面 L2 正则化的式子进行对比，可以看出在SGD中将 L2 正则化的更新方程重新参数化后（  ）二者之间是等价的\n结语\n在最后，我们将上文讲述的内容进行总结：\nL1 正则化与 L2 正则化的损失函数是及其相似的，二者使用的 范数项不同\nL1 正则化具有输出稀疏的特点，并且能够用于特征选择；L2 正则化的计算较快\n权重衰减和 L2 正则化概念上比较相似，但是不能够完全等价\nReferrence\nLasso and Ridge Regression Tutorial\n权值衰减和L2正则化傻傻分不清楚？\nl1正则与l2正则的特点是什么，各有什么优势？","slug":"一问读懂-L1-正则化-Lasso-、L2-正则化-Ridge-、权重衰减","published":1,"updated":"2022-11-21T09:29:21.342Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp1000dqnwyhmt6hkb6","content":"<p>正则化<br>在训练模型的过程中经常会遇到两个问题：过拟合和欠拟合<br>正则化技术则是用于防止过拟合的一大利器。<br>那么正则化技术是怎么样使得模型不倾向于过拟合的呢？<br>通过缩减系数项使得模型对于自变量的敏感度降低，进一步降低模型的整体方差。</p>\n<span id=\"more\"></span>\n<p>在实践中主要的正则化技术包括：<br>L2 正则化<br>L1 正则化<br>数据增强<br>Dropout<br>Early Stopping<br>本文中主要介绍， L1 与 L2 正则化，其中他们二者分别也可以被称为 lasso 回归和 ridge 回归。<br>lasso 回归和 ridge 回归有哪些区别？为什么 lasso 回归可以用于特征选择而后者不能？L2 正则化和权重衰减又有这哪些细微差别？<br>这些问题看完本文后都能够告诉你答案！<br>L1 正则化 (Lasso) 和 L2 正则化 (Ridge)<br>在介绍 L1 正则化和 L2 正则化之前，先对于范数概念进行介绍。<br>在数学上，针对于 p-norm 的严格定义是：<br>$$ ||x||p:&#x3D;(∑i&#x3D;1n|xi|p)(1&#x2F;p) $$<br>其中，当 p 取 1 时被称为 1-norm，也就是提到的 L1-norm 同理 L2-norm。<br>而我们的 L1 正则化与 L2 正则化实际上也就是在损失函数中加入 L1&#x2F;L2 范数项，并且使用 λ 作为惩罚系数，防止过拟合。<br>二者的公式如下：<br>$$ Lasso(w^)&#x3D;∑i&#x3D;1n(yi−xiw^)2+λ∑j&#x3D;1m|wi| $$</p>\n<p>关于这两个正则项的主要的不同，包括以下几点：<br>L2 计算起来更方便，而 L1 特别是在非稀疏向量上的计算效率就很低；<br>L1 正则项的输出稀疏，会把不重要的特征直接置0，而 L2 不会；<br>L2 有唯一解，而 L1 不是。<br>关于计算方便与唯一解，是因为 L2 为两次项，求最小值则可以直接通过求导后计算最小值即可。但是 L1 为一个绝对值项，对于一个绝对值项去计算最小值是非常麻烦的。<br>关于输出稀疏这一点，也是 L1 正则项能够被用于特征选择的原因。<br>这里简要的介绍一下，因为 L1 不管大小为多少梯度都为 1 或者 -1，所以每次更新都是稳步向 0 前进。而反观 L2 就会发现，它的梯度越靠近 0，就会变得越小。<br>也就是说 L1 正则经过一定步数很可能变为0，而 L2 则不太可能，因为在其值小的时候梯度也会变小。这就造就了 L1 的输出稀疏的特性。<br>正则化的原理<br>首先，我们推导一下 L2 正则化产生作用的原理。<br>针对损失函数：</p>\n<p>我们对  求偏导数得到</p>\n<p>基于反向传播算法的  的学习规则变为：</p>\n<p>通过上式可以看出，正则化过程中，会使得网络的权重更小。而权重更小意味着，如果我们在这里或那里改变一些随机的输入，网络的行为不会有太大的变化，这反过来使正则化很难学习到数据中的局部噪声。这也是 L2 正则化被称为权重衰减的原因，因为它是权重减小。<br>权重衰减与 L2 正则化<br>L2 正则化和权重衰减并不是一回事，但是在 SGD 中两个方法是等价的。<br>以  为衰减因子，给出权值的衰减方程</p>\n<p>与上面 L2 正则化的式子进行对比，可以看出在SGD中将 L2 正则化的更新方程重新参数化后（  ）二者之间是等价的<br>结语<br>在最后，我们将上文讲述的内容进行总结：<br>L1 正则化与 L2 正则化的损失函数是及其相似的，二者使用的 范数项不同<br>L1 正则化具有输出稀疏的特点，并且能够用于特征选择；L2 正则化的计算较快<br>权重衰减和 L2 正则化概念上比较相似，但是不能够完全等价<br>Referrence<br>Lasso and Ridge Regression Tutorial<br>权值衰减和L2正则化傻傻分不清楚？<br>l1正则与l2正则的特点是什么，各有什么优势？</p>\n","site":{"data":{}},"excerpt":"<p>正则化<br>在训练模型的过程中经常会遇到两个问题：过拟合和欠拟合<br>正则化技术则是用于防止过拟合的一大利器。<br>那么正则化技术是怎么样使得模型不倾向于过拟合的呢？<br>通过缩减系数项使得模型对于自变量的敏感度降低，进一步降低模型的整体方差。</p>","more":"<p>在实践中主要的正则化技术包括：<br>L2 正则化<br>L1 正则化<br>数据增强<br>Dropout<br>Early Stopping<br>本文中主要介绍， L1 与 L2 正则化，其中他们二者分别也可以被称为 lasso 回归和 ridge 回归。<br>lasso 回归和 ridge 回归有哪些区别？为什么 lasso 回归可以用于特征选择而后者不能？L2 正则化和权重衰减又有这哪些细微差别？<br>这些问题看完本文后都能够告诉你答案！<br>L1 正则化 (Lasso) 和 L2 正则化 (Ridge)<br>在介绍 L1 正则化和 L2 正则化之前，先对于范数概念进行介绍。<br>在数学上，针对于 p-norm 的严格定义是：<br>$$ ||x||p:&#x3D;(∑i&#x3D;1n|xi|p)(1&#x2F;p) $$<br>其中，当 p 取 1 时被称为 1-norm，也就是提到的 L1-norm 同理 L2-norm。<br>而我们的 L1 正则化与 L2 正则化实际上也就是在损失函数中加入 L1&#x2F;L2 范数项，并且使用 λ 作为惩罚系数，防止过拟合。<br>二者的公式如下：<br>$$ Lasso(w^)&#x3D;∑i&#x3D;1n(yi−xiw^)2+λ∑j&#x3D;1m|wi| $$</p>\n<p>关于这两个正则项的主要的不同，包括以下几点：<br>L2 计算起来更方便，而 L1 特别是在非稀疏向量上的计算效率就很低；<br>L1 正则项的输出稀疏，会把不重要的特征直接置0，而 L2 不会；<br>L2 有唯一解，而 L1 不是。<br>关于计算方便与唯一解，是因为 L2 为两次项，求最小值则可以直接通过求导后计算最小值即可。但是 L1 为一个绝对值项，对于一个绝对值项去计算最小值是非常麻烦的。<br>关于输出稀疏这一点，也是 L1 正则项能够被用于特征选择的原因。<br>这里简要的介绍一下，因为 L1 不管大小为多少梯度都为 1 或者 -1，所以每次更新都是稳步向 0 前进。而反观 L2 就会发现，它的梯度越靠近 0，就会变得越小。<br>也就是说 L1 正则经过一定步数很可能变为0，而 L2 则不太可能，因为在其值小的时候梯度也会变小。这就造就了 L1 的输出稀疏的特性。<br>正则化的原理<br>首先，我们推导一下 L2 正则化产生作用的原理。<br>针对损失函数：</p>\n<p>我们对  求偏导数得到</p>\n<p>基于反向传播算法的  的学习规则变为：</p>\n<p>通过上式可以看出，正则化过程中，会使得网络的权重更小。而权重更小意味着，如果我们在这里或那里改变一些随机的输入，网络的行为不会有太大的变化，这反过来使正则化很难学习到数据中的局部噪声。这也是 L2 正则化被称为权重衰减的原因，因为它是权重减小。<br>权重衰减与 L2 正则化<br>L2 正则化和权重衰减并不是一回事，但是在 SGD 中两个方法是等价的。<br>以  为衰减因子，给出权值的衰减方程</p>\n<p>与上面 L2 正则化的式子进行对比，可以看出在SGD中将 L2 正则化的更新方程重新参数化后（  ）二者之间是等价的<br>结语<br>在最后，我们将上文讲述的内容进行总结：<br>L1 正则化与 L2 正则化的损失函数是及其相似的，二者使用的 范数项不同<br>L1 正则化具有输出稀疏的特点，并且能够用于特征选择；L2 正则化的计算较快<br>权重衰减和 L2 正则化概念上比较相似，但是不能够完全等价<br>Referrence<br>Lasso and Ridge Regression Tutorial<br>权值衰减和L2正则化傻傻分不清楚？<br>l1正则与l2正则的特点是什么，各有什么优势？</p>"},{"title":"Hello World","cover":"/gallery/cover/cover.jpeg","thumbnails":"/gallery/thumbnails/cover.jpeg","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ncover: /gallery/cover/cover.jpeg\nthumbnails: /gallery/thumbnails/cover.jpeg\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2022-11-02T04:31:13.677Z","updated":"2022-11-02T04:31:13.677Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp2000mqnwy9hpnfvn9","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"优化算法总结","date":"2023-02-10T07:43:03.000Z","_content":"\n优化和深度学习的目标是不同的。前者主要关注的是最小化目标，而后者则关注在给定有限数据量的情况下寻找合适的模型。具体而言，优化的目标是为了减少训练误差，而深度学习的目的除了在减小训练误差以外还需要减少泛化误差。","source":"_posts/优化算法总结.md","raw":"---\ntitle: 优化算法总结\ndate: 2023-02-10 15:43:03\ntags:\n---\n\n优化和深度学习的目标是不同的。前者主要关注的是最小化目标，而后者则关注在给定有限数据量的情况下寻找合适的模型。具体而言，优化的目标是为了减少训练误差，而深度学习的目的除了在减小训练误差以外还需要减少泛化误差。","slug":"优化算法总结","published":1,"updated":"2023-02-21T13:03:44.766Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp3000nqnwye4so09vi","content":"<p>优化和深度学习的目标是不同的。前者主要关注的是最小化目标，而后者则关注在给定有限数据量的情况下寻找合适的模型。具体而言，优化的目标是为了减少训练误差，而深度学习的目的除了在减小训练误差以外还需要减少泛化误差。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>优化和深度学习的目标是不同的。前者主要关注的是最小化目标，而后者则关注在给定有限数据量的情况下寻找合适的模型。具体而言，优化的目标是为了减少训练误差，而深度学习的目的除了在减小训练误差以外还需要减少泛化误差。</p>\n"},{"title":"准备工作：统计与因果模型","date":"2022-11-17T02:27:48.000Z","_content":"\n## 1 辛普森悖论\n\n辛普森悖论指出了数据中存在的一个现象，对整个整体成立的统计关系但是在每个个体中是相反的。\n\n这里举一个例子。我们记录了 700 个病人的用药情况和康复情况。其中 350 个病人选择用药，另外 350 个病人选择不用药。详细的数据如下表所示：\n\n|               | Drug                          | No Drug                       |\n|---------------|-------------------------------|-------------------------------|\n| Men           | 81 out of 87 recovered(93%)   | 234 out of 270 recovered(87%) |\n| Women         | 192 out of 263 recovered(73%) | 55 out of 80 recovered(69%)   |\n| Combined data | 273 out of 350 recovered(78%) | 289 out of 350 recovered(83%) |\n\n## 2 因果关系的定义\n\n因果关系的定义: 如果变量 Y 依赖于变量 X 的值那么就可以称 X 与 Y 存在因果关系。\n\n## 3 结构因果模型（Structural Causal Models, SCM）与图模型\n","source":"_posts/准备工作：统计与因果模型.md","raw":"---\ntitle: 准备工作：统计与因果模型\ndate: 2022-11-17 10:27:48\ntags:\n---\n\n## 1 辛普森悖论\n\n辛普森悖论指出了数据中存在的一个现象，对整个整体成立的统计关系但是在每个个体中是相反的。\n\n这里举一个例子。我们记录了 700 个病人的用药情况和康复情况。其中 350 个病人选择用药，另外 350 个病人选择不用药。详细的数据如下表所示：\n\n|               | Drug                          | No Drug                       |\n|---------------|-------------------------------|-------------------------------|\n| Men           | 81 out of 87 recovered(93%)   | 234 out of 270 recovered(87%) |\n| Women         | 192 out of 263 recovered(73%) | 55 out of 80 recovered(69%)   |\n| Combined data | 273 out of 350 recovered(78%) | 289 out of 350 recovered(83%) |\n\n## 2 因果关系的定义\n\n因果关系的定义: 如果变量 Y 依赖于变量 X 的值那么就可以称 X 与 Y 存在因果关系。\n\n## 3 结构因果模型（Structural Causal Models, SCM）与图模型\n","slug":"准备工作：统计与因果模型","published":1,"updated":"2022-11-21T08:59:59.793Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp3000oqnwy94grd9jq","content":"<h2 id=\"1-辛普森悖论\"><a href=\"#1-辛普森悖论\" class=\"headerlink\" title=\"1 辛普森悖论\"></a>1 辛普森悖论</h2><p>辛普森悖论指出了数据中存在的一个现象，对整个整体成立的统计关系但是在每个个体中是相反的。</p>\n<p>这里举一个例子。我们记录了 700 个病人的用药情况和康复情况。其中 350 个病人选择用药，另外 350 个病人选择不用药。详细的数据如下表所示：</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Drug</th>\n<th>No Drug</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Men</td>\n<td>81 out of 87 recovered(93%)</td>\n<td>234 out of 270 recovered(87%)</td>\n</tr>\n<tr>\n<td>Women</td>\n<td>192 out of 263 recovered(73%)</td>\n<td>55 out of 80 recovered(69%)</td>\n</tr>\n<tr>\n<td>Combined data</td>\n<td>273 out of 350 recovered(78%)</td>\n<td>289 out of 350 recovered(83%)</td>\n</tr>\n</tbody></table>\n<h2 id=\"2-因果关系的定义\"><a href=\"#2-因果关系的定义\" class=\"headerlink\" title=\"2 因果关系的定义\"></a>2 因果关系的定义</h2><p>因果关系的定义: 如果变量 Y 依赖于变量 X 的值那么就可以称 X 与 Y 存在因果关系。</p>\n<h2 id=\"3-结构因果模型（Structural-Causal-Models-SCM）与图模型\"><a href=\"#3-结构因果模型（Structural-Causal-Models-SCM）与图模型\" class=\"headerlink\" title=\"3 结构因果模型（Structural Causal Models, SCM）与图模型\"></a>3 结构因果模型（Structural Causal Models, SCM）与图模型</h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1-辛普森悖论\"><a href=\"#1-辛普森悖论\" class=\"headerlink\" title=\"1 辛普森悖论\"></a>1 辛普森悖论</h2><p>辛普森悖论指出了数据中存在的一个现象，对整个整体成立的统计关系但是在每个个体中是相反的。</p>\n<p>这里举一个例子。我们记录了 700 个病人的用药情况和康复情况。其中 350 个病人选择用药，另外 350 个病人选择不用药。详细的数据如下表所示：</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Drug</th>\n<th>No Drug</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Men</td>\n<td>81 out of 87 recovered(93%)</td>\n<td>234 out of 270 recovered(87%)</td>\n</tr>\n<tr>\n<td>Women</td>\n<td>192 out of 263 recovered(73%)</td>\n<td>55 out of 80 recovered(69%)</td>\n</tr>\n<tr>\n<td>Combined data</td>\n<td>273 out of 350 recovered(78%)</td>\n<td>289 out of 350 recovered(83%)</td>\n</tr>\n</tbody></table>\n<h2 id=\"2-因果关系的定义\"><a href=\"#2-因果关系的定义\" class=\"headerlink\" title=\"2 因果关系的定义\"></a>2 因果关系的定义</h2><p>因果关系的定义: 如果变量 Y 依赖于变量 X 的值那么就可以称 X 与 Y 存在因果关系。</p>\n<h2 id=\"3-结构因果模型（Structural-Causal-Models-SCM）与图模型\"><a href=\"#3-结构因果模型（Structural-Causal-Models-SCM）与图模型\" class=\"headerlink\" title=\"3 结构因果模型（Structural Causal Models, SCM）与图模型\"></a>3 结构因果模型（Structural Causal Models, SCM）与图模型</h2>"},{"title":"主成分分析（PCA）","date":"2022-11-02T04:32:24.000Z","toc":true,"_content":"\n主成分分析（principal component analysis, PCA）是一种常用的无监督学习方法，这一方法利用正交变换把由线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，线性无关的变量称为主成分。主成分的个数通常小于原始变量的个数，所以主成分分析属于降维方法。\n\n本文主要针对使用和实现进行介绍，如果想要针对算法推导过程进行学习，可以参考李航老师的《统计学习方法》。\n<!-- more -->\n## 原理介绍\n\n主成分分析方法就是将原始数据通过正交变化的方法投影到方差最大的几个方向上，每一个方向也就是一个主成分。\n关于方差最大的解释。假如有两个变量x1 和 x2，三个样本点 A、B、C，样本分布在由 x1 和 x2 组成的坐标系中。对该数据集进行降维处理为一维，就是找到一个投影之后使得三个数据点方差最大也就是投影到 y1 上使得 $OA_1^2 + OB_1^2 + OC_1^2$ 的平方和最大，根据勾股定理也就是$AA_1^2 + BB_1^2 + CC_1^2$ 平方和最小。所以等价的主成分分析在旋转变化中选取离样本点的距离平方和最小的轴作为第一主成分；第二主成分，即在第一主成分固定的情况下类似。\n\n![](/images/PCA/PCA.png)\n\n## 与 t-SNE 方法进行对比\n\nt-SNE 是一种非线性的降维技术，而 PCA 基于正交变换是一种线性的降维技术。t-SNE 工作原理的简要介绍：\n- 该算法一开始通过计算在高维空间中的数据点的相似度概率和与其对应的低维空间中的点的相似度的概率。点的相似度计算方法是：以A为中心的高斯分布中，如果按概率密度的比例选取相邻点，则点A将选择点B作为其相邻点的条件概率，以此计算点A的相似性。\n- 为了更好将数据投影至低维空间中，算法尝试去最小化高维数据空间和低维数据空间之间的条件概率（相似度）之差。\n- 为了评估t-SNE条件概率差和的最小化，使用梯度下降的方法最小化原分布中数据与映射分布中的对应数据的KL散度的总和。\n\n在高维数据的聚类分析中，往往是二者结合，先通过 PCA 将数据降维到较低维，再通过 t-SNE 进行降维。\n\n## 算法过程\n\n1. 按照下式进行规范化处理，得到规范化矩阵 X。\n$$ x_{ij} = \\frac{x_{ij} - \\bar x}{\\sqrt{s_{ij}}}$$\n其中\n$$ \\bar x = \\frac{1}{n} \\sum_{j=1}^{n} x_{ij} $$\n$$ s_{ij} = \\frac{1}{n-1} \\sum_{j=1}^{n}(x_{ij} - \\bar x_i)^2 $$\n\n2. 依据规范化矩阵，计算样本相关矩阵 R\n$$R = [r_{ij}]_{m \\times m} = \\frac{1}{n-1}XX^T$$\n其中\n$$r_{ij} = \\frac{1}{n-1} \\sum_{l=1}^{n}x_{il}x_{lj}$$\n\n3. 求样本相关矩阵 R 的 k 个特征值和对应的 k 个单位特征向量\n求解 R 的特征方程\n$$|R - \\lambda I| = 0$$\n得 R 的 m 个特征值\n$$\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_m$$\n求方差贡献率 $\\sum_{i=1}^k \\eta_i$达到预定值的主成分个数 k\n求前 k 个特征值对应的特征向量 $a_i$。\n\n4. 求 k 个样本主成分\n以 k 个单位特征向量为系数进行线性变换，求出 k 个样本成分\n$$y_i = a_i^Tx$$\n\nPS：\n其中关于特征值的求法也对应了两种 PCA 的实现方式，上述的求法是通过计算相关矩阵计算，而另一种方法是通过奇异值分解的方法计算特征值。\n\n\n## 手写实现 PCA\n\n``` python\nclass PCA():\n    # 计算协方差矩阵\n    def calc_cov(self, X):\n        m = X.shape[0]\n        # 数据标准化\n        X = (X - np.mean(X, axis=0))  / np.var(X, axis=0)\n        return 1 / m * np.matmul(X.T, X)\n    \n    # 根据累计方差贡献率选择特征\n    def select_by_per(self, eigVals, percentage):\n        eigVals_sort = sorted(eigVals, reverse=True)\n        eigVals_sum = sum(eigVals)\n        n = 0\n        sum_per = 0\n        for i in eigVals_sort:\n            n += 1\n            sum_per += i / eigVals_sum\n            if sum_per >= percentage:\n                return n\n\n    def pca(self, X, n_components=None, percentage=None):\n        # 计算协方差矩阵\n        cov_matrix = self.calc_cov(X)\n        # 计算协方差矩阵的特征值和对应特征向量\n        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n        # 对特征值排序\n        idx = eigenvalues.argsort()[::-1]\n        # 取最大的前n_component组\n        eigenvectors = eigenvectors[:, idx]\n        if  percentage:\n            print(eigenvalues)\n            n_components = self.select_by_per(eigenvalues, percentage)\n        print(n_components)\n        eigenvectors = eigenvectors[:, :n_components]\n        # Y=PX转换\n        return np.matmul(X, eigenvectors)\n```\n\n关于代码完整实现可以参考我的 [github](https://github.com/LALBJ/Deep-Learning-Models-Pytorch/blob/master/UL/decomposition/PCA.ipynb) \n\n## 使用技巧\n1. 关于数据是采用规范化处理还是均值0化处理：关于该问题的详细讨论可以参考后面附的参考文献，这个问题的结论的话呢就是具体问题具体分析。如果你想要消除不同单位及量级对于降维结果的影响就使用规范化的处理，否则就采用均值0化的处理；如果数据的特征由多个不同单位的特征，采用规范化处理一般会更好。这里举个例子，对于一个以千米作为单位的特征，如果将其转化为毫米，特征的方差就会变大，因此该特征对于降维结果的贡献度也就越高了。\n2. PCA 处理过程的特征值代表特征的重要程度，特征向量代表不同特征对于特征值的贡献度，如果使用 sklearn 的话可以使用 .components_ 这一 API 输出各个特征对于主成分的贡献度。\n3. 与聚类方法结合进行使用，这个也是引起广泛讨论的一个问题，将数据降维结果进行聚类是否合理？关于这一问题的答案是，不合理。因为无论是 PCA 还是 t-SNE 在进行降维的过程中都损失了一些高维空间距离信息，针对于这个结果再进行降维处理显然是不合理的\n\n## 参考文献\n1. 《统计学习方法》\n2. [Why do you need to standardize of your data before you apply a PCA?](https://www.quora.com/Why-do-you-need-to-standardize-zero-mean-unit-variance-of-your-data-before-you-apply-a-PCA-Which-important-assumptions-does-PCA-make)\n3. [Clustering on the output of t-SNE](https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne)","source":"_posts/主成分分析（PCA）.md","raw":"---\ntitle: 主成分分析（PCA）\ndate: 2022-11-02 12:32:24\ntags:\n- 涨知识\ncategories:\n- [机器学习]\n- [无监督学习]\ntoc: true\n---\n\n主成分分析（principal component analysis, PCA）是一种常用的无监督学习方法，这一方法利用正交变换把由线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，线性无关的变量称为主成分。主成分的个数通常小于原始变量的个数，所以主成分分析属于降维方法。\n\n本文主要针对使用和实现进行介绍，如果想要针对算法推导过程进行学习，可以参考李航老师的《统计学习方法》。\n<!-- more -->\n## 原理介绍\n\n主成分分析方法就是将原始数据通过正交变化的方法投影到方差最大的几个方向上，每一个方向也就是一个主成分。\n关于方差最大的解释。假如有两个变量x1 和 x2，三个样本点 A、B、C，样本分布在由 x1 和 x2 组成的坐标系中。对该数据集进行降维处理为一维，就是找到一个投影之后使得三个数据点方差最大也就是投影到 y1 上使得 $OA_1^2 + OB_1^2 + OC_1^2$ 的平方和最大，根据勾股定理也就是$AA_1^2 + BB_1^2 + CC_1^2$ 平方和最小。所以等价的主成分分析在旋转变化中选取离样本点的距离平方和最小的轴作为第一主成分；第二主成分，即在第一主成分固定的情况下类似。\n\n![](/images/PCA/PCA.png)\n\n## 与 t-SNE 方法进行对比\n\nt-SNE 是一种非线性的降维技术，而 PCA 基于正交变换是一种线性的降维技术。t-SNE 工作原理的简要介绍：\n- 该算法一开始通过计算在高维空间中的数据点的相似度概率和与其对应的低维空间中的点的相似度的概率。点的相似度计算方法是：以A为中心的高斯分布中，如果按概率密度的比例选取相邻点，则点A将选择点B作为其相邻点的条件概率，以此计算点A的相似性。\n- 为了更好将数据投影至低维空间中，算法尝试去最小化高维数据空间和低维数据空间之间的条件概率（相似度）之差。\n- 为了评估t-SNE条件概率差和的最小化，使用梯度下降的方法最小化原分布中数据与映射分布中的对应数据的KL散度的总和。\n\n在高维数据的聚类分析中，往往是二者结合，先通过 PCA 将数据降维到较低维，再通过 t-SNE 进行降维。\n\n## 算法过程\n\n1. 按照下式进行规范化处理，得到规范化矩阵 X。\n$$ x_{ij} = \\frac{x_{ij} - \\bar x}{\\sqrt{s_{ij}}}$$\n其中\n$$ \\bar x = \\frac{1}{n} \\sum_{j=1}^{n} x_{ij} $$\n$$ s_{ij} = \\frac{1}{n-1} \\sum_{j=1}^{n}(x_{ij} - \\bar x_i)^2 $$\n\n2. 依据规范化矩阵，计算样本相关矩阵 R\n$$R = [r_{ij}]_{m \\times m} = \\frac{1}{n-1}XX^T$$\n其中\n$$r_{ij} = \\frac{1}{n-1} \\sum_{l=1}^{n}x_{il}x_{lj}$$\n\n3. 求样本相关矩阵 R 的 k 个特征值和对应的 k 个单位特征向量\n求解 R 的特征方程\n$$|R - \\lambda I| = 0$$\n得 R 的 m 个特征值\n$$\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_m$$\n求方差贡献率 $\\sum_{i=1}^k \\eta_i$达到预定值的主成分个数 k\n求前 k 个特征值对应的特征向量 $a_i$。\n\n4. 求 k 个样本主成分\n以 k 个单位特征向量为系数进行线性变换，求出 k 个样本成分\n$$y_i = a_i^Tx$$\n\nPS：\n其中关于特征值的求法也对应了两种 PCA 的实现方式，上述的求法是通过计算相关矩阵计算，而另一种方法是通过奇异值分解的方法计算特征值。\n\n\n## 手写实现 PCA\n\n``` python\nclass PCA():\n    # 计算协方差矩阵\n    def calc_cov(self, X):\n        m = X.shape[0]\n        # 数据标准化\n        X = (X - np.mean(X, axis=0))  / np.var(X, axis=0)\n        return 1 / m * np.matmul(X.T, X)\n    \n    # 根据累计方差贡献率选择特征\n    def select_by_per(self, eigVals, percentage):\n        eigVals_sort = sorted(eigVals, reverse=True)\n        eigVals_sum = sum(eigVals)\n        n = 0\n        sum_per = 0\n        for i in eigVals_sort:\n            n += 1\n            sum_per += i / eigVals_sum\n            if sum_per >= percentage:\n                return n\n\n    def pca(self, X, n_components=None, percentage=None):\n        # 计算协方差矩阵\n        cov_matrix = self.calc_cov(X)\n        # 计算协方差矩阵的特征值和对应特征向量\n        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n        # 对特征值排序\n        idx = eigenvalues.argsort()[::-1]\n        # 取最大的前n_component组\n        eigenvectors = eigenvectors[:, idx]\n        if  percentage:\n            print(eigenvalues)\n            n_components = self.select_by_per(eigenvalues, percentage)\n        print(n_components)\n        eigenvectors = eigenvectors[:, :n_components]\n        # Y=PX转换\n        return np.matmul(X, eigenvectors)\n```\n\n关于代码完整实现可以参考我的 [github](https://github.com/LALBJ/Deep-Learning-Models-Pytorch/blob/master/UL/decomposition/PCA.ipynb) \n\n## 使用技巧\n1. 关于数据是采用规范化处理还是均值0化处理：关于该问题的详细讨论可以参考后面附的参考文献，这个问题的结论的话呢就是具体问题具体分析。如果你想要消除不同单位及量级对于降维结果的影响就使用规范化的处理，否则就采用均值0化的处理；如果数据的特征由多个不同单位的特征，采用规范化处理一般会更好。这里举个例子，对于一个以千米作为单位的特征，如果将其转化为毫米，特征的方差就会变大，因此该特征对于降维结果的贡献度也就越高了。\n2. PCA 处理过程的特征值代表特征的重要程度，特征向量代表不同特征对于特征值的贡献度，如果使用 sklearn 的话可以使用 .components_ 这一 API 输出各个特征对于主成分的贡献度。\n3. 与聚类方法结合进行使用，这个也是引起广泛讨论的一个问题，将数据降维结果进行聚类是否合理？关于这一问题的答案是，不合理。因为无论是 PCA 还是 t-SNE 在进行降维的过程中都损失了一些高维空间距离信息，针对于这个结果再进行降维处理显然是不合理的\n\n## 参考文献\n1. 《统计学习方法》\n2. [Why do you need to standardize of your data before you apply a PCA?](https://www.quora.com/Why-do-you-need-to-standardize-zero-mean-unit-variance-of-your-data-before-you-apply-a-PCA-Which-important-assumptions-does-PCA-make)\n3. [Clustering on the output of t-SNE](https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne)","slug":"主成分分析（PCA）","published":1,"updated":"2022-11-21T09:29:13.865Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp3000pqnwy71xo4a06","content":"<p>主成分分析（principal component analysis, PCA）是一种常用的无监督学习方法，这一方法利用正交变换把由线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，线性无关的变量称为主成分。主成分的个数通常小于原始变量的个数，所以主成分分析属于降维方法。</p>\n<p>本文主要针对使用和实现进行介绍，如果想要针对算法推导过程进行学习，可以参考李航老师的《统计学习方法》。</p>\n<span id=\"more\"></span>\n<h2 id=\"原理介绍\"><a href=\"#原理介绍\" class=\"headerlink\" title=\"原理介绍\"></a>原理介绍</h2><p>主成分分析方法就是将原始数据通过正交变化的方法投影到方差最大的几个方向上，每一个方向也就是一个主成分。<br>关于方差最大的解释。假如有两个变量x1 和 x2，三个样本点 A、B、C，样本分布在由 x1 和 x2 组成的坐标系中。对该数据集进行降维处理为一维，就是找到一个投影之后使得三个数据点方差最大也就是投影到 y1 上使得 $OA_1^2 + OB_1^2 + OC_1^2$ 的平方和最大，根据勾股定理也就是$AA_1^2 + BB_1^2 + CC_1^2$ 平方和最小。所以等价的主成分分析在旋转变化中选取离样本点的距离平方和最小的轴作为第一主成分；第二主成分，即在第一主成分固定的情况下类似。</p>\n<p><img src=\"/images/PCA/PCA.png\"></p>\n<h2 id=\"与-t-SNE-方法进行对比\"><a href=\"#与-t-SNE-方法进行对比\" class=\"headerlink\" title=\"与 t-SNE 方法进行对比\"></a>与 t-SNE 方法进行对比</h2><p>t-SNE 是一种非线性的降维技术，而 PCA 基于正交变换是一种线性的降维技术。t-SNE 工作原理的简要介绍：</p>\n<ul>\n<li>该算法一开始通过计算在高维空间中的数据点的相似度概率和与其对应的低维空间中的点的相似度的概率。点的相似度计算方法是：以A为中心的高斯分布中，如果按概率密度的比例选取相邻点，则点A将选择点B作为其相邻点的条件概率，以此计算点A的相似性。</li>\n<li>为了更好将数据投影至低维空间中，算法尝试去最小化高维数据空间和低维数据空间之间的条件概率（相似度）之差。</li>\n<li>为了评估t-SNE条件概率差和的最小化，使用梯度下降的方法最小化原分布中数据与映射分布中的对应数据的KL散度的总和。</li>\n</ul>\n<p>在高维数据的聚类分析中，往往是二者结合，先通过 PCA 将数据降维到较低维，再通过 t-SNE 进行降维。</p>\n<h2 id=\"算法过程\"><a href=\"#算法过程\" class=\"headerlink\" title=\"算法过程\"></a>算法过程</h2><ol>\n<li><p>按照下式进行规范化处理，得到规范化矩阵 X。<br>$$ x_{ij} &#x3D; \\frac{x_{ij} - \\bar x}{\\sqrt{s_{ij}}}$$<br>其中<br>$$ \\bar x &#x3D; \\frac{1}{n} \\sum_{j&#x3D;1}^{n} x_{ij} $$<br>$$ s_{ij} &#x3D; \\frac{1}{n-1} \\sum_{j&#x3D;1}^{n}(x_{ij} - \\bar x_i)^2 $$</p>\n</li>\n<li><p>依据规范化矩阵，计算样本相关矩阵 R<br>$$R &#x3D; [r_{ij}]<em>{m \\times m} &#x3D; \\frac{1}{n-1}XX^T$$<br>其中<br>$$r</em>{ij} &#x3D; \\frac{1}{n-1} \\sum_{l&#x3D;1}^{n}x_{il}x_{lj}$$</p>\n</li>\n<li><p>求样本相关矩阵 R 的 k 个特征值和对应的 k 个单位特征向量<br>求解 R 的特征方程<br>$$|R - \\lambda I| &#x3D; 0$$<br>得 R 的 m 个特征值<br>$$\\lambda_1 \\geq \\lambda_2 \\geq … \\geq \\lambda_m$$<br>求方差贡献率 $\\sum_{i&#x3D;1}^k \\eta_i$达到预定值的主成分个数 k<br>求前 k 个特征值对应的特征向量 $a_i$。</p>\n</li>\n<li><p>求 k 个样本主成分<br>以 k 个单位特征向量为系数进行线性变换，求出 k 个样本成分<br>$$y_i &#x3D; a_i^Tx$$</p>\n</li>\n</ol>\n<p>PS：<br>其中关于特征值的求法也对应了两种 PCA 的实现方式，上述的求法是通过计算相关矩阵计算，而另一种方法是通过奇异值分解的方法计算特征值。</p>\n<h2 id=\"手写实现-PCA\"><a href=\"#手写实现-PCA\" class=\"headerlink\" title=\"手写实现 PCA\"></a>手写实现 PCA</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PCA</span>():</span><br><span class=\"line\">    <span class=\"comment\"># 计算协方差矩阵</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">calc_cov</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        m = X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"comment\"># 数据标准化</span></span><br><span class=\"line\">        X = (X - np.mean(X, axis=<span class=\"number\">0</span>))  / np.var(X, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span> / m * np.matmul(X.T, X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 根据累计方差贡献率选择特征</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">select_by_per</span>(<span class=\"params\">self, eigVals, percentage</span>):</span><br><span class=\"line\">        eigVals_sort = <span class=\"built_in\">sorted</span>(eigVals, reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        eigVals_sum = <span class=\"built_in\">sum</span>(eigVals)</span><br><span class=\"line\">        n = <span class=\"number\">0</span></span><br><span class=\"line\">        sum_per = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> eigVals_sort:</span><br><span class=\"line\">            n += <span class=\"number\">1</span></span><br><span class=\"line\">            sum_per += i / eigVals_sum</span><br><span class=\"line\">            <span class=\"keyword\">if</span> sum_per &gt;= percentage:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> n</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">pca</span>(<span class=\"params\">self, X, n_components=<span class=\"literal\">None</span>, percentage=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"comment\"># 计算协方差矩阵</span></span><br><span class=\"line\">        cov_matrix = self.calc_cov(X)</span><br><span class=\"line\">        <span class=\"comment\"># 计算协方差矩阵的特征值和对应特征向量</span></span><br><span class=\"line\">        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)</span><br><span class=\"line\">        <span class=\"comment\"># 对特征值排序</span></span><br><span class=\"line\">        idx = eigenvalues.argsort()[::-<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"comment\"># 取最大的前n_component组</span></span><br><span class=\"line\">        eigenvectors = eigenvectors[:, idx]</span><br><span class=\"line\">        <span class=\"keyword\">if</span>  percentage:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(eigenvalues)</span><br><span class=\"line\">            n_components = self.select_by_per(eigenvalues, percentage)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(n_components)</span><br><span class=\"line\">        eigenvectors = eigenvectors[:, :n_components]</span><br><span class=\"line\">        <span class=\"comment\"># Y=PX转换</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.matmul(X, eigenvectors)</span><br></pre></td></tr></table></figure>\n\n<p>关于代码完整实现可以参考我的 <a href=\"https://github.com/LALBJ/Deep-Learning-Models-Pytorch/blob/master/UL/decomposition/PCA.ipynb\">github</a> </p>\n<h2 id=\"使用技巧\"><a href=\"#使用技巧\" class=\"headerlink\" title=\"使用技巧\"></a>使用技巧</h2><ol>\n<li>关于数据是采用规范化处理还是均值0化处理：关于该问题的详细讨论可以参考后面附的参考文献，这个问题的结论的话呢就是具体问题具体分析。如果你想要消除不同单位及量级对于降维结果的影响就使用规范化的处理，否则就采用均值0化的处理；如果数据的特征由多个不同单位的特征，采用规范化处理一般会更好。这里举个例子，对于一个以千米作为单位的特征，如果将其转化为毫米，特征的方差就会变大，因此该特征对于降维结果的贡献度也就越高了。</li>\n<li>PCA 处理过程的特征值代表特征的重要程度，特征向量代表不同特征对于特征值的贡献度，如果使用 sklearn 的话可以使用 .components_ 这一 API 输出各个特征对于主成分的贡献度。</li>\n<li>与聚类方法结合进行使用，这个也是引起广泛讨论的一个问题，将数据降维结果进行聚类是否合理？关于这一问题的答案是，不合理。因为无论是 PCA 还是 t-SNE 在进行降维的过程中都损失了一些高维空间距离信息，针对于这个结果再进行降维处理显然是不合理的</li>\n</ol>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><ol>\n<li>《统计学习方法》</li>\n<li><a href=\"https://www.quora.com/Why-do-you-need-to-standardize-zero-mean-unit-variance-of-your-data-before-you-apply-a-PCA-Which-important-assumptions-does-PCA-make\">Why do you need to standardize of your data before you apply a PCA?</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne\">Clustering on the output of t-SNE</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>主成分分析（principal component analysis, PCA）是一种常用的无监督学习方法，这一方法利用正交变换把由线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，线性无关的变量称为主成分。主成分的个数通常小于原始变量的个数，所以主成分分析属于降维方法。</p>\n<p>本文主要针对使用和实现进行介绍，如果想要针对算法推导过程进行学习，可以参考李航老师的《统计学习方法》。</p>","more":"<h2 id=\"原理介绍\"><a href=\"#原理介绍\" class=\"headerlink\" title=\"原理介绍\"></a>原理介绍</h2><p>主成分分析方法就是将原始数据通过正交变化的方法投影到方差最大的几个方向上，每一个方向也就是一个主成分。<br>关于方差最大的解释。假如有两个变量x1 和 x2，三个样本点 A、B、C，样本分布在由 x1 和 x2 组成的坐标系中。对该数据集进行降维处理为一维，就是找到一个投影之后使得三个数据点方差最大也就是投影到 y1 上使得 $OA_1^2 + OB_1^2 + OC_1^2$ 的平方和最大，根据勾股定理也就是$AA_1^2 + BB_1^2 + CC_1^2$ 平方和最小。所以等价的主成分分析在旋转变化中选取离样本点的距离平方和最小的轴作为第一主成分；第二主成分，即在第一主成分固定的情况下类似。</p>\n<p><img src=\"/images/PCA/PCA.png\"></p>\n<h2 id=\"与-t-SNE-方法进行对比\"><a href=\"#与-t-SNE-方法进行对比\" class=\"headerlink\" title=\"与 t-SNE 方法进行对比\"></a>与 t-SNE 方法进行对比</h2><p>t-SNE 是一种非线性的降维技术，而 PCA 基于正交变换是一种线性的降维技术。t-SNE 工作原理的简要介绍：</p>\n<ul>\n<li>该算法一开始通过计算在高维空间中的数据点的相似度概率和与其对应的低维空间中的点的相似度的概率。点的相似度计算方法是：以A为中心的高斯分布中，如果按概率密度的比例选取相邻点，则点A将选择点B作为其相邻点的条件概率，以此计算点A的相似性。</li>\n<li>为了更好将数据投影至低维空间中，算法尝试去最小化高维数据空间和低维数据空间之间的条件概率（相似度）之差。</li>\n<li>为了评估t-SNE条件概率差和的最小化，使用梯度下降的方法最小化原分布中数据与映射分布中的对应数据的KL散度的总和。</li>\n</ul>\n<p>在高维数据的聚类分析中，往往是二者结合，先通过 PCA 将数据降维到较低维，再通过 t-SNE 进行降维。</p>\n<h2 id=\"算法过程\"><a href=\"#算法过程\" class=\"headerlink\" title=\"算法过程\"></a>算法过程</h2><ol>\n<li><p>按照下式进行规范化处理，得到规范化矩阵 X。<br>$$ x_{ij} &#x3D; \\frac{x_{ij} - \\bar x}{\\sqrt{s_{ij}}}$$<br>其中<br>$$ \\bar x &#x3D; \\frac{1}{n} \\sum_{j&#x3D;1}^{n} x_{ij} $$<br>$$ s_{ij} &#x3D; \\frac{1}{n-1} \\sum_{j&#x3D;1}^{n}(x_{ij} - \\bar x_i)^2 $$</p>\n</li>\n<li><p>依据规范化矩阵，计算样本相关矩阵 R<br>$$R &#x3D; [r_{ij}]<em>{m \\times m} &#x3D; \\frac{1}{n-1}XX^T$$<br>其中<br>$$r</em>{ij} &#x3D; \\frac{1}{n-1} \\sum_{l&#x3D;1}^{n}x_{il}x_{lj}$$</p>\n</li>\n<li><p>求样本相关矩阵 R 的 k 个特征值和对应的 k 个单位特征向量<br>求解 R 的特征方程<br>$$|R - \\lambda I| &#x3D; 0$$<br>得 R 的 m 个特征值<br>$$\\lambda_1 \\geq \\lambda_2 \\geq … \\geq \\lambda_m$$<br>求方差贡献率 $\\sum_{i&#x3D;1}^k \\eta_i$达到预定值的主成分个数 k<br>求前 k 个特征值对应的特征向量 $a_i$。</p>\n</li>\n<li><p>求 k 个样本主成分<br>以 k 个单位特征向量为系数进行线性变换，求出 k 个样本成分<br>$$y_i &#x3D; a_i^Tx$$</p>\n</li>\n</ol>\n<p>PS：<br>其中关于特征值的求法也对应了两种 PCA 的实现方式，上述的求法是通过计算相关矩阵计算，而另一种方法是通过奇异值分解的方法计算特征值。</p>\n<h2 id=\"手写实现-PCA\"><a href=\"#手写实现-PCA\" class=\"headerlink\" title=\"手写实现 PCA\"></a>手写实现 PCA</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PCA</span>():</span><br><span class=\"line\">    <span class=\"comment\"># 计算协方差矩阵</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">calc_cov</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        m = X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"comment\"># 数据标准化</span></span><br><span class=\"line\">        X = (X - np.mean(X, axis=<span class=\"number\">0</span>))  / np.var(X, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span> / m * np.matmul(X.T, X)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 根据累计方差贡献率选择特征</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">select_by_per</span>(<span class=\"params\">self, eigVals, percentage</span>):</span><br><span class=\"line\">        eigVals_sort = <span class=\"built_in\">sorted</span>(eigVals, reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        eigVals_sum = <span class=\"built_in\">sum</span>(eigVals)</span><br><span class=\"line\">        n = <span class=\"number\">0</span></span><br><span class=\"line\">        sum_per = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> eigVals_sort:</span><br><span class=\"line\">            n += <span class=\"number\">1</span></span><br><span class=\"line\">            sum_per += i / eigVals_sum</span><br><span class=\"line\">            <span class=\"keyword\">if</span> sum_per &gt;= percentage:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> n</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">pca</span>(<span class=\"params\">self, X, n_components=<span class=\"literal\">None</span>, percentage=<span class=\"literal\">None</span></span>):</span><br><span class=\"line\">        <span class=\"comment\"># 计算协方差矩阵</span></span><br><span class=\"line\">        cov_matrix = self.calc_cov(X)</span><br><span class=\"line\">        <span class=\"comment\"># 计算协方差矩阵的特征值和对应特征向量</span></span><br><span class=\"line\">        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)</span><br><span class=\"line\">        <span class=\"comment\"># 对特征值排序</span></span><br><span class=\"line\">        idx = eigenvalues.argsort()[::-<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"comment\"># 取最大的前n_component组</span></span><br><span class=\"line\">        eigenvectors = eigenvectors[:, idx]</span><br><span class=\"line\">        <span class=\"keyword\">if</span>  percentage:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(eigenvalues)</span><br><span class=\"line\">            n_components = self.select_by_per(eigenvalues, percentage)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(n_components)</span><br><span class=\"line\">        eigenvectors = eigenvectors[:, :n_components]</span><br><span class=\"line\">        <span class=\"comment\"># Y=PX转换</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.matmul(X, eigenvectors)</span><br></pre></td></tr></table></figure>\n\n<p>关于代码完整实现可以参考我的 <a href=\"https://github.com/LALBJ/Deep-Learning-Models-Pytorch/blob/master/UL/decomposition/PCA.ipynb\">github</a> </p>\n<h2 id=\"使用技巧\"><a href=\"#使用技巧\" class=\"headerlink\" title=\"使用技巧\"></a>使用技巧</h2><ol>\n<li>关于数据是采用规范化处理还是均值0化处理：关于该问题的详细讨论可以参考后面附的参考文献，这个问题的结论的话呢就是具体问题具体分析。如果你想要消除不同单位及量级对于降维结果的影响就使用规范化的处理，否则就采用均值0化的处理；如果数据的特征由多个不同单位的特征，采用规范化处理一般会更好。这里举个例子，对于一个以千米作为单位的特征，如果将其转化为毫米，特征的方差就会变大，因此该特征对于降维结果的贡献度也就越高了。</li>\n<li>PCA 处理过程的特征值代表特征的重要程度，特征向量代表不同特征对于特征值的贡献度，如果使用 sklearn 的话可以使用 .components_ 这一 API 输出各个特征对于主成分的贡献度。</li>\n<li>与聚类方法结合进行使用，这个也是引起广泛讨论的一个问题，将数据降维结果进行聚类是否合理？关于这一问题的答案是，不合理。因为无论是 PCA 还是 t-SNE 在进行降维的过程中都损失了一些高维空间距离信息，针对于这个结果再进行降维处理显然是不合理的</li>\n</ol>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><ol>\n<li>《统计学习方法》</li>\n<li><a href=\"https://www.quora.com/Why-do-you-need-to-standardize-zero-mean-unit-variance-of-your-data-before-you-apply-a-PCA-Which-important-assumptions-does-PCA-make\">Why do you need to standardize of your data before you apply a PCA?</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne\">Clustering on the output of t-SNE</a></li>\n</ol>"},{"title":"archive todo pages","date":"2023-11-30T05:48:17.000Z","_content":"","source":"_posts/archive-todo-pages.md","raw":"---\ntitle: archive todo pages\ndate: 2023-11-30 13:48:17\ntags:\n---\n","slug":"archive-todo-pages","published":1,"updated":"2023-11-30T05:48:17.203Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp3000qqnwy6ug99muk","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"损失函数汇总","date":"2023-03-24T03:34:33.000Z","_content":"\n## Cross Entropy\n\n$$L_{CE} = - \\sum_{i=1}^{n}t_ilog(p_i), \\ for\\ n\\ classes$$\n\n## Binary Cross Entropy \n\n$$\n\\begin{align*}\n  L_{CE} &= \\sum_{i=1}^{2}t_ilog(p_i) \\\\\n    &= -[tlog(p) + (1-t)log(1-p)] \n\\end{align*}\n$$\n","source":"_posts/损失函数汇总.md","raw":"---\ntitle: 损失函数汇总\ndate: 2023-03-24 11:34:33\ntags:\n---\n\n## Cross Entropy\n\n$$L_{CE} = - \\sum_{i=1}^{n}t_ilog(p_i), \\ for\\ n\\ classes$$\n\n## Binary Cross Entropy \n\n$$\n\\begin{align*}\n  L_{CE} &= \\sum_{i=1}^{2}t_ilog(p_i) \\\\\n    &= -[tlog(p) + (1-t)log(1-p)] \n\\end{align*}\n$$\n","slug":"损失函数汇总","published":1,"updated":"2023-03-24T06:30:58.688Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp4000sqnwy0c6e2v9w","content":"<h2 id=\"Cross-Entropy\"><a href=\"#Cross-Entropy\" class=\"headerlink\" title=\"Cross Entropy\"></a>Cross Entropy</h2><p>$$L_{CE} &#x3D; - \\sum_{i&#x3D;1}^{n}t_ilog(p_i), \\ for\\ n\\ classes$$</p>\n<h2 id=\"Binary-Cross-Entropy\"><a href=\"#Binary-Cross-Entropy\" class=\"headerlink\" title=\"Binary Cross Entropy\"></a>Binary Cross Entropy</h2><p>$$<br>\\begin{align*}<br>  L_{CE} &amp;&#x3D; \\sum_{i&#x3D;1}^{2}t_ilog(p_i) \\<br>    &amp;&#x3D; -[tlog(p) + (1-t)log(1-p)]<br>\\end{align*}<br>$$</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Cross-Entropy\"><a href=\"#Cross-Entropy\" class=\"headerlink\" title=\"Cross Entropy\"></a>Cross Entropy</h2><p>$$L_{CE} &#x3D; - \\sum_{i&#x3D;1}^{n}t_ilog(p_i), \\ for\\ n\\ classes$$</p>\n<h2 id=\"Binary-Cross-Entropy\"><a href=\"#Binary-Cross-Entropy\" class=\"headerlink\" title=\"Binary Cross Entropy\"></a>Binary Cross Entropy</h2><p>$$<br>\\begin{align*}<br>  L_{CE} &amp;&#x3D; \\sum_{i&#x3D;1}^{2}t_ilog(p_i) \\<br>    &amp;&#x3D; -[tlog(p) + (1-t)log(1-p)]<br>\\end{align*}<br>$$</p>\n"},{"title":"图模型及其应用","date":"2022-11-21T08:59:52.000Z","_content":"\n三种结构：Fork\\Collider\\Chain 的三个规则","source":"_posts/图模型及其应用.md","raw":"---\ntitle: 图模型及其应用\ndate: 2022-11-21 16:59:52\ntags:\n---\n\n三种结构：Fork\\Collider\\Chain 的三个规则","slug":"图模型及其应用","published":1,"updated":"2022-11-25T12:19:38.546Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp4000tqnwy05dcgkai","content":"<p>三种结构：Fork\\Collider\\Chain 的三个规则</p>\n","site":{"data":{}},"excerpt":"","more":"<p>三种结构：Fork\\Collider\\Chain 的三个规则</p>\n"},{"title":"深度学习提分技巧","date":"2022-12-27T08:26:48.000Z","_content":"\n## 1. 迁移训练\n\n## 2. 差分学习率\n\n## 3. 对抗训练\n\n## 4. K 折训练","source":"_posts/深度学习提分技巧.md","raw":"---\ntitle: 深度学习提分技巧\ndate: 2022-12-27 16:26:48\ntags:\n---\n\n## 1. 迁移训练\n\n## 2. 差分学习率\n\n## 3. 对抗训练\n\n## 4. K 折训练","slug":"深度学习提分技巧","published":1,"updated":"2023-01-11T12:42:27.317Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp4000vqnwy1z3n6j8k","content":"<h2 id=\"1-迁移训练\"><a href=\"#1-迁移训练\" class=\"headerlink\" title=\"1. 迁移训练\"></a>1. 迁移训练</h2><h2 id=\"2-差分学习率\"><a href=\"#2-差分学习率\" class=\"headerlink\" title=\"2. 差分学习率\"></a>2. 差分学习率</h2><h2 id=\"3-对抗训练\"><a href=\"#3-对抗训练\" class=\"headerlink\" title=\"3. 对抗训练\"></a>3. 对抗训练</h2><h2 id=\"4-K-折训练\"><a href=\"#4-K-折训练\" class=\"headerlink\" title=\"4. K 折训练\"></a>4. K 折训练</h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1-迁移训练\"><a href=\"#1-迁移训练\" class=\"headerlink\" title=\"1. 迁移训练\"></a>1. 迁移训练</h2><h2 id=\"2-差分学习率\"><a href=\"#2-差分学习率\" class=\"headerlink\" title=\"2. 差分学习率\"></a>2. 差分学习率</h2><h2 id=\"3-对抗训练\"><a href=\"#3-对抗训练\" class=\"headerlink\" title=\"3. 对抗训练\"></a>3. 对抗训练</h2><h2 id=\"4-K-折训练\"><a href=\"#4-K-折训练\" class=\"headerlink\" title=\"4. K 折训练\"></a>4. K 折训练</h2>"},{"title":"激活函数汇总","date":"2023-02-09T13:32:00.000Z","_content":"\n","source":"_posts/激活函数汇总.md","raw":"---\ntitle: 激活函数汇总\ndate: 2023-02-09 21:32:00\ntags:\n---\n\n","slug":"激活函数汇总","published":1,"updated":"2023-02-21T13:03:44.767Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp4000wqnwy24zn5yy4","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"深度学习调参指南","date":"2023-01-31T13:42:34.000Z","_content":"\n## 新项目指南\n","source":"_posts/深度学习调参指南.md","raw":"---\ntitle: 深度学习调参指南\ndate: 2023-01-31 21:42:34\ntags:\n---\n\n## 新项目指南\n","slug":"深度学习调参指南","published":1,"updated":"2023-02-08T13:56:39.374Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpku7kp4000xqnwyf5x49ur9","content":"<h2 id=\"新项目指南\"><a href=\"#新项目指南\" class=\"headerlink\" title=\"新项目指南\"></a>新项目指南</h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"新项目指南\"><a href=\"#新项目指南\" class=\"headerlink\" title=\"新项目指南\"></a>新项目指南</h2>"}],"PostAsset":[],"PostCategory":[{"post_id":"clpku7kp1000dqnwyhmt6hkb6","category_id":"clpku7kp1000gqnwyeym019np","_id":"clpku7kp2000kqnwydltzhlj2"},{"post_id":"clpku7kp1000dqnwyhmt6hkb6","category_id":"clpku7kp1000jqnwy16asc1ar","_id":"clpku7kp2000lqnwy0zlcgz9z"},{"post_id":"clpku7kp3000pqnwy71xo4a06","category_id":"clpku7kp1000gqnwyeym019np","_id":"clpku7kp4000yqnwy1wq2e3nx"},{"post_id":"clpku7kp3000pqnwy71xo4a06","category_id":"clpku7kp4000uqnwyfra4a7tb","_id":"clpku7kp4000zqnwyb6on5yqg"}],"PostTag":[{"post_id":"clpku7koy0005qnwy05sg611e","tag_id":"clpku7koy0007qnwy9ud54ghj","_id":"clpku7kp1000eqnwy94hlhf1c"},{"post_id":"clpku7koy0005qnwy05sg611e","tag_id":"clpku7kp0000bqnwya4qwd2ks","_id":"clpku7kp1000hqnwy2fi9hwv9"},{"post_id":"clpku7kp1000dqnwyhmt6hkb6","tag_id":"clpku7kp1000fqnwy422v2dl8","_id":"clpku7kp1000iqnwy0vmi0rn0"},{"post_id":"clpku7kp3000pqnwy71xo4a06","tag_id":"clpku7kp1000fqnwy422v2dl8","_id":"clpku7kp4000rqnwy4g2514py"}],"Tag":[{"name":"强化学习","_id":"clpku7koy0007qnwy9ud54ghj"},{"name":"可视化","_id":"clpku7kp0000bqnwya4qwd2ks"},{"name":"涨知识","_id":"clpku7kp1000fqnwy422v2dl8"}]}}